================================================================================
ISIT312 - ALL LECTURE CONTENT
================================================================================



################################################################################
FILE: L0 Subject Overview.pdf
################################################################################

================================================================================
PAGE 1 of 10
================================================================================

1
ISIT312 
Big Data Management
SIM S
 4
 
202
5
================================================================================
PAGE 2 of 10
================================================================================

2Lecturer and Tutor
Lecturer:  Dr Fenghui Ren
Email:  fren@uow.edu.au
Tutor:  Mr. Sionggo Japit
Email:  sjapit@uow.edu.au
================================================================================
PAGE 3 of 10
================================================================================

3Subject Schedule
Session:   SIM S 4 2025
Lecture schedule:

================================================================================
PAGE 4 of 10
================================================================================

4Subject Schedule
Session:   SIM S 4 2025
Lab schedule:

================================================================================
PAGE 5 of 10
================================================================================

5Assessment
ISIT 312 Assignments (individual) 10% + 20% + 20% = 50%
1 Final examination   50%
Exam period    
40% TF clause on the final examination !
================================================================================
PAGE 6 of 10
================================================================================

6Topics

================================================================================
PAGE 7 of 10
================================================================================

7Tools
The VM: Ubuntu 14.04 LTS VM (run with VirtualBox)
Inside VM:
Big data software: 
  Hadoop, Hive, HBase, Pig, Spark, Kafka
      Tools: 
 Linux shell (bash)
 Text editor (gedit)
 Zeppelin
 Other (optional): Eclipse
      Programming languages:
 Java
================================================================================
PAGE 8 of 10
================================================================================

8Subject motto
If we only hear a thing,
  we soon forget it ! 
If we see it,
  we remember it !
If we do it ourselves,
  we know it !
================================================================================
PAGE 9 of 10
================================================================================

9IMPORTANT:
Read your emails in time and watch closely 
on the Moodle announcements.
================================================================================
PAGE 10 of 10
================================================================================

10Questions ?
================================================================================


################################################################################
FILE: 01clustercomputing.pdf
################################################################################

================================================================================
PAGE 1 of 28
================================================================================

  ISIT312 Big Data Management
Cluster Computing
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 28
================================================================================

Cluster Computing
Outline
Computer Cluster
Big Data
Traditional Data Architectures
Meet Hadoop !
Big Data on Database Clusters
Big Data on Kubernetes
TOP                    ISIT312 Big Data Management,  SIM S4 2025 2/28
================================================================================
PAGE 3 of 28
================================================================================

Computer Cluster
What is a computer cluster ?
A computer cluster is a collection of computers (also called as nodes)
connected through high speed network that work together to simulate a
single much more powerful computer system
Each node in a computer cluster is controlled by its own operating
system
Each node in a computer cluster performs a di !erent version of the
same task
A di!erence between computer cluster and computer grid is such that
the nodes in a computer grid perfrom di!erent tasks
An architecture of computer cluster ranges from a simple two-node
system connecting two personal computers to a supercomputer with a
cluster architecture
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/28
================================================================================
PAGE 4 of 28
================================================================================

Computer Cluster
Computer clusters are used to speed up computing through shared
nothing (sharding ) partitioning of data and paralellization of data
procesing on the nodes of a cluster
Computer clusters provide high availability through automatic
replacement of a failed node with a replica node
Advantages of computer clusters: faster processing speed, larger
storage capacity, better data integrity, greater reliability and wider
availability of resources
A Linux cluster is a collection of connected computers that can be
viewed and managed as a single system
A sample computer cluster: 54 regular compute nodes (with two 32-Core
Intel 8358 processors, 1.6TB of local NVME storage and 512GB of
memory each) and 5 GPU nodes with two 24-Core AMD EPYC 7413
processors, eight A100 GPU cards, 960GB of local storage and 512GB of
memory each
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/28
================================================================================
PAGE 5 of 28
================================================================================

Computer Cluster
What is a cluster computing ?
Cluster computing is the process of sharing the computation tasks
among multiple computers included in a computer cluster
Advantages of cluster computing: cost e "ciency, processing speed,
expandability, high vailability of resources
At the moment cluster computing is an attractive paradigm for
processing large scale science, engineering and commercial applications
Cluster computing requires the specialized algorithms like load
balancing, resource sharing and resource scheduling for optimization of
data processing
Cluster computing is an attractive alternative to data processing on large
parallel supercomputers
The simplest conﬁguration of nodes for cluster computing consists of a
master node  and slave nodes
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/28
================================================================================
PAGE 6 of 28
================================================================================

Cluster Computing
Outline
Computer Cluster
Big Data
Traditional Data Architectures
Meet Hadoop !
Big Data on Database Clusters
Big Data on Kubernetes
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/28
================================================================================
PAGE 7 of 28
================================================================================

Big Data
What does Big Data mean and how big is Big Data ?
Big Data is so big that it cannot be stored on the persistent storage
devices attached to a single computer system
Big Data may also mean an inﬁnite amount of data
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/28
================================================================================
PAGE 8 of 28
================================================================================

Big Data
What are the sources of Big Data ?
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/28
================================================================================
PAGE 9 of 28
================================================================================

Big Data
Big Data is characterized by so called 3V features :
Additional Vs:
There are many, many other Vs, the largest number of Vs I found on
Web was 42 !Volume: e.g., billions of rows ? millions of columns
Variety : Complexity of data types and structures
Velocity: Speed of new data creation and growth-
-
-
Veracity: Ability to represent and process uncertain and imprecise data
Value: Data is the driving force of the next-generate business
Viability : Beneﬁts we can potentially have from data analysis-
-
-
Vagueness : The meaning of found data is often very unclear, regardless of how
much data is available
Validity : Rigor in analysis is essential for valid predictions where data is the
driving force of the next-generate business
Vane: Data science can aid decision making by pointing in the correct direction
... and many, many others ... :)-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/28
================================================================================
PAGE 10 of 28
================================================================================

Big Data
Examples of Big Data:
Clickstream data
Call centre data
E-mail and instant-messaging
Sensor data
Unstructured data
Geographic data
Satellite data
Image data
Temporal data
and more ...-
-
-
-
-
-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/28
================================================================================
PAGE 11 of 28
================================================================================

Cluster Computing
Outline
Computer Cluster
Big Data
Traditional Data Architectures
Meet Hadoop !
Big Data on Database Clusters
Big Data on Kubernetes
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/28
================================================================================
PAGE 12 of 28
================================================================================

Traditional Data Architectures
Data warehousing  technologies
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/28
================================================================================
PAGE 13 of 28
================================================================================

Traditional Data Architectures
The strength of traditional data architectures:
The challenges for traditional data architectures:Centralised governance of data repositories
Light-fast inquires performed regularly in daily business
Optimisation for OLTP and OLAP
Security and access control
Fault-Tolerance and backup-
-
-
-
-
New types of data such as unstructured data and semi-structured data
Increasingly large amounts of data ﬂowing into organisations
New computational paradigms use non-traditional NoSQL databases to rapidly
mine and analyse very large data sets
Increasing cost of storing and analysing the large amounts of data
Increasing use of data analytics, which requires signiﬁcant storage and
processing capabilities-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/28
================================================================================
PAGE 14 of 28
================================================================================

Traditional Data Architectures
A sample Data Lake  architecture
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/28
================================================================================
PAGE 15 of 28
================================================================================

Traditional Data Architectures
Hardware for Big Data has two scalability dimensions
TOP                    ISIT312 Big Data Management, SIM S4 2025 15/28
================================================================================
PAGE 16 of 28
================================================================================

Cluster Computing
Outline
Computer Cluster
Big Data
Traditional Data Architectures
Meet Hadoop !
Big Data on Database Clusters
Big Data on Kubernetes
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/28
================================================================================
PAGE 17 of 28
================================================================================

Meet Hadoop !
Hadoop , in terms of its developers, is a project that develops open-
source software for reliable, scalable, distributed computing
Features of Hadoop
Capability to handle large data sets, e.g. simple scalability and coordination
File size range from gigabytes to terabytes
Can store millions of those ﬁles
High fault tolerance
Supports data replication
Supports streaming access to data
Supports batch processing
Support interactive, iterative and stream processing
Implements a data consistency model of write-once-read-many access model
Run on commodity hardware, not high-performance computers
Inexpensive
It can be deployed on premises or in the cloud-
-
-
-
-
-
-
-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 17/28
================================================================================
PAGE 18 of 28
================================================================================

Meet Hadoop !
Core components of Hadoop
TOP                    ISIT312 Big Data Management, SIM S4 2025 18/28
================================================================================
PAGE 19 of 28
================================================================================

Hadoop Ecosystem
Hadoop  ecosystem
TOP                    ISIT312 Big Data Management, SIM S4 2025 19/28
================================================================================
PAGE 20 of 28
================================================================================

Commercial Hadoop Landscape
Commercial Hadoop  landscape
TOP                    ISIT312 Big Data Management, SIM S4 2025 20/28
================================================================================
PAGE 21 of 28
================================================================================

Meet Hadoop !
Master-slave  architecture of Hadoop clusters
TOP                    ISIT312 Big Data Management, SIM S4 2025 21/28
================================================================================
PAGE 22 of 28
================================================================================

Meet Hadoop !
Hadoop clusters  can support up to 10,000 server and receives near-to-
linear scalability in computing power
A typical Hadoop cluster consists of:
A set of master nodes  (servers) where the daemons supporting key Hadoop
frame-works run
A set of worker nodes  that host the storage (HDFS) and computing (YARN) work
One or more edge servers, which are used for accessing the Hadoop cluster to
launch applications
One or more relational databases  such as MySQL for storing the metadata
repositories
Dedicated servers for special frameworks such as Kafka-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 22/28
================================================================================
PAGE 23 of 28
================================================================================

Meet Hadoop !
Hadoop  also support the pseudo-distributed mode
Our lab setting is the pseudo-distributed modeAll HDFS and YARN daemons running on a single node.
Highly simulate the full cluster
Easy for beginner's practice
Easy for testing and debug-
-
-
-
The single node is a Ubuntu 14.04 Virtual Machine (VM)-
TOP                    ISIT312 Big Data Management, SIM S4 2025 23/28
================================================================================
PAGE 24 of 28
================================================================================

Cluster Computing
Outline
Computer Cluster
Big Data
Traditional Data Architectures
Meet Hadoop !
Big Data on Database Clusters
Big Data on Kubernetes
TOP                    ISIT312 Big Data Management, SIM S4 2025 24/28
================================================================================
PAGE 25 of 28
================================================================================

Big Data on Database Clusters
A database cluster is a collection of databases that is managed by a
single instance of a running database server
A very large database in a database cluster is partitioned over a number
of smaller databases each located on a separate node of a computer
cluster
Database clustering requires replication and sharding
Database clustering improve performance, availability, and scalability
The classes of database system that allow for database clustering:
NoSQL systems: MongoDB, RavenDB, Cassandra, Amazon Aurora, ...
NewSQL systems: ClustrixDB, NuoDB, CockroachDB, Pivotal GemFire XD,
Altibase, MemSQL, VoltDB, ...
Improved OldSQL  systems: Oracle RAC, SQL Server (Windows server Failover
Cluster), DB2 Cluster, PostgreSQL, MySQL Cluster, ...-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 25/28
================================================================================
PAGE 26 of 28
================================================================================

Cluster Computing
Outline
Computer Cluster
Big Data
Traditional Data Architectures
Meet Hadoop !
Big Data on Database Clusters
Big Data on Kubernetes
TOP                    ISIT312 Big Data Management, SIM S4 2025 26/28
================================================================================
PAGE 27 of 28
================================================================================

Big Data on Kubernetes
Kubernetes  (K8) is a container or microservice platform that
orchestrates computing, networking, and storage infrastructure
workloads
in a plain language Kubernetes  is an orchestration platform to manage
any containerized application
A Kubernetes  cluster consists of a single master node  and potentially
multiple corresponding worker nodes
The beneﬁts of Kubernetes :
horizontal scaling,
automated rollouts and rollbacks,
service discovery and load balancing,
storage orchestration,
self healing,
batch execution,
automatic binpacking-
-
-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 27/28
================================================================================
PAGE 28 of 28
================================================================================

References
White T., Hadoop The Deﬁnitive Guide: Storage and analysis at Internet
scale, O'Reilly, 2015 (Available through UOW library)
Vohra D., Practical Hadoop ecosystem: a deﬁnitive guide to Hadoop-
related frameworks and tools, Apress, 2016 (Available through UOW
library)
Aven J., Hadoop in 24 Hours, SAMS Teach Yourself, SAMS 2017
Alapati S. R., Expert Hadoop Adiministration: Managing, Tuning, and
Securing Spark, YARN and HDFS, Addison-Wesley 2017
TOP               ISIT312 Big Data Management, SIM S4 2025 28/28
================================================================================


################################################################################
FILE: 02mapreduceframework.pdf
################################################################################

================================================================================
PAGE 1 of 12
================================================================================

  ISIT312 Big Data Management
MapReduce Framework
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 12
================================================================================

MapReduce Framework
Outline
MapReduce
Real world scenario: log data analysis
MapReduce implementation in Hadoop
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/12
================================================================================
PAGE 3 of 12
================================================================================

MapReduce
MapReduce  is the most important processing framework in Hadoop
Many high-level data processing languages are abstractions of
MapReduce, e.g. Pig and Hive or are heavily inﬂuenced by MapReduce
concepts e.g. Spark
Historically, Hadoop  version 1 supported MapReduce  only
MapReduce  is also a platform and language-independent programming
model at the heart of most big data and NoSQL platforms
A programming model means a pattern/format in accordance to which
we write our programs
The logic of a MapReduce  application consists of a Map phase and a
Reduce  phase
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/12
================================================================================
PAGE 4 of 12
================================================================================

MapReduce
Limitations of early distributed computing and grid computing
frameworks:
Complexity in parallel programming
Hardware failures
Bottlenecks in data exchange
Scalability problem
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/12
================================================================================
PAGE 5 of 12
================================================================================

MapReduce
The 2004 Google MapReduce white papers determined the following
design goals of MapReduce
Automatic parallelization and distribution
Fault tolerance
Input/output (I/O) scheduling
Status and monitoring-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/12
================================================================================
PAGE 6 of 12
================================================================================

MapReduce
MapReduce  model uses key-value  pairs for processing data
WordCount: MapReduce Hello World example
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/12
================================================================================
PAGE 7 of 12
================================================================================

MapReduce Framework
Outline
MapReduce
Real world scenario: log data analysis
MapReduce implementation in Hadoop
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/12
================================================================================
PAGE 8 of 12
================================================================================

A Real-World scenario: log data analysis
In online purchasing, users sometimes abandon their shopping carts
before completing the purchase
In order to improve their business, companies are usually interested to
ﬁnd out more about the nature of these abandoned purchases
A MapReduce job for this analysis
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/12
================================================================================
PAGE 9 of 12
================================================================================

MapReduce Framework
Outline
MapReduce
Real world scenario: log data analysis
MapReduce implementation in Hadoop
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/12
================================================================================
PAGE 10 of 12
================================================================================

MapReduce implementation in Hadoop
Hadoop MapReduce frees the users from the low-level communication
and coordination of nodes and processes
Let programmers focus on the MapReduce implementation and a few
conﬁguration parameters
As the data ﬁle is usually too large to be stored in a single persistent
storage device (of the commodity hardware), Hadoop handles the
shipment of code to data fragments (aka, data locality)
This can dramatically reduce the overhead of network transmits
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/12
================================================================================
PAGE 11 of 12
================================================================================

MapReduce implementation in Hadoop
Why Hadoop is useful to Big Data ?
Cost-e!ective fault-tolerant storage (HDFS)
Scalability
Data that is ingested may be interpreted at runtime
Low cost in storing unstructured and semi-structured data
Fast transfer of data into storage
Separation of programming logic and scheduling/management
Multiple levels of distributed system abstractions: Hive, Pig, Spark
Multi-language tooling: Java: MapReduce; SQL: Hive; data-ﬂow: Pig; Scala,
Python: Spark;-
-
-
-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/12
================================================================================
PAGE 12 of 12
================================================================================

References
White T., Hadoop The Deﬁnitive Guide: Storage and analysis at Internet
scale, O'Reilly, 2015 (Available through UOW library)
Vohra D., Practical Hadoop ecosystem: a deﬁnitive guide to Hadoop-
related frameworks and tools, Apress, 2016 (Available through UOW
library)
Aven J., Hadoop in 24 Hours, SAMS Teach Yourself, SAMS 2017
Alapati S. R., Expert Hadoop Adiministration: Managing, tuning, and
securing Spark, YARN and HDFS, Addison-Wesley 2017
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/12
================================================================================


################################################################################
FILE: 03hadooparchitecture.pdf
################################################################################

================================================================================
PAGE 1 of 27
================================================================================

  ISIT312 Big Data Management
Hadoop Architecture
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 27
================================================================================

Hadoop Architecture
Outline
Hadoop Distributed File System (HDFS)
NameNode metadata
DataNode and Secondary node
Yet Another Resource Negotiator (YARN)
ResourceManger
NodeManager
ApplicationMaster
Summary
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/27
================================================================================
PAGE 3 of 27
================================================================================

HDFS: Hadoop Distributed File System
HDFS is designed for:
But not for:Very large ﬁles
Stream data access
Commodity hardware-
-
-
Low-latency data access
Lots of small ﬁles
Multiple writers, arbitrary ﬁle modiﬁcations-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/27
================================================================================
PAGE 4 of 27
================================================================================

HDFS: Hadoop Distributed File System
HDFS contains the following key components:
NameNode :
SecondaryNameNode  and Standby NameNode
DataNodeHDFS master node process
manages the ﬁlesystem metadata
does not store a ﬁle itself-
-
-
SecondaryNameNode  expedites the ﬁlesystem metadata recovery
Standby NameNode  (optional) provides high availability-
-
runs HDFS slave node process
manages block storage and access for reading or writing of data, block
replication-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/27
================================================================================
PAGE 5 of 27
================================================================================

HDFS: Hadoop Distributed File System
Architecture of HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/27
================================================================================
PAGE 6 of 27
================================================================================

HDFS: Hadoop Distributed File System
HDFS is a virtual ﬁlesystem
Each ﬁle in HDFS consists of blocksappears to a client as one ﬁle system, but the data is stored in multiple di!erent
locations
deployed on the top of the native ﬁlesystems (such as ext3 , ext4  and xfs  in
Linux)-
-
The size of each block defaults to 128MB but is conﬁgurable
The default number of replicates for blocks is 3, but it is also conﬁgurable-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/27
================================================================================
PAGE 7 of 27
================================================================================

HDFS: Hadoop Distributed File System
Logical view of data storage
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/27
================================================================================
PAGE 8 of 27
================================================================================

HDFS: Hadoop Distributed File Systeme
Physical implementation of data ﬁle storage
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/27
================================================================================
PAGE 9 of 27
================================================================================

Hadoop Architecture
Outline
Hadoop Distributed File System (HDFS)
NameNode metadata
DataNode and Secondary node
Yet Another Resource Negotiator (YARN)
ResourceManger
NodeManager
ApplicationMaster
Summary
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/27
================================================================================
PAGE 10 of 27
================================================================================

NameNode Metadata
NameNode  stores the metadata of the ﬁles in HDFS
NameNode  functions:
Maintain the metadata pertaining to the ﬁle system (e.g., the ﬁle hierarchy and
the block locations for each ﬁle)
Manage user access to the data ﬁles
Map the data blocks to the DataNodes in the cluster
Perform ﬁle system operations (e.g., opening and closing the ﬁles and
directories)
Provide registration services and periodic heartbeats for DataNodes-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/27
================================================================================
PAGE 11 of 27
================================================================================

Hadoop Architecture
Outline
Hadoop Distributed File System (HDFS)
NameNode metadata
DataNode and Secondary node
Yet Another Resource Negotiator (YARN)
ResourceManger
NodeManager
ApplicationMaster
Summary
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/27
================================================================================
PAGE 12 of 27
================================================================================

DataNode and Secondary node
DataNode functions:
Secondary NameNode  and Standby NameNode  functions:Provide the block storage by storing blocks on the local ﬁle system
Fulﬁl the read/write requests
Replicating data across the cluster
Keeping in touch with the NameNode  by sending periodic block reports and
heartbeats
A heartbeat conﬁrms the DataNode is alive and healthy, and a block report
shows the blocks being managed by the DataNode-
-
-
-
-
Without a NameNode , there is no way to know to which ﬁles the blocks stored
on the DataNodes correspond to
In essence, all ﬁles in HDFS are lost
Secondary NameNode  periodically backups the metadata in the (primary)
NameNode , which is usually for recovery
Standby NameNode  is a hot node that running together with the (primary)
NameNode  in the cluster, facilitating high-availability-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/27
================================================================================
PAGE 13 of 27
================================================================================

Hadoop Architecture
Outline
Hadoop Distributed File System (HDFS)
NameNode metadata
DataNode and Secondary node
Yet Another Resource Negotiator (YARN)
ResourceManger
NodeManager
ApplicationMaster
Summary
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/27
================================================================================
PAGE 14 of 27
================================================================================

Yet Another Resource Negotiator (YARN)
YARN: the core subsystem in Hadoop responsible for governing,
allocating, and managing the ﬁnite distributed processing resources
available on a Hadoop cluster
YARN provides its core services via two types of long-running daemons:introduced in Hadoop 2 to improve the MapReduce implementation, but
general enough to support other distributed computing paradigms-
A ResourceManager  (one per cluster) to manage the use of resources across
the cluster, and
NodeManagers  running on all the nodes in the cluster to launch and monitor
containers-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/27
================================================================================
PAGE 15 of 27
================================================================================

Yet Another Resource Negotiator (YARN)
Architecture of YARN
A client is the program that submits jobs to the cluster
A job, also called an application, contains one or more tasks
Each mapper  and reducer task runs within a containerMay also be the gateway machine that the client program runs on-
A task in a MapReduce job can be either a mapper  and a reducer task -
Containers  are logical constructs that represent a speciﬁc amount of memory
and other resources, such as processing cores (CPU)
For example, a container can represent 2GB memory and 2 processing cores
Containers  may also refer to the running environment of an application-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 15/27
================================================================================
PAGE 16 of 27
================================================================================

Yet Another Resource Negotiator (YARN)
Architecture of YARN
ResourceManager : YARN's daemon running on a master node
NodeManager: YARN's daemon running on a slave node.ResourceManager  is responsible for granting cluster computing resources to
applications running on the cluster
Resources are granted the items of containers-
-
NodeManager manages containers on a slave node
ApplicationMaster: the ﬁrst container allocated by the ResourceManager  to run
on a NodeManager for each application-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/27
================================================================================
PAGE 17 of 27
================================================================================

Yet Another Resource Negotiator (YARN)
Architecture of YARN
TOP                    ISIT312 Big Data Management, SIM S4 2025 17/27
================================================================================
PAGE 18 of 27
================================================================================

Yet Another Resource Negotiator (YARN)
Architecture of YARN
TOP                    ISIT312 Big Data Management, SIM S4 2025 18/27
================================================================================
PAGE 19 of 27
================================================================================

Hadoop Architecture
Outline
Hadoop Distributed File System (HDFS)
NameNode metadata
DataNode and Secondary node
Yet Another Resource Negotiator (YARN)
ResourceManger
NodeManager
ApplicationMaster
Summary
TOP                    ISIT312 Big Data Management, SIM S4 2025 19/27
================================================================================
PAGE 20 of 27
================================================================================

ResourceManager
There is one ResourceManager  per cluster, which consists of two key
components: Scheduler and ApplicationManager
Key functions of ResourceManager :
The role of ResourceManager  is pure management and scheduler
It does not perform any actual data processing, for example the Map
and Reduce  functions in a MapReduce applicationCreates the ﬁrst container for an application to run ApplicationMaster for that
application
Tracks the heartbeats from NodeManagers  to manage DataNodes
Runs Scheduler to determine resource allocation among the clusters
Manages cluster level security
Manages the resource requests from ApplicationMasters
Monitors the status of ApplicationMasters and restarts that container upon its
failure
Deallocates the containers when the application completes or after they expire-
-
-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 20/27
================================================================================
PAGE 21 of 27
================================================================================

Hadoop Architecture
Outline
Hadoop Distributed File System (HDFS)
NameNode metadata
DataNode and Secondary node
Yet Another Resource Negotiator (YARN)
ResourceManger
NodeManager
ApplicationMaster
Summary
TOP                    ISIT312 Big Data Management, SIM S4 2025 21/27
================================================================================
PAGE 22 of 27
================================================================================

NodeManager
Each DataNode runs a NodeManager daemon for performing YARN
functions
Main functions of a NodeManager daemon:
Communicates with ResourceManager  through health heartbeats and
container status notiﬁcations.
Registers and starts the application processes
Launches both ApplicationMaster and the rest of an application's resource
containers (that is, the map and reduce tasks that run in the containers) on
request from ApplicationMaster
Oversees the lifecycle of the application containers
Monitors, manages and provides information regarding the resource
consumption (CPU/memory) by the containers
Tracks the health of DataNode
Provides auxiliary services to YARN applications, such as services used by the
MapReduce framework for its shu"e and sort operations-
-
-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 22/27
================================================================================
PAGE 23 of 27
================================================================================

Hadoop Architecture
Outline
Hadoop Distributed File System (HDFS)
NameNode metadata
DataNode and Secondary node
Yet Another Resource Negotiator (YARN)
ResourceManger
NodeManager
ApplicationMaster
Summary
TOP                    ISIT312 Big Data Management, SIM S4 2025 23/27
================================================================================
PAGE 24 of 27
================================================================================

ApplicationMaster
For each YARN application, there is a dedicated ApplicationMaster
Functions of ApplicationMaster:
ApplicationMaster is running within a container
ApplicationMaster's existence is associated with the running application
When an application is completed, its ApplicationMaster no longer exists
Once created, ApplicationMaster is in charge of requesting resources
with ResourceManager  to run the application
The resource request are very speciﬁc, for example:Managing task scheduling and execution
Allocating resources locally for the application's tasks-
-
the ﬁle blocks needed to process the job,
the amount of the resource, in terms of the number of containers to create for
the application,
the size of the containers, etc.-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 24/27
================================================================================
PAGE 25 of 27
================================================================================

Hadoop Architecture
Outline
Hadoop Distributed File System (HDFS)
NameNode metadata
DataNode and Secondary node
Yet Another Resource Negotiator (YARN)
ResourceManger
NodeManager
ApplicationMaster
Summary
TOP                    ISIT312 Big Data Management, SIM S4 2025 25/27
================================================================================
PAGE 26 of 27
================================================================================

Summary
Terminologies
Hadoop  is a leading platform for big data
Hadoop consists of a storage layer ( HDFS), a coordination and
management layer (YARN) and a processing layer (e.g., MapReduce )
HDFS and YARN have key services (daemons)
MapReduce  is a fundamental computing model (i.e., batch processing)
for big data
Next: Interaction with Hadoop  and "dive" into the MapReduce
framework
TOP                    ISIT312 Big Data Management, SIM S4 2025 26/27
================================================================================
PAGE 27 of 27
================================================================================

References
White T., Hadoop The Deﬁnitive Guide: Storage and analysis at Internet
scale, O'Reilly, 2015 (Available through UOW library)
Vohra D., Practical Hadoop ecosystem: a deﬁnitive guide to Hadoop-
related frameworks and tools, Apress, 2016 (Available through UOW
library)
Aven J., Hadoop in 24 Hours, SAMS Teach Yourself, SAMS 2017
Alapati S. R., Expert Hadoop Adiministration: Managing, tuning, and
securing Spark, YARN and HDFS, Addison-Wesley 2017
TOP                    ISIT312 Big Data Management, SIM S4 2025 27/27
================================================================================


################################################################################
FILE: 04hdfsinterfaces.pdf
################################################################################

================================================================================
PAGE 1 of 28
================================================================================

  ISIT312 Big Data Management
HDFS Interfaces
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 28
================================================================================

HDFS Interfaces
Outline
Hadoop Cluster vs. Pseudo-Distributed Hadoop
Shell Interface to HDFS
Web Interface to HDFS
Java Interface to HDFS
Internals of HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/28
================================================================================
PAGE 3 of 28
================================================================================

Hadoop Cluster vs. Pseudo-Distributed Hadoop
A Hadoop  cluster is deployed in a cluster of computer nodes
Hadoop  provides a pseudo-distributed mode on a single machine
HDFS provides the following interfaces to read, write, interrogate, and
manage the ﬁlesystemAs Hadoop  is developed in Java, all Hadoop  services sit on Java Virtual Machines
running on the cluster nodes-
All Java Virtual Machines for necessary Hadoop  services are running on a single
machine
In our case this machine is a Virtual Machine running under Ubuntu 14.04-
-
The ﬁle system shell (Command-Line Interface): hadoop fs  or hdfs dfs
Hadoop  Filesystem Java API
Hadoop  simple Web User Interface
Other interfaces, such as RESTful proxy interfaces (e.g.,HttpFS)-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/28
================================================================================
PAGE 4 of 28
================================================================================

HDFS Interfaces
Outline
Hadoop Cluster vs. Pseudo-Distributed Hadoop
Shell Interface to HDFS
Web Interface to HDFS
Java Interface to HDFS
Internals of HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/28
================================================================================
PAGE 5 of 28
================================================================================

Shell Interface to HDFS
Commands are provided in the shell Bash
Hadoop's home directory
You will mostly use scripts in the bin and sbin folders, and use jar ﬁles
in the share folder
Hadoop Daemons
Hadoop is running properly only if the above services are running
$ which bash
/bin/bash
Bash shell
$ cd $HADOOP_HOME
$ ls
bin include libexec logs README.txt share
etc lib LICENSE.txt NOTICE.txt sbin
Home of Hadoop
$ jps
28530 SecondaryNameNode
11188 NodeManager
28133 NameNode
28311 DataNode
10845 ResourceManager
3542 Jps
Hadoop daemons
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/28
================================================================================
PAGE 6 of 28
================================================================================

Shell Interface to HDFS
Create a HDFS user account (already created in a virtual machine used by
us)
Create an folder input
View the folders in Hadoop  home
Upload a ﬁle to HDFS
Read a ﬁle in HDFS
$ bin/hadoop fs -mkdir -p /user/bigdata
 Creating home of user account
$ bin/hadoop fs -mkdir input
 Creating a folder
$ bin/hadoop fs -ls
Found 1 item
drwxr-xr-x - bigdata supergroup 0 2017-07-17 16:33 input
Listing home of user account
$ bin/hadoop fs -put README.txt input
$ bin/hadoop fs -ls input
-rw-r--r-- 1 bigdata supergroup 1494 2017-07-12 17:53 input/README.txt
Uploading a file
$ bin/hadoop fs -cat input/README.txt
<contents of README.txt goes here>
Listing a file
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/28
================================================================================
PAGE 7 of 28
================================================================================

Shell Interface to HDFS
The path in HDFS is represented as a URI with the preﬁx hdfs://
For example
When interacting with HDFS interface in the default setting, one can
omitt IP, port, and user, and simply mention the directory or ﬁle
Thus, the full-spelling of hadoop fs -ls input is
hadoop fs -ls hdfs://<hostname>:
<port>/user/bigdata/inputhdfs://<hostname>:<port>/user/bigdata/input  refers to the
input  directory in HDFS under the user of bigdata
hdfs ://<hostname>:
<port>/user/bigdata/input/README.txt  refers to the ﬁle
README.txt  in the above input  directory in HDFS-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/28
================================================================================
PAGE 8 of 28
================================================================================

Shell Interface to HDFS
Some of frequently used commands
Command          Description
-put             Upload a file ( or files)  from the local filesystem to HDFS
-mkdir           Create a directory in HDFS
-ls              List the files in a directory in HDFS
-cat             Read the content of a file ( or files)  in HDFS
-copyFromLocal   Copy a file from the local filesystem to HDFS ( similar to
                 put)
-copyToLocal     Copy a file ( or files)  from HDFS to the local filesystem
-rm              Delete a file ( or files)  in HDFS
-rm -r           Delete a directory in HDFS
Commands of Hadoop shell interface
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/28
================================================================================
PAGE 9 of 28
================================================================================

HDFS Interfaces
Outline
Hadoop Cluster vs. Pseudo-Distributed Hadoop
Shell Interface to HDFS
Web Interface to HDFS
Java Interface to HDFS
Internals of HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/28
================================================================================
PAGE 10 of 28
================================================================================

Web Interface of HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/28
================================================================================
PAGE 11 of 28
================================================================================

Web Interface of HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/28
================================================================================
PAGE 12 of 28
================================================================================

HDFS Interfaces
Outline
Hadoop Cluster vs. Pseudo-Distributed Hadoop
Shell Interface to HDFS
Web Interface to HDFS
Java Interface to HDFS
Internals of HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/28
================================================================================
PAGE 13 of 28
================================================================================

Java Interface to HDFS
A ﬁle in a Hadoop  ﬁlesystem is represented by a Hadoop Path  object
To get an instance of FileSystem, use the following factory methods
The following method gets a local ﬁlesystem instanceIts syntax is URI
For example,
hdfs://localhost:8020/user/bigdata/input/README.txt-
-
public static  FileSystem get(Configuration conf)   throws  IOException
public static  FileSystem get(URI uri, Configuration conf)   throws IOException
public static  FileSystem get(URI uri, Configuration conf,  String user)
                                            throws IOException
Factory methods
public static  FileSystem getLocal( Configuration conf)   throws  IOException
Get local file system method
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/28
================================================================================
PAGE 14 of 28
================================================================================

Java interface to HDFS
A Configuration object is determined by the Hadoop  conﬁguration
ﬁles or user-provided parameters
Using the default conﬁguration, one can simply set
With a FileSystem instance in hand, we invoke an open() method to
get the input stream for a ﬁle
A Path object can be created by using a designated URI
Configuration conf =   new Configuration()
Configuration object
public FSDataInputStream open( Path f)  throws IOException
public abstract  FSDataInputStream open( Path f, int bufferSize)   throws IOException
Open method
    
Path f =  new  Path( uri)
Path object
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/28
================================================================================
PAGE 15 of 28
================================================================================

Java interface to HDFS
Putting together, we can create the following ﬁle reading application
    public class FileSystemCat {
       public static void main( String[] args)  throws Exception {
           String uri =  args[0];
           Configuration conf =  new Configuration();
           FileSystem fs =  FileSystem. get(URI.create(uri), conf);
           FSDataInputStream in = null;
           Path path =  new Path(uri);
           in = fs.open(path);
           IOUtils. copyBytes( in, System.out, 4096, true);
       }
    }
Class FileSystemCat
TOP                    ISIT312 Big Data Management, SIM S4 2025 15/28
================================================================================
PAGE 16 of 28
================================================================================

Java interface to HDFS
The compilation simply uses the javac command, but it needs to point
the dependencies in the class path.
Then, a jar ﬁle is created and run as follows
The output is the same as processing a command hadoop fs -cat
export HADOOP_CLASSPATH= $($HADOOP_HOME/ bin/hadoop classpath)
javac -cp $HADOOP_CLASSPATH FileSystemCat. java
Compilation
jar cvf FileSystemCat. jar FileSystemCat*.class
hadoop jar FileSystemCat. jar FileSystemcat input/ README.txt
jar file and processing
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/28
================================================================================
PAGE 17 of 28
================================================================================

Java interface to HDFS
Suppose an input stream is created to read a local ﬁle
To write a ﬁle on HDFS, the simplest way is to take a Path object for the
ﬁle to be created and return an output stream to write to
And then just copy the input stream to the output stream
Another, more ﬂexible, way is to read the input stream into a bu!er and
then write to the output stream
    
public  FSDataOutputStream create( Path f)  throws  IOException
Path object
TOP                    ISIT312 Big Data Management, SIM S4 2025 17/28
================================================================================
PAGE 18 of 28
================================================================================

Java interface to HDFS
A ﬁle writing application
public class FileSystemPut {
   public static void main( String[] args)  throws Exception {
      String localStr =  args[0];
      String hdfsStr =  args[1];
      Configuration conf =  new Configuration();
      FileSystem local = FileSystem. getLocal( conf);
      FileSystem hdfs =  FileSystem. get(URI.create(hdfsStr), conf);
      Path localFile =  new Path(localStr);
      Path hdfsFile =  new Path(hdfsStr);
      FSDataInputStream in = local.open(localFile);
      FSDataOutputStream out = hdfs.create(hdfsFile);
      IOUtils. copyBytes( in, out, 4096, true);
    }
  }
File writing
TOP                    ISIT312 Big Data Management, SIM S4 2025 18/28
================================================================================
PAGE 19 of 28
================================================================================

Java interface to HDFS
Another ﬁle writing application
public class FileSystemPutAlt {
    public static void main( String[] args)  throws Exception {
      String localStr =  args[0];
      String hdfsStr =  args[1];
      Configuration conf =  new Configuration();
      FileSystem local = FileSystem. getLocal( conf);
      FileSystem hdfs =  FileSystem. get(URI.create(hdfsStr), conf);
      Path localFile =  new Path(localStr);
      Path hdfsFile =  new Path(hdfsStr);
      FSDataInputStream in = local.open(localFile);
      FSDataOutputStream out = hdfs.create(hdfsFile);
      byte[] buffer =  new byte[256];
      int bytesRead =  0;
      while(  (bytesRead = in.read(buffer)) > 0) {
        out. write(buffer, 0, bytesRead);
      }
      in. close();
      out. close();
     }
   }
File writing
TOP                    ISIT312 Big Data Management, SIM S4 2025 19/28
================================================================================
PAGE 20 of 28
================================================================================

Java interface to HDFS
Other ﬁle system API methods
The method mkdirs() creates a directory
The method getFileStatus() gets the meta information for a single
ﬁle or directory
The method listStatus() lists contents of ﬁles in a directory
The method exists() checks whether a ﬁle exists
The method delete() removes a ﬁle
The Java API enables the implementation of customised applications to
interact with HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 20/28
================================================================================
PAGE 21 of 28
================================================================================

HDFS Interfaces
Outline
Hadoop Cluster vs. Pseudo-Distributed Hadoop
Shell Interface to HDFS
Web Interface to HDFS
Java Interface to HDFS
Internals of HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 21/28
================================================================================
PAGE 22 of 28
================================================================================

Internals of HDFS
What happens "inside" when we read data into HDFS ?
TOP                    ISIT312 Big Data Management, SIM S4 2025 22/28
================================================================================
PAGE 23 of 28
================================================================================

Internals of HDFS
Read data from HDFS
Step 1: The client opens the ﬁle it wishes to read by calling open() on
the FileSystem object, which for HDFS is an instance of
DistributedFileSystem
Step 2: DistributedFileSystem calls the namenode, using remote
procedure calls (RPCs), to determine the locations of the ﬁrst few blocks
in the ﬁle
Step 3: The DistributedFileSystem returns an FSDataInputStream
to the client and the client calls read() on the stream
TOP                    ISIT312 Big Data Management, SIM S4 2025 23/28
================================================================================
PAGE 24 of 28
================================================================================

Internals of HDFS
Step 4: FSDataInputStream connects to the ﬁrst datanode for the ﬁrst
block in the ﬁle, and then data is streamed from the datanode back to
the client, by calling read() repeatedly on the stream
Step 5: When the end of the block is reached, FSDataInputStream will
close the connection to the datanode, then ﬁnd the best (possibly the
same) datanode for the next block
Step 6: When the client has ﬁnished reading, it calls close() on the
FSDataInputStream
TOP                    ISIT312 Big Data Management, SIM S4 2025 24/28
================================================================================
PAGE 25 of 28
================================================================================

Internals of HDFS
Write data into HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 25/28
================================================================================
PAGE 26 of 28
================================================================================

Internals of HDFS
Step 1: The client creates the ﬁle by calling create() on
DistributedFileSystem
Step 2: DistributedFileSystem makes an RPC call to the namenode
to create a new ﬁle in the ﬁle system namespace and returns an
FSDataOutputStream for the client to start writing data to
Step 3: The client writes data into the FSDataOutputStream
Step 4: Data wrapped by the FSDataOutputStream is split into
packages, which are ﬂushed into a queue; data packages are sent to the
blocks in a datanode and forwarded to other (usually two) datanodes
TOP                    ISIT312 Big Data Management, SIM S4 2025 26/28
================================================================================
PAGE 27 of 28
================================================================================

Internals of HDFS
Step 5: If FSDataStream receives an ack signal from the datanode the
data packages are removed from the queue
Step 6: When the client has ﬁnished writing data, it calls close() on the
stream
Step 7: The client signals the namenode that the writing is completed
TOP                    ISIT312 Big Data Management, SIM S4 2025 27/28
================================================================================
PAGE 28 of 28
================================================================================

References
Vohra D., Practical Hadoop ecosystem: a deﬁnitive guide to Hadoop-
related frameworks and tools, Apress, 2016 (Available through UOW
library)
Aven J., Hadoop in 24 Hours, SAMS Teach Yourself, SAMS 2017
TOP                    ISIT312 Big Data Management, SIM S4 2025 28/28
================================================================================


################################################################################
FILE: 05mapreducemodel.pdf
################################################################################

================================================================================
PAGE 1 of 31
================================================================================

  ISIT312 Big Data Management
MapReduce Data Processing
Model
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 31
================================================================================

MapReduce Data Processing Model
Outline
Key-value pairs
MapReduce model
Map phase
Reduce phase
Shu! e and sort
Combine phase
Example
Running MapReduce jobs
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/31
================================================================================
PAGE 3 of 31
================================================================================

Key-value pairs
Key-Value pairs : MapReduce  basic data model
Input, output, and intermediate records in MapReduce  are represented
as key-value pairs  (aka name-value/attribute-value pairs )
A key is an identiﬁer, for example, a name of attribute
A value  is a data associated with a key
Key                          Value
City                         Sydney
Employer                     Cloudera
sql
In MapReduce , a key is not required to be unique. -
It may be simple value  or a complex object -
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/31
================================================================================
PAGE 4 of 31
================================================================================

MapReduce Data Processing Model
Outline
Key-value pairs
MapReduce model
Map phase
Reduce phase
Shu! e and sort
Combine phase
Example
Running MapReduce jobs
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/31
================================================================================
PAGE 5 of 31
================================================================================

MapReduce Model
MapReduce data processing model is a sequence of Map, Partition,
Shu!e and Sort, and Reduce stages
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/31
================================================================================
PAGE 6 of 31
================================================================================

MapReduce Model
An abstract MapReduce program: WordCount
  
function Map(Long lineNo, String line):
  lineNo: the position no. of a line in the text
  line: a line of text
    for each word w in line:
      emit (w, 1)
Function Map
function Reduce(String w, List loc):
  w: a word
  loc: a list of counts outputted from map instances
    sum = 0
    for each c in loc:
      sum += c
    emit (word, sum)
           
Function Reduce
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/31
================================================================================
PAGE 7 of 31
================================================================================

MapReduce Model
A diagram of data processing in MapReduce  model
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/31
================================================================================
PAGE 8 of 31
================================================================================

MapReduce Data Processing Model
Outline
Key-value pairs
MapReduce model
Map phase
Reduce phase
Shu! e and sort
Combine phase
Example
Running MapReduce jobs
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/31
================================================================================
PAGE 9 of 31
================================================================================

Map phase
Map phase  uses input format and record reader functions to derive
records in the form of key-value pairs  for the input data
Map phase  applies a function or functions to each key-value  pair over a
portion of the dataset
Each Map task operates against one ﬁlesystem ( HDFS) block
In the diagram fragment, a Map task will call its map() function,
represented by M in the diagram, once for each record, or key-value
pair; for example, rec1, rec2, and so on.In the case of a dataset hosted in HDFS, this portion is usually called as a block
If there are n blocks of data in the input dataset, there will be at least n Map
tasks (also referred to as Mappers )-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/31
================================================================================
PAGE 10 of 31
================================================================================

Map phase
Each call of the map() function accepts one key-value pair and emits zero
or more key-value pairs
The emitted data from Mapper , also in the form of lists of key-value
pairs , will be subsequently processed in the Reduce phase
Di"erent Mappers  do not communicate or share data with each other
Common Map() functions include ﬁltering of speciﬁc keys, such as
ﬁltering log messages if you only wanted to count or analyse ERROR log
messages
Another example of Map() function would be to manipulate values, such
as a function that converts a text value to lowercase
map (in_key, in_value) -> list (intermediate_key, intermediate_value)
A call of Map() function
       
Map (k, v) = if (ERROR in v) then emit (k, v)
Sample Map() function
      
 Map (k, v) = emit (k, v.toLowercase ( ))
Sample Map() function
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/31
================================================================================
PAGE 11 of 31
================================================================================

Map phase
Partition function, or Partitioner , ensures each key and its list of values is
passed to one and only one Reduce  task or Reducer
The number of partitions is determined by the (default or user-deﬁned)
number of Reducers
Custom Partitioners  are developed for various practical purposes
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/31
================================================================================
PAGE 12 of 31
================================================================================

MapReduce Data Processing Model
Outline
Key-value pairs
MapReduce model
Map phase
Reduce phase
Shu! e and sort
Combine phase
Example
Running MapReduce jobs
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/31
================================================================================
PAGE 13 of 31
================================================================================

Reduce Phase
Input of the Reduce phase  is output of the Map phase  (via shu !e-and
sort)
Each Reduce task (or Reducer) executes a reduce() function for each
intermediate key and its list of associated intermediate values
The output from each reduce() function is zero or more key-values
Note that, in the reality, an output from Reducer may be an input to
another Map phase  in a complex multistage computational workﬂow
    
reduce (intermediate_key,  list (intermediate_value)) -> (out_key, out_value)
A call of Reduce() function
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/31
================================================================================
PAGE 14 of 31
================================================================================

Example of Reduce Functions
The simplest and most common reduce() function is the Sum Reducer,
which simply sums a list of values for each key
A count operation is as simple as summing a set of numbers
representing instances of the values you wish to count
Other examples of reduce() function are max() and average()
reduce (k, list ) =
  {
    sum =  0
    for int i in list  :
      sum +  = i
    emit ( k, sum)
  }
Sum reducer
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/31
================================================================================
PAGE 15 of 31
================================================================================

MapReduce Data Processing Model
Outline
Key-value pairs
MapReduce model
Map phase
Reduce phase
Shu! e and sort
Combine phase
Example
Running MapReduce jobs
TOP                    ISIT312 Big Data Management, SIM S4 2025 15/31
================================================================================
PAGE 16 of 31
================================================================================

Shuffle and Sort
Shu! e-and-sort  is the process where data are transferred from Mapper
to Reducer
The most important purpose of Shu! e-and-sort  is to minimise data
transmission through a network
In general, in Shu! e-and-Sort , the Mapper  output is sent to the target
Reduce task according to the partitioning functionIt is the heart of MapReduce  where the "magic" happens -
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/31
================================================================================
PAGE 17 of 31
================================================================================

MapReduce Data Processing Model
Outline
Key-value pairs
MapReduce model
Map phase
Reduce phase
Shu! e and sort
Combine phase
Example
Running MapReduce jobs
TOP                    ISIT312 Big Data Management, SIM S4 2025 17/31
================================================================================
PAGE 18 of 31
================================================================================

Combine phase
A structure of Combine phase
TOP                    ISIT312 Big Data Management, SIM S4 2025 18/31
================================================================================
PAGE 19 of 31
================================================================================

Combine phase
If the Reduce function is commutative  and associative  then it can be
performed before the Shu! e-and-Sort phase
In this case, the Reduce function is called a Combiner function
For example, sum and count is commutative  and associative , but
average is not
The use of a Combiner  can minimise the amount of data transferred to
Reduce phase  and in such a way reduce the network transmit overhead
A MapReduce  application may contain zero Reduce tasks
In this case, it is a Map-Only  application
Examples of Map-only MapReduce  jobs
ETL routines without data summarization, aggregation and reduction
File format conversion jobs
Image processing jobs-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 19/31
================================================================================
PAGE 20 of 31
================================================================================

Combine phase
Map-Only MapReduce
TOP                    ISIT312 Big Data Management, SIM S4 2025 20/31
================================================================================
PAGE 21 of 31
================================================================================

Combine phase
An election Analogy for MapReduce
TOP                    ISIT312 Big Data Management, SIM S4 2025 21/31
================================================================================
PAGE 22 of 31
================================================================================

MapReduce Data Processing Model
Outline
Key-value pairs
MapReduce model
Map phase
Reduce phase
Shu! e and sort
Combine phase
Example
Running MapReduce jobs
TOP                    ISIT312 Big Data Management, SIM S4 2025 22/31
================================================================================
PAGE 23 of 31
================================================================================

Example
For a database of 1 billion people, compute the average number of
social contacts a person has according to age
In SQL like language
If the records are stored in di"erent datanodes then in Map function is
the following
SELECT age,  AVG(contacts)
FROM social. person
GROUP BY age
SELECT statement
function Map is
      input:  integer K between 1  and 1000, representing a batch of 1
      million social. person records
      for each social. person record in the K- th batch do
        let Y be the person age
        let N be the number of contacts the person has
          produce one output record ( Y,(N,1))
      repeat
end function
Map function
TOP                    ISIT312 Big Data Management, SIM S4 2025 23/31
================================================================================
PAGE 24 of 31
================================================================================

Example
Then Reduce  function is the following
MapReduce  sends the codes to the location of each data batch (not the
other way around)
Question: the output from Map is multiple copies of (Y, (N, 1)), but
the input to Reduce  is (Y, (N, C)), so what ﬁlls the gap?
function Reduce is
       input:  age (in years)  Y
       for each input record ( Y,(N,C)) do
         Accumulate in S the sum of N* C
         Accumulate in C_new the sum of C
       repeat
       let A be S/ C_new
       produce one output record ( Y,(A,C_new ))
end function
Reduce function
TOP                    ISIT312 Big Data Management, SIM S4 2025 24/31
================================================================================
PAGE 25 of 31
================================================================================

Example
A MapReduce  application in Hadoop  is a Java implementation of the
MapReduce model for a speciﬁc problem, for example, word count
TOP                    ISIT312 Big Data Management, SIM S4 2025 25/31
================================================================================
PAGE 26 of 31
================================================================================

Example
Sample processing on a screen
TOP                    ISIT312 Big Data Management, SIM S4 2025 26/31
================================================================================
PAGE 27 of 31
================================================================================

Example
Sample processing on a screen
TOP                    ISIT312 Big Data Management, SIM S4 2025 27/31
================================================================================
PAGE 28 of 31
================================================================================

MapReduce Data Processing Model
Outline
Key-value pairs
MapReduce model
Map phase
Reduce phase
Shu! e and sort
Combine phase
Example
Running MapReduce jobs
TOP                    ISIT312 Big Data Management, SIM S4 2025 28/31
================================================================================
PAGE 29 of 31
================================================================================

Running MapReduce Jobs
Client submits Mapreduce job
YARN resource manager coordinates the allocation of computing
resources in the cluster
YARN node manager(s): launch & monitor containers on machines in the
cluster
TOP                    ISIT312 Big Data Management, SIM S4 2025 29/31
================================================================================
PAGE 30 of 31
================================================================================

Running MapReduce Jobs
MapReduce application master runs in a container, and coordinates the
tasks in a MapReduce job
HDFS is used for sharing job ﬁles between the other ﬁles
TOP                    ISIT312 Big Data Management, SIM S4 2025 30/31
================================================================================
PAGE 31 of 31
================================================================================

References
White T., Hadoop The Deﬁnitive Guide: Storage and Analysis at Internet
Scale, O'Reilly, 2015
TOP                    ISIT312 Big Data Management, SIM S4 2025 31/31
================================================================================


################################################################################
FILE: 06javamapreduceapplication.pdf
################################################################################

================================================================================
PAGE 1 of 53
================================================================================

  ISIT312 Big Data Management
Java MapReduce Application
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 53
================================================================================

Java MapReduce Application
Outline
Building blocks of MapReduce program
Word Count: The "Hello, World" of MapReduce
Hadoop datatype objects
Input and output formats
Java code of Driver
Java code of Mapper
Java code of Reducer
Java code of ToolRunner
Setting up a local running environment
Combiner API
Partitioner API
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/53
================================================================================
PAGE 3 of 53
================================================================================

Building blocks of MapReduce program
Structure of MapReduce Java Application
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/53
================================================================================
PAGE 4 of 53
================================================================================

Building blocks of MapReduce program
Mapper , Reducer, Combiner , and Partitioner  classes correspond to their
counterparts in the MapReduce model
The Driver  or ToolRunner  in a MapReduce  program represents the client
programThese classes implement the MapReduce  logic -
The main method of a MapReduce program is in the Driver  or ToolRunner
The code of the two is very standard-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/53
================================================================================
PAGE 5 of 53
================================================================================

Building blocks of MapReduce program
An elementary MapReduce program consists only of a Mapper class , a
Reducer class  and a Driver
As the main method is contained in the Driver , sometimes (but not
always) it is convenient to make Mapper  and Reducer as inner classes in
Driver , which contains routine codes
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/53
================================================================================
PAGE 6 of 53
================================================================================

Building blocks of MapReduce program: Driver
Driver  is the program which sets up and starts a MapReduce  application
Driver  code is executed on the client; this code submits the application
to the ResourceManager  along with the application's conﬁguration
Driver  can submit the job asynchronously (in a non-blocking fashion) or
synchronously (waiting for the application to complete before
performing another action)– the second way will be used in our
examples
Driver  can also conﬁgure and submit more than one application; for
instance, running a workﬂow consisting of multiple MapReduce
applications
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/53
================================================================================
PAGE 7 of 53
================================================================================

Building blocks of MapReduce program: Mapper
Mapper  Java class contains a map() method
Its object instance iterates through the input to execute a map()
method, using the InputFormat  and its associated RecordReader
The number of HDFS blocks for the ﬁle determines the number of input
splits, which, in turn, determines the number of Mapper  objects (or Map
tasks) in a MapReduce  application (also see the previous lecture)
Mappers  do most of the heavy lifting in data processing in MapReduce ,
as they read the entire input ﬁle for the application
Mappers  can also include setup and cleanup code to run in any given
object lifespan
Setup is called before the map() method; cleanup is called after it
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/53
================================================================================
PAGE 8 of 53
================================================================================

Building blocks of MapReduce program: Reducer
Reducer runs against a partition and each key and its associated values
are passed to a reduce() method inside Reducer class
Reduce 's InputFormat matches Mapper 's OutputFormat
While Mapper  usually do the data preparation, for example, ﬁltering and
extracting), Reducer usually contains the main application logic
The runtime of Reducer instances is usually faster (and much faster in
some cases) than the runtime of Mapper  instancesFor example, summation, counting, and averaging operations are implemented
in Reducers-
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/53
================================================================================
PAGE 9 of 53
================================================================================

Java MapReduce Application
Outline
Building blocks of MapReduce program
Word Count: The "Hello, World" of MapReduce
Hadoop datatype objects
Input and output formats
Java code of Driver
Java code of Mapper
Java code of Reducer
Java code of ToolRunner
Setting up a local running environment
Combiner API
Partitioner API
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/53
================================================================================
PAGE 10 of 53
================================================================================

Word Count: The "Hello, World" of MapReduce
WordCount: Read a text ﬁle and count occurrences of each word
Consider a text document containing a fragment of the works of
Shakespeare
The input format is TextInputFormat
After the text is read, the input to Map task, i.e. the process running
Mapper  is the following
O Romeo, Romeo! wherefore art thou Romeo?
Deny thy father,  and refuse thy name
Romeo and Juliet
      
(0, 'O Romeo , Romeo ! wherefore art thou Romeo ?')
(45, 'Deny thy father, and refuse thy name')
Romeo and Juliet
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/53
================================================================================
PAGE 11 of 53
================================================================================

Word Count: The "Hello, World" of MapReduce
The output of the Map task is the following
Note that if you want to ﬁlter out some trivial words such as "a", "and", ...
then it can be done in a Mapper
 
    ('O', 1)
    ('Romeo', 1)
    ('Romeo', 1)
    ('wherefore', 1)
    ('art', 1)
    ('thou', 1)
    ('Romeo', 1)
    ('Deny', 1)
    ('thy', 1)
    ('father', 1)
    ('and', 1)
    ('refuse', 1)
    ('thy', 1)
    ('name', 1)
Key-Value pairs
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/53
================================================================================
PAGE 12 of 53
================================================================================

Word Count: The "Hello, World" of MapReduce
Before sending data to Reduce  task, there is a shu!e-and-sort stage
Shu! e-and-sort  is usually hidden from a programmer
The following is the input to Reduce task
     
    ('and', [1])
    ('art', [1])
    ('Deny', [1])
    ('father', [1])
    ('name', [1])
    ('O', [1])
    ('refuse', [1])
    ('Romeo', [1,1,1])
    ('thou', [1])
    ('thy', [1,1])
    ('wherefore', [1])
Key-value pairs after shuffle-and sort
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/53
================================================================================
PAGE 13 of 53
================================================================================

Word Count: The "Hello, World" of MapReduce
The following is the ﬁnal output from Reduce  task:
Note that we use plain texts to illustrate the data passing through the
MapReduce  stages, but in the Java implementation, all texts are
wrapped in some object that implements the Writable interface
 
  ('and', 1)
  ('art', 1)
  ('Deny', 1)
  ('father',  1)
  ('name', 1)
  ('O', 1)
  ('refuse',  1)
  ('Romeo', 3)
  ('thou', 1)
  ('thy', 2)
  ('wherefore',  1)
Final output from Reduce
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/53
================================================================================
PAGE 14 of 53
================================================================================

Java MapReduce Application
Outline
Building blocks of MapReduce program
Word Count: The "Hello, World" of MapReduce
Hadoop datatype objects
Input and output formats
Java code of Driver
Java code of Mapper
Java code of Reducer
Java code of ToolRunner
Setting up a local running environment
Combiner API
Partitioner API
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/53
================================================================================
PAGE 15 of 53
================================================================================

Hadoop datatype objects
Some imported package members
org.apache.hadoop.io.IntWritable contains a speciﬁcation of
Writable interface
 
import java. io.IOException;
import org. apache.hadoop.conf.Configuration;
import org. apache.hadoop.fs.Path;
import org. apache.hadoop.io.IntWritable;
import org. apache.hadoop.io.Text;
import org. apache.hadoop.mapreduce. Job;
import org. apache.hadoop.mapreduce. Mapper;
import org. apache.hadoop.mapreduce. Reducer;
import org. apache.hadoop.mapreduce. lib.input.FileInputFormat;
import org. apache.hadoop.mapreduce. lib.output.FileOutputFormat;
Imports
TOP                    ISIT312 Big Data Management, SIM S4 2025 15/53
================================================================================
PAGE 16 of 53
================================================================================

Hadoop data type objects
In most programming languages, when deﬁning most data elements, we
usually use simple, or primitive, datatypes such as int, long, or char
However, in Hadoop a key or a value is an object that is an instantiation
of a class, with attributes and deﬁned methods
A key or a value contains (or encapsulates) the data with methods
deﬁned for reading and writing data from and to the object
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/53
================================================================================
PAGE 17 of 53
================================================================================

Hadoop data type objects
Writable interface
Hadoop  serialisation format is Writable interface
For example, a class that implements Writable is IntWritable, which
a wrapper for a Java int
One can create such a class and set its value in the following way
Alternatively, we can write
 
IntWritable writable =  new IntWritable();
writable. set(163);
Creating IntWritable object
     
IntWritable writable =  new IntWritable( 163);
Creating IntWritable object
TOP                    ISIT312 Big Data Management, SIM S4 2025 17/53
================================================================================
PAGE 18 of 53
================================================================================

Hadoop data type objects
WritableComparable interface
IntWritable implements WritableComparable interface
It is interface of Writable and java.lang.Comparable interfaces
Comparison is crucial for MapReduce , because MapReduce  contains a
sorting phase during which keys are compared with one another
WritableComparable permits to compare records read from a stream
without deserialising them into objects, thereby avoiding any overhead
of object creation
 
package org. apache.hadoop.io;
public interface WritableComparable <T> extends Writable,
                                                Comparable< T> { ... }
Creating WritableComparable interface
TOP                    ISIT312 Big Data Management, SIM S4 2025 18/53
================================================================================
PAGE 19 of 53
================================================================================

Hadoop data type objects
Hadoop primitive Writable wrappers
 
Writable Wrapper                        Java Primitive
--------------------------------------------------------     
BooleanWritable                         boolean
ByteWritable                            byte
IntWritable                             int
FloatWritable                           float
LongWritable                            long
DoubleWritable                          double
NullWritable                            null
Text                                    String
Writable Wrappers
TOP                    ISIT312 Big Data Management, SIM S4 2025 19/53
================================================================================
PAGE 20 of 53
================================================================================

Java MapReduce Application
Outline
Building blocks of MapReduce program
Word Count: The "Hello, World" of MapReduce
Hadoop datatype objects
Input and output formats
Java code of Driver
Java code of Mapper
Java code of Reducer
Java code of ToolRunner
Setting up a local running environment
Combiner API
Partitioner API
TOP                    ISIT312 Big Data Management, SIM S4 2025 20/53
================================================================================
PAGE 21 of 53
================================================================================

Input and output formats
FileInputFormat (the base class of InputFormat) reads data (keys
and values) from a given path, using the default or user-deﬁned format
FileOutputFormat (the base class of OutputFormat) writes data into
a ﬁle in a given pathThe default input format is LongWritable  for the keys and Text  for the
values-
The output format is usually deﬁned by a programmer
For example, the output format is Text  for the keys and IntWritable  for
the values-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 21/53
================================================================================
PAGE 22 of 53
================================================================================

Java MapReduce Application
Outline
Building blocks of MapReduce program
Word Count: The "Hello, World" of MapReduce
Hadoop datatype objects
Input and output formats
Java code of Driver
Java code of Mapper
Java code of Reducer
Java code of ToolRunner
Setting up a local running environment
Combiner API
Partitioner API
TOP                    ISIT312 Big Data Management, SIM S4 2025 22/53
================================================================================
PAGE 23 of 53
================================================================================

Java code of Driver
 
public class WordCount {
  public static void main( String[] args)  throws Exception {
    Configuration conf =  new Configuration();
    Job job =  Job.getInstance( conf, "word count");
    job.setJarByClass( WordCount. class);
    job.setMapperClass( MyMapper. class);   //the Mapper class
    job.setReducerClass( MyReducer. class); //the Reducer class
    job.setOutputKeyClass( Text.class);
    job.setOutputValueClass( IntWritable. class);
    FileInputFormat. addInputPath( job, new Path(args[0]));
    FileOutputFormat. setOutputPath( job, new Path(args[1]));
    System. exit(job.waitForCompletion( true) ? 0 : 1);
  }
}
Java code of Driver
TOP                    ISIT312 Big Data Management, SIM S4 2025 23/53
================================================================================
PAGE 24 of 53
================================================================================

Java code of Driver
Job object and conﬁguration
Driver class instantiates a Job object
A Job object creates and stores the conﬁguration options for a Job,
including the classes to be used as Mapper and Reducer, input and
output directories, etc
The conﬁguration options are speciﬁed in (one or more of) the following
places
Hadoop  defaults (*-default.xml , e.g., core-default.xml )
A default conﬁguration is documented in the Apache Hadoop documentation
The *-site.xml  ﬁles on the client node where Driver  code is processed
The *-site.xml  ﬁles on the slave nodes where Mapper  runs on
Configuration  properties set at the command line as arguments to a
MapReduce  application (in a ToolRunner  object)
Configuration  properties set explicitly in code and compiled through a
Job  object-
-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 24/53
================================================================================
PAGE 25 of 53
================================================================================

Java code of Driver
Driver routines:
Parses the command line for positional arguments - input  ﬁle(s)/directory and
output  directory
Creates a new Job  object instance, using getConf()  method to obtain
conﬁguration from the various sources ( *-default.xml  and *-
site.xml )
Gives a Job  a friendly name (the name you will see in the ResourceManager UI)
Sets the InputFormat  and OutputFormat  for a Job  and determines the
input splits for a Job
Deﬁnes Mapper  and Reducer classes to be used for a Job (They must be
available in the Java classpath where Driver  is run - typically these classes are
packaged alongside the Driver)
Sets the ﬁnal output key and value classes, which will be written out ﬁles in the
output directory
Submits a Job  object through job.waitForCompletion(true)-
-
-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 25/53
================================================================================
PAGE 26 of 53
================================================================================

Java MapReduce Application
Outline
Building blocks of MapReduce program
Word Count: The "Hello, World" of MapReduce
Hadoop datatype objects
Input and output formats
Java code of Driver
Java code of Mapper
Java code of Reducer
Java code of ToolRunner
Setting up a local running environment
Combiner API
Partitioner API
TOP                    ISIT312 Big Data Management, SIM S4 2025 26/53
================================================================================
PAGE 27 of 53
================================================================================

Java code of Mapper
 
public static class MyMapper
extends Mapper{
   private final static IntWritable one =  new IntWritable( 1);
   private Text wordObject =  new Text();
   public void map( Object key,  Text value,  Context context
            )  throws IOException,  InterruptedException {
         String line =  value.toString();
         for (String word :  line.split("\\W+")) {
           if (word.length() > 0) {
                 wordObject. set(word);
                 context. write(wordObject,  one);
           }
         }
   }
}
Java code of Mapper
TOP                    ISIT312 Big Data Management, SIM S4 2025 27/53
================================================================================
PAGE 28 of 53
================================================================================

Java code of Mapper
MyMapper class
A class MyMapper extends a base Mapper class included within the
Hadoop  libraries
In the example, the four generics in
Mapper<Object, Text, Text, IntWritable>represent
<map_input_key, map_input_value, map_output_key,
map_output_value>
These generics must correspond to:
Key-value pair types as deﬁned by InputFormat  in Driver  (may be the
default one)
job.setMapOutputKeyClass  and
job.setMapOutputValueClass  deﬁned in Driver
Input and output to the map()  method-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 28/53
================================================================================
PAGE 29 of 53
================================================================================

Java code of Mapper
In map() method, before performing any functions against a key or a
value (such as split()), we need to get the value contained in the
serialised Writable or WritableComparable object, by using the
value.toString() method
After performing operations against the input data (key-value pairs), the
output data (intermediate data, also key-value pairs ) are
WritableComparable and Writable objects, both of which are
emitted using a Context object
In the case of a Map-only  job, the output from Map phase , namely the
set of key-value pairs  emitted from all map() methods in all map tasks,
is the ﬁnal output, without intermediate data or Shu! e-and-Sort phase
TOP                    ISIT312 Big Data Management, SIM S4 2025 29/53
================================================================================
PAGE 30 of 53
================================================================================

Java code of Mapper
Context object
A Context object is used to pass information between processes in
Hadoop
We mostly invoke its write() method to write the output data from
Mapper  and Reducer
Other functions of Context object are the following
It contains conﬁguration and state needed for processes within the MapReduce
application, including enabling parameters to be passed to distributed
processes
It is used in the optional setup()  and cleanup()  methods within a Mapper
or Reducer-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 30/53
================================================================================
PAGE 31 of 53
================================================================================

Java MapReduce Application
Outline
Building blocks of MapReduce program
Word Count: The "Hello, World" of MapReduce
Hadoop datatype objects
Input and output formats
Java code of Driver
Java code of Mapper
Java code of Reducer
Java code of ToolRunner
Setting up a local running environment
Combiner API
Partitioner API
TOP                    ISIT312 Big Data Management, SIM S4 2025 31/53
================================================================================
PAGE 32 of 53
================================================================================

Java code of Reducer
 
   public static class MyReducer
         extends Reducer {
   private IntWritable result =  new IntWritable();
   public void reduce( Text key,
      Iterable values,
      Context context)
               throws IOException,  InterruptedException {
         int sum =  0;
         for (IntWritable val :  values)
 {
             sum += val. get();
         }
         result. set(sum);
         context. write(key, result);
   }
}
Java code of Reducer
TOP                    ISIT312 Big Data Management, SIM S4 2025 32/53
================================================================================
PAGE 33 of 53
================================================================================

Java code of Reducer
MyReducer class
A class MyReducer extends the based Reducer class included with the
Hadoop  libraries.
The four generics in
Reducer<Text, IntWritable, Text, IntWritable> represents
<reduce_input_key, reduce_input_value,reduce_output_key,
reduce_output_value>
A reduce() method accepts a key and an Iterable list of values as
input
The values have type Iterable<T> (e.g., Iterable<IntWritable>)
TOP                    ISIT312 Big Data Management, SIM S4 2025 33/53
================================================================================
PAGE 34 of 53
================================================================================

Java code of Reducer
As in Mapper , to operate or perform Java string or numeric operations
against keys or values from the input list of values, we ﬁrst extract a
value included in Hadoop  object
Also, the emit of key-value pairs in the form of WritableComparable
objects for keys and values uses a Context object
The Reducer class implements the main application logic
For example, the actually counting of words is implemented in Reducer
TOP                    ISIT312 Big Data Management, SIM S4 2025 34/53
================================================================================
PAGE 35 of 53
================================================================================

Java code of Reducer
Data ﬂow of keys and values
TOP                    ISIT312 Big Data Management, SIM S4 2025 35/53
================================================================================
PAGE 36 of 53
================================================================================

Java MapReduce Application
Outline
Building blocks of MapReduce program
Word Count: The "Hello, World" of MapReduce
Hadoop datatype objects
Input and output formats
Java code of Driver
Java code of Mapper
Java code of Reducer
Java code of ToolRunner
Setting up a local running environment
Combiner API
Partitioner API
TOP                    ISIT312 Big Data Management, SIM S4 2025 36/53
================================================================================
PAGE 37 of 53
================================================================================

Java code of ToolRunner
Despite optional, Driver can leverage a class called ToolRunner, which
is used to parse command-line options
     
// ... Originally imported package members of WordCount
import org. apache.hadoop.util.Tool;
import org. apache.hadoop.util.ToolRunner;
import org. apache.hadoop.conf.Configured;
public class WordCountTR extends Configured implements Tool {
       public static void main( String[] args)
                          throws Exception {
          int res =  ToolRunner. run(new Configuration(), new WordCount(), args);
          System. exit(res);
       }
// "run" method of ToolRunner (in the next slide)
}
A class ToolRunner
TOP                    ISIT312 Big Data Management, SIM S4 2025 37/53
================================================================================
PAGE 38 of 53
================================================================================

Java code of ToolRunner
run method of ToolRunner
 
@Override
public int run(String[] args)  throws Exception {
  Configuration conf =  this.getConf();
  Job job =  Job.getInstance( conf, "word count with ToolRunner");
  job.setJarByClass( WordCountTR. class);
  job.setMapperClass( MyMapper. class); //the Mapper class
  job.setReducerClass( MyReducer. class); //the Reducer class
  job.setOutputKeyClass( Text.class);
  job.setOutputValueClass( IntWritable. class);
  FileInputFormat. addInputPath( job, new Path(args[0]));
  FileOutputFormat. setOutputPath( job, new Path(args[1]));
  return job. waitForCompletion( true) ? 0 : 1;
}
run method of ToolRunner
TOP                    ISIT312 Big Data Management, SIM S4 2025 38/53
================================================================================
PAGE 39 of 53
================================================================================

Java code of ToolRunner
ToolRunner in a command line
ToolRunner enables ﬂexibility in supplying conﬁguration parameters at
the command line when submitting a MapReduce  job
The following submission has a command to specify the number of
Reduce  tasks from the command line
The following submission speciﬁes the location of HDFS
 
hadoop jar mr. jar MyDriver -D mapreduce. job.reduces=10 myinputdir myoutputdir
ToolRunner in a command line
 
hadoop jar mr. jar MyDriver
       -D fs.defaultFS= hdfs://localhost:9000 myinputdir myoutputdir
ToolRunner in a command line
TOP                    ISIT312 Big Data Management, SIM S4 2025 39/53
================================================================================
PAGE 40 of 53
================================================================================

Java code of ToolRunner
ToolRunner options
 
Option                 Description
-D property=value      Sets the given Hadoop configuration property to the given
                       value. Overrides any default or site properties in the
                       configuration and any properties set via the -conf option.
-conf filename ...     Adds the given files to the list of resources in the
                       configuration. This is a convenient way to set site
                       properties or to set a number of properties at once.
-fs uri                Sets the default filesystem to the given URI. Shortcut for
                       -D fs.defaultFS=uri.
-jt host:port          Sets the YARN resource manager to the given host and
                       port. (In Hadoop 1, it sets the jobtracker address, hence
                       the option name.) Shortcut for 
                       -D yarn.resourcemanager.address=host:port.
ToolRunner options
TOP                    ISIT312 Big Data Management, SIM S4 2025 40/53
================================================================================
PAGE 41 of 53
================================================================================

Java code of ToolRunner
ToolRunner options
  
Option               Description
-files file1, file2, ...
                     Copies the specified files from the local filesystem (or any
                     filesystem if a scheme is specified) to the shared filesystem
                     used by MapReduce (e.g. HDFS) and makes them available
                     to MapReduce programs in the working directory of task.
-archives archive1, archive2, ...
                     Copies the specified archives from the local filesystem (or
                     any filesystem if a scheme is specified) to the shared
                     filesystem used by MapReduce (usually HDFS), unarchives
                     them, and makes them available to MapReduce programs
                     in the working directory of task.
-libjars jar1, jar2, ...
                     Copies the specified JAR files from the local filesystem (or
                     any filesystem if a scheme is specified) to the shared
                     filesystem used by MapReduce (usually HDFS) and adds
                     them to the MapReduce classpath of task. This option is a
                     useful way of shipping JAR files that a job is dependent on.
ToolRunner options
TOP                    ISIT312 Big Data Management, SIM S4 2025 41/53
================================================================================
PAGE 42 of 53
================================================================================

Java MapReduce Application
Outline
Building blocks of MapReduce program
Word Count: The "Hello, World" of MapReduce
Hadoop datatype objects
Input and output formats
Java code of Driver
Java code of Mapper
Java code of Reducer
Java code of ToolRunner
Setting up a local running environment
Combiner API
Partitioner API
TOP                    ISIT312 Big Data Management, SIM S4 2025 42/53
================================================================================
PAGE 43 of 53
================================================================================

Setting up a local running environment
A local running environment is often convenient in MapReduce
application development
Create a conﬁguration ﬁle, say, hadoop-local.xmlBefore sending it out to the whole cluster.
Used for debugging and testing-
-
<property>
  <name>fs.defaultFS</name>
  <value>file:///</value>
</property>
<property>
  <name>mapreduce.framework.name</name>
  <value>local</value>
</property>
hadoop-local.xml
hadoop jar mr. jar MyDriver -conf hadoop- local.xml myinputdir myoutputdir
Setting local environment
TOP                    ISIT312 Big Data Management, SIM S4 2025 43/53
================================================================================
PAGE 44 of 53
================================================================================

Java MapReduce Application
Outline
Building blocks of MapReduce program
Word Count: The "Hello, World" of MapReduce
Hadoop datatype objects
Input and output formats
Java code of Driver
Java code of Mapper
Java code of Reducer
Java code of ToolRunner
Setting up a local running environment
Combiner API
Partitioner API
TOP                    ISIT312 Big Data Management, SIM S4 2025 44/53
================================================================================
PAGE 45 of 53
================================================================================

Combiner API
Combiner functions can decrease the amount of intermediate data sent
between Mapper s and Reducers as part of a Shu!e-and-Sort process
One can reuse Reducer code to implement Combiner  ifIn a sense, Combiner  is the "map-side reducers" -
A combiner()  function is identical to a reduce()  function deﬁned in your
Reducer class
The output key and value object types from a map()  function implemented in
Mapper  match the input to the function used in Combiner
The output key and value object types from the function used in Combiner
match the input key and value object types used in Reducer's reduce()
method
The operation to be performed is commutative and associative.-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 45/53
================================================================================
PAGE 46 of 53
================================================================================

Java MapReduce Application
Outline
Building blocks of MapReduce program
Word Count: The "Hello, World" of MapReduce
Hadoop datatype objects
Input and output formats
Java code of Driver
Java code of Mapper
Java code of Reducer
Java code of ToolRunner
Setting up a local running environment
Combiner API
Partitioner API
TOP                    ISIT312 Big Data Management, SIM S4 2025 46/53
================================================================================
PAGE 47 of 53
================================================================================

Partitioner API
Partitioner  divides the output keyspace for a MapReduce  application,
controlling which Reducers get which intermediate data
A default Partitioner  is a HashPartitioner, which arbitrarily hashes
the key space such that
In case of one Reduce  task (default in pseudo-distributed mode), the
Partitioner  is "academic" because all intermediate data goes to the same
ReducerIt is useful in process distribution or load balancing, for example, getting more
Reduce  tasks running in parallel
It can be used to to segregate the outputs, for example, creating a ﬁle for
monthly data in a year-
-
the same keys go to the same Reducers and
the keyspace is distributed roughly equally among the number of Reducers
when determined by a programmer-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 47/53
================================================================================
PAGE 48 of 53
================================================================================

Partitioner API
Example: LetterPartitioner
 
// ... other imported package members
import org. apache.hadoop.mapreduce. Partitioner;
public static class LetterPartitioner
                extends Partitioner  {
        @Override
public int getPartition( Text key,
                        IntWritable value,
                        int numReduceTasks)  {
                 String word =  key.toString();
                 if (word.toLowerCase().matches( "^[a-m].*$")) {
//               if word starts with a to m, go to the first Reducer or partition
                    return 0;
                 }  else {
//                 else go to the second Reducer or partition
                   return 1;
                 }
        }
}
LetterPartitioner
TOP                    ISIT312 Big Data Management, SIM S4 2025 48/53
================================================================================
PAGE 49 of 53
================================================================================

Partitioner API
Declare Combiner and Partitioner in a Driver
 
public class WordCountWithLetPar {
  public static void main( String[] args)  throws Exception {
    ...
    job.setMapperClass( MyMapper. class);    // Mapper class
    job.setCombinerClass( MyReducer. class); // Combiner class, which is
                                              same as Reducer class in this program
    job.setPartitionerClass( MyPartitioner. class); // Partitioner class
    job.setReducerClass( MyReducer. class);  // Reducer class
    ...
  }
}
Declaring Combiner and Partitioner in a Driver
TOP                    ISIT312 Big Data Management, SIM S4 2025 49/53
================================================================================
PAGE 50 of 53
================================================================================

Java MapReduce Application
Outline
Building blocks of MapReduce program
Word Count: The "Hello, World" of MapReduce
Hadoop datatype objects
Input and output formats
Java code of Driver
Java code of Mapper
Java code of Reducer
Java code of ToolRunner
Setting up a local running environment
Combiner API
Partitioner API
TOP                    ISIT312 Big Data Management, SIM S4 2025 50/53
================================================================================
PAGE 51 of 53
================================================================================

Miscellaneous
Some points for Mappers & Reducers Implementation
No saving or sharing of state:  do not attempt to persist or share data between
Map tasks and between Reduce  tasks
No side e "ects: as Map and Reduce  tasks may be executed in any sequence and
may be executed more than once, any task must not create any side e "ects,
such as creating external events or triggering external processes
No attempt to communicate with other tasks:the Map tasks run in an
interleaving manner, and do the Reduce  tasks-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 51/53
================================================================================
PAGE 52 of 53
================================================================================

Miscellaneous
Compound MapReduce  jobs
How to turn a (possibly complex) data processing problem into the
MapReduce model ?
As a rule of thumb, for complex data processing problems, think about
adding more jobs, rather than adding the complexity to jobs
For example, to ﬁnd the mean of maximum daily temperature in each
month in di "erent weather stations , consider these two jobs:
A library ChainMapper allows to develop chained MapReduce JobsCompute the maximum daily temperature for every station-day key
Compute the mean of the maximum daily temperature for every station-day-
month key-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 52/53
================================================================================
PAGE 53 of 53
================================================================================

References
White T., Hadoop The Deﬁnitive Guide: Storage and Analysis at Internet
Scale, O'Reilly, 2015
TOP                    ISIT312 Big Data Management, SIM S4 2025 53/53
================================================================================


################################################################################
FILE: 07hive.pdf
################################################################################

================================================================================
PAGE 1 of 16
================================================================================

         ISIT312 Big Data Management
Hive
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 16
================================================================================

Hive
Outline
Hive ? What is it ?
Deployment and conﬁguration
Metastore
Interfaces
HQL
Hive versus relational DBMSs
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/16
================================================================================
PAGE 3 of 16
================================================================================

Hive ? What is it ?
Hive is a software system that provides tabular view of data stored in
HDFS and SQL-like methods for manipulating data in HDFS
Apache Hive project started at Facebook in 2010 to provide a high-level
interface to HDFS
Contrary to Pig, Hive provides SQL-like abstractions on top of
MapReduce
A language called HQL (Hive Query Language) implements SQL-92
standard (almost)
HQL provides a tabular view of data and it can be used to access data
located in HDFS
Hive frees data analysts from Java MapReduce  programming skills (not
completely)
HQL statements are parsed by the Hive client  and translated into a
sequence of Java MapReduce  operations, which are later on processed
by HadoopTOP                    ISIT312 Big Data Management, SIM S4 2025 3/16
================================================================================
PAGE 4 of 16
================================================================================

Hive ? What is it ?
The results of processing by Hadoop  are returned to the client or saved
in HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/16
================================================================================
PAGE 5 of 16
================================================================================

Hive
Outline
Hive ? What is it ?
Deployment and conﬁguration
Metastore
Interfaces
HQL
Hive versus relational DBMSs
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/16
================================================================================
PAGE 6 of 16
================================================================================

Deployment and Configuration
Hive is available on all of commercial distributions of Hadoop  and on
Hadoop installation on our virtual machine
A relational embedded database system Derby is used for
implementation of metastore
It is possible to use other relational database systems for
implementation of metastore  like for example MySQL
To use Hive Hadoop  and HDFS must be "up and running"
A top level view of data provided by Hive consists of databases  and
tables
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/16
================================================================================
PAGE 7 of 16
================================================================================

Hive
Outline
Hive ? What is it ?
Deployment and conﬁguration
Metastore
Interfaces
HQL
Hive versus relational DBMSs
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/16
================================================================================
PAGE 8 of 16
================================================================================

Metastore
Metastore  contains the mappings of tables to the directory locations in
HDFS
Metastore  is a relational database read and written by Hive client
Metastore  also includes the input and output formats  for the ﬁles
represented by the table objects, e.g. CSV InputFormat , etc, and SerDes
(Serialization/ Deserialization ) functions
Input and output formats for the ﬁles and functions are used by Hive to
extract records and ﬁelds from the ﬁles
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/16
================================================================================
PAGE 9 of 16
================================================================================

Metastore
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/16
================================================================================
PAGE 10 of 16
================================================================================

Hive
Outline
Hive ? What is it ?
Deployment and conﬁguration
Metastore
Interfaces
HQL
Hive versus relational DBMSs
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/16
================================================================================
PAGE 11 of 16
================================================================================

Interfaces
Hive provides Command Line Interface (CLI) that accepts and parses
HQL commands
Hive provides JDBC/ODBC connector (drivers) to work with other tools
such as:
Hive provides a storage handler mechanism to integrate with HBase
HUE (Hadoop User Experience) provides a uniﬁed web interface to HDFS
and Hive in an interactive environment
HCatalog  provides metadata management system for Hadoop, Pig , Hive,
and MapReducebeeline  (CLI),
Oracle SQL Developer (GUI),
Talend Open Studio  (data extraction, transformation, loading, and integration
tools),
Jasper reports, QlikView (business intelligence reporting tools ),
Microsoft Excel 2013 (data analysis tools), and Tableau  (data visualization tools)-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/16
================================================================================
PAGE 12 of 16
================================================================================

Hive
Outline
Hive ? What is it ?
Deployment and conﬁguration
Metastore
Interfaces
HQL
Hive versus relational DBMSs
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/16
================================================================================
PAGE 13 of 16
================================================================================

HQL
HQL consists of Data Deﬁnition Language, Data Selection and Scope
Language, Data Manipulation Language , and Data Aggregation and
Sampling Language
Data Deﬁnition Language  is used for creating, deleting, and altering
schema objects like database tables, views, partitions , and buckets
Data Selection and Scope Language  is used for querying data, linking
data, and limiting the data ranges or scopes
Data Manipulation Language  is used for exchanging, moving, sorting,
and transforming data
Data Aggregation and Sampling Language  is used for exchanging,
moving, sorting, and transforming data
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/16
================================================================================
PAGE 14 of 16
================================================================================

Hive
Outline
Hive ? What is it ?
Deployment and conﬁguration
Metastore
Interfaces
HQL
Hive versus relational DBMSs
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/16
================================================================================
PAGE 15 of 16
================================================================================

Hive versus relational DBMS
Similarities
Di!erencesTabular view of data objects in HDFS
Directories and ﬁles viewed as tables
Types of columns in tables
Access to tables through HQL very similar to SQL
API interface the same as JDBC programming interface-
-
-
-
-
Load and read-only data management system based on implementation of
HDFS
It is still possible to access data visible in tabular format in Hive directly through
HDFS
UPDATE supported as coarse-grained transformation instead of ﬁne-grained
transformation in relational DBMSs
No transaction processing system
No veriﬁcation of consistency constraints, e.g. primary keys, foreign keys,
domains constraints, etc-
-
-
-
-TOP                    ISIT312 Big Data Management, SIM S4 2025 15/16
================================================================================
PAGE 16 of 16
================================================================================

References
Gross C., GuptaA., Shaw S., Vermeulen A. F., Kjerrumgaar D., Practical
Hive: A guide to Hadoop's Data Warehouse System, Apress 2016,
Chapter 4 (Available through UOW library)
Lee D., Instant Apache Hive essentials how-to: leverage your knowledge
of SQL to easily write distributed data processing applications on
Hadoop using Apache Hive, Packt Publishing Ltd. 2013 (Available
through UOW library)
Apache Hive
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/16
================================================================================


################################################################################
FILE: 08hivedatastructures.pdf
################################################################################

================================================================================
PAGE 1 of 24
================================================================================

         ISIT312 Big Data Management
Hive Data Structures
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 24
================================================================================

Hive Data Structures
Outline
Primitive Data Types
Complex Data Types
Databases
Tables
Partitions
Buckets
Views
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/24
================================================================================
PAGE 3 of 24
================================================================================

Primitive Data Types
TINYINT, 1 byte, example: 10Y
SMALLINT, 2 bytes, example: 10S
INT, 4 bytes, example: 10
BIGINT, 8 bytes, example: 10L
FLOAT, 4 bytes, example: 0.1234567
DOUBLE, 8 bytes, example: 0.1234567891234
DECIMAL, (m,n), example: 3.14
BINARY, n bytes, example: 1011001
BOOLEAN, 1 byte example: TRUE
STRING, 2G bytes, example: 'Abcdef'
CHAR, 255 bytes, example: 'Hello'
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/24
================================================================================
PAGE 4 of 24
================================================================================

Primitive Data Types
VARCHAR, 1 byte, example: 'Hive'
DATE, YYYY-MM-DD, example: '2017-05-03'
TIMESTAMP, YYYY-MM-DD HH:MM:SS[.!f...] example: '2017-05-03
15:10:00.345'
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/24
================================================================================
PAGE 5 of 24
================================================================================

Hive Data Structures
Outline
Primitive Data Types
Complex Data Types
Databases
Tables
Partitions
Buckets
Views
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/24
================================================================================
PAGE 6 of 24
================================================================================

Complex Data Types
ARRAY: list of values of the same types,
MAP: a set of key-value pairs,
STRUCT: user deﬁned structure of any type of ﬁelds,
example: ['Hadoop',  'Pig','Hive']
access: bigdata[ 1]
Array type
example: {'k1':'Hadoop',  'k2':'Pig'}
access: bigdata[ 'k2']
Map type
example: {name:'Hadoop',  age:24, salary:50000.06}
access: bigdata. name
Struct type
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/24
================================================================================
PAGE 7 of 24
================================================================================

Complex Data Types
The following CREATE TABLE command creates a table types with
complex data types columns
INSERT INTO and SELECT statements can be used to load data into a
table with complex data types columns
CREATE TABLE types(
   array_col array<string>,
   map_col map< int,string>,
   struct_col struct< a:string, b:int, c:double> );
A table with columns of types
INSERT INTO types
   SELECT array( 'bolt', 'nut', 'screw'),
          map( 1,'bolt', 2,'nut', 3,'screw'),
          named_struct( 'a','bolt', 'b',5, 'c',0.5)
   FROM DUAL;
Inserting values into a table
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/24
================================================================================
PAGE 8 of 24
================================================================================

Hive Data Structures
Outline
Primitive Data Types
Complex Data Types
Databases
Tables
Partitions
Buckets
Views
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/24
================================================================================
PAGE 9 of 24
================================================================================

Databases
Database is a collection of conceptually related tables, i.e. tables that
implement a conceptual schema
Database is implemented as a folder/directory in HDFS
A default database  is located at /user/hive/warehouse
A new database is created in a folder /user/hive/warehouse
For example, a database tpchr is located at
/user/hive/warehouse/tpchr.db
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/24
================================================================================
PAGE 10 of 24
================================================================================

Databases
The following CREATE DATABASE command creates a database tpchr
To ﬁnd more information about a database we can use DESCRIBE
DATABASE command
A command USE makes a database "current" (there is no need to preﬁx
a table name with a database name)
To delete a database we can use DROP DATABASE command
CREATE DATABASE tpchr;
Creating a database
DESCRIBE DATABASE tpchr;
Listing a database
USE tpchr;
Making a database current
DROP DATABASE tpchr;
Dropping a database
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/24
================================================================================
PAGE 11 of 24
================================================================================

Hive Data Structures
Outline
Primitive Data Types
Complex Data Types
Databases
Tables
Partitions
Buckets
Views
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/24
================================================================================
PAGE 12 of 24
================================================================================

Tables
An internal table  (or managed table ) is a table created by Hive in HDFS
If data is already stored in HDFS then an external Hive table can be
created to provide a tabular view of the data
Location in HDFS of data stored in an external table is speciﬁed in the
LOCATION properties instead of the default warehouse directory
Hive fully manages the life cycle (add/delete data, create/drop table) of
internal tables  and data in the internal tables
When an external table is deleted its metadata information is deleted
from a metastore  and the data is kept in HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/24
================================================================================
PAGE 13 of 24
================================================================================

Tables
CREATE TABLE statement creates an internal table
LOAD DATA statement loads data into an internal table
CREATE TABLE IF NOT EXISTS intregion(
   R_REGIONKEY DECIMAL( 12),
   R_NAME VARCHAR( 25),
   R_COMMENT VARCHAR( 152) )
      ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
      STORED AS TEXTFILE;
Creating an internal table
LOAD DATA LOCAL INPATH 'region.tbl' INTO TABLE intregion;
Loading data into an internal table
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/24
================================================================================
PAGE 14 of 24
================================================================================

Tables
CREATE EXTERNAL TABLE statement creates an external table
LOAD DATA statement loads data into an external table
CREATE EXTERNAL TABLE IF NOT EXISTS extregion(
   R_REGIONKEY DECIMAL( 12),
   R_NAME VARCHAR( 25),
   R_COMMENT VARCHAR( 152) )
      ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
      STORED AS TEXTFILE LOCATION '/user/tpchr/region';
Creating an external table
LOAD DATA LOCAL INPATH 'region.tbl' INTO TABLE extregion;
Loading data into an external table
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/24
================================================================================
PAGE 15 of 24
================================================================================

Tables
An external table can be created "over" an already existing ﬁle in HDFS
hadoop fs - mkdir /user/tpchr/nation
hadoop fs - put nation. tbl /user/tpchr/nation
hadoop fs - ls /user/tpchr/nation
-rw-r--r-- 3 janusz supergroup 401 2017-07-02 10:24 /user/tpchr/nation/nation.tbl
Loading a file to HDFS
     
CREATE EXTERNAL TABLE IF NOT EXISTS extnation(
   N_NATIONKEY DECIMAL( 12),
   N_NAME      VARCHAR( 25),
   N_COMMENT   VARCHAR( 152) )
      ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE LOCATION '/user/tpchr/nation';
Creating an external table over a file in HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 15/24
================================================================================
PAGE 16 of 24
================================================================================

Hive Data Structures
Outline
Primitive Data Types
Complex Data Types
Databases
Tables
Partitions
Buckets
Views
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/24
================================================================================
PAGE 17 of 24
================================================================================

Partitions
To eliminate unnecessary scans of entire table when only a fragment is
needed a table can be divided into partitions
A partition  corresponds to predeﬁned columns and it is stored as
subfolder in HDFS
When a table is searched only required partitions  are accessed
CREATE TABLE IF NOT EXISTS part(
   P_PARTKEY DECIMAL( 12),
   P_NAME VARCHAR( 55),
   P_TYPE VARCHAR( 25),
   P_SIZE DECIMAL( 12),
   P_COMMENT VARCHAR( 23) )
   PARTITIONED BY (P_BRAND VARCHAR(20))
      ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
      STORED AS TEXTFILE;
Creating a partitioned table
TOP                    ISIT312 Big Data Management, SIM S4 2025 17/24
================================================================================
PAGE 18 of 24
================================================================================

Partitions
A partition must be added before data is loaded
A command that loads a ﬁle into a table can be used to load a partition
A partition is stored in HDFS as a subfolder
ALTER TABLE part ADD PARTITION (P_BRAND='GoldenBolts');
Adding a partition
show partitions part;
OK
p_brand=GoldenBolts
Time taken:  0.072 seconds,  Fetched:  1 row(s)
Listing partitions
LOAD DATA LOCAL INPATH '/local/home/janusz/HIVE-EXAMPLES/TPCHR/part.txt'
OVERWRITE INTO TABLE part PARTITION (P_BRAND='GoldenBolts');
Loading data into a partition
hadoop fs - ls /user/hive/warehouse/ part
Found 1 items
drwxrwxr- x - janusz supergroup 0  2017-07-01
Finding a partition in HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 18/24
================================================================================
PAGE 19 of 24
================================================================================

Hive Data Structures
Outline
Primitive Data Types
Complex Data Types
Databases
Tables
Partitions
Buckets
Views
TOP                    ISIT312 Big Data Management, SIM S4 2025 19/24
================================================================================
PAGE 20 of 24
================================================================================

Buckets
Another way to speed up processing of a table is to divide it into buckets
A bucket corresponds to segment of ﬁle in HDFS
The values in a bucket column will be hashed by a user deﬁned number
into buckets.
CREATE TABLE customer(
   C_CUSTKEY DECIMAL( 12),
   C_NAME VARCHAR( 25),
   C_PHONE CHAR( 15),
   C_ACCTBAL DECIMAL( 12,2) )
   CLUSTERED BY (C_CUSTKEY)  INTO 2 BUCKETS
      ROW FORMAT DELIMITED FIELDS TERMINATED BY '|';
Creating a table with buckets
set map.reduce.tasks = 2;
set hive. enforce.bucketing = true;
Setting MapReduce and Hive parameters
TOP                    ISIT312 Big Data Management, SIM S4 2025 20/24
================================================================================
PAGE 21 of 24
================================================================================

Buckets
INSERT can be used to populate a bucket table
INSERT INTO customer
values(1,'Customer#000000001', '25-989-741-2988', 711.56);
INSERT INTO customer
values(2,'Customer#000000002', '23-768-687-3665', 121.65);
INSERT INTO customer
values(3,'Customer#000000003', '11-719-748-3364', 7498.12);
INSERT INTO customer
values(4,'Customer#000000004', '14-128-190-5944', 2866.83);
INSERT INTO customer
values(5,'Customer#000000005', '13-750-942-6364', 794.47)
Inserting into a table with buckets
TOP                    ISIT312 Big Data Management, SIM S4 2025 21/24
================================================================================
PAGE 22 of 24
================================================================================

Hive Data Structures
Outline
Primitive Data Types
Complex Data Types
Databases
Tables
Partitions
Buckets
Views
TOP                    ISIT312 Big Data Management, SIM S4 2025 22/24
================================================================================
PAGE 23 of 24
================================================================================

Views
Views are logical data structures that simplify queries
Views do not store data or get materialized
Once a views is created its deﬁnition is frozen and changes in the tables
used in the view deﬁnition are not reﬂected in the view schema
CREATE VIEW vcustomer AS
   SELECT C_CUSTKEY,  C_NAME, C_PHONE
   FROM CUSTOMER
   WHERE C_CUSTKEY <  5;
Creating a view
TOP                    ISIT312 Big Data Management, SIM S4 2025 23/24
================================================================================
PAGE 24 of 24
================================================================================

References
Gross C., GuptaA., Shaw S., Vermeulen A. F., Kjerrumgaar D., Practical
Hive: A guide to Hadoop's Data Warehouse System, Apress 2016,
Chapter 4 (Available through UOW library)
Lee D., Instant Apache Hive essentials how-to: leverage your knowledge
of SQL to easily write distributed data processing applications on
Hadoop using Apache Hive, Packt Publishing Ltd. 2013 (Available
through UOW library)
Apache Hive
TOP                    ISIT312 Big Data Management, SIM S4 2025 24/24
================================================================================


################################################################################
FILE: 09dwconcepts.pdf
################################################################################

================================================================================
PAGE 1 of 26
================================================================================

         ISIT312 Big Data Management
Data Warehouse Concepts
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 26
================================================================================

Data Warehouse Concepts
Outline
OLAP versus OLTP
The Multidimensional Model
OLAP Operations
Data Warehouse Architecture
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/26
================================================================================
PAGE 3 of 26
================================================================================

OLAP versus OLTP
Traditional database systems designed and tuned to support the day-to-
day operation:
OLTP database characteristics:
Data analysis requires a new paradigm: online analytical processing
(OLAP)Ensure fast, concurrent access to data
Transaction processing and concurrency control
Focus on online update data consistency
Known as operational databases or online transaction processing (OLTP )-
-
-
-
Detailed data
Do not include historical data
Highly normalized
Poor performance on complex queries including joins an aggregation-
-
-
-
Typical OLTP  query: pending orders for a customer
Typical OLAP query: total sales amount by a product and by a customer-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/26
================================================================================
PAGE 4 of 26
================================================================================

OLAP versus OLTP
OLAP characteristics
The need for a di !erent database model to support OLAP was clear: led
to data warehouses
Data warehouse : (usually) large repositories that consolidate data from
di!erent sources (internal and external to the organization), are updated
o"ine, follow the multidimensional data model , designed and optimized
to e#ciently support OLAP queriesOLTP paradigm focused on transactions, OLAP focused on analytical queries
Normalization not good for analytical queries, reconstructing data requires a
high number of joins
OLAP databases support a heavy query load
OLTP indexing techniques not e #cient in OLAP: oriented to access few records;
OLAP queries typically include aggregation-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/26
================================================================================
PAGE 5 of 26
================================================================================

Data Warehouse Concepts
Outline
OLAP versus OLTP
The Multidimensional Model
OLAP Operations
Data Warehouse Architecture
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/26
================================================================================
PAGE 6 of 26
================================================================================

The Multidimensional Model
A view of data in n-dimensional space: a data cube
A data cube  is composed of dimensions  and facts
Dimensions : Perspectives used to analyze the data
Attributes  describe dimensionsExample: A three-dimensional cube for sales data with dimensions Product,
Time, and Customer, and a measure Quantity
-
Product dimension may have attributes ProductNumber and UnitPrice (not
shown in the ﬁgure)-
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/26
================================================================================
PAGE 7 of 26
================================================================================

The Multidimensional Model
The cells or facts of a data cube have associated numeric values called
measures
Each cell of the data cube  represents Quantity of units sold by
category, quarter, and customer's city
Data granularity: level of detail at which measures are represented for
each dimension of the cube
Example: sales ﬁgures aggregated to granularities Category, Quarter, and City -
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/26
================================================================================
PAGE 8 of 26
================================================================================

The Multidimensional Model
Instances of a dimension are called members
A data cube  contains several measures, e.g. Amount, indicating the total
sales amount (not shown)
A data cube  may be sparse (typical case) or dense
Hierarchies: allow viewing data at several granularitiesExample: Seafood and Beverages are members  of the Product at the
granularity Category-
Example: not all customers may have ordered products of all categories during all
quarters-
Deﬁne a sequence of mappings relating lower-level, detailed concepts to higher-
level ones
The lower level is called the child and the higher level is called the parent
The hierarchical structure of a dimension is called the dimension schema
A dimension instance  comprises all members at all levels in a dimension-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/26
================================================================================
PAGE 9 of 26
================================================================================

The Multidimensional Model
In the previous ﬁgure, granularity of each dimension indicated between
parentheses: Category for the Product dimension, Quarter for Time,
and City for Customer
We may want sales ﬁgures at a ﬁner granularity ( Month), or at a coarser
granularity (Country)
Hierarchies of the Product, Time, and Customer dimensions
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/26
================================================================================
PAGE 10 of 26
================================================================================

The Multidimensional Model
Members of a hierarchy Product - Category
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/26
================================================================================
PAGE 11 of 26
================================================================================

The Multidimensional Model: Measures
Aggregation of measures changes the abstraction level at which data in a
cube are visualized
Measures can be:
Additive : can be meaningfully summarized along all the dimensions, using
addition; The most common type of measures
Semiadditive : can be meaningfully summarized using addition along some
dimensions; Example: inventory quantities, which cannot be added along the
Time dimension
Nonadditive measures  cannot be meaningfully summarized using addition across
any dimension; Example: item price, cost per unit, and exchange rate-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/26
================================================================================
PAGE 12 of 26
================================================================================

The Multidimensional Model: Measures
Another classiﬁcation of measures:
Distributive : deﬁned by an aggregation function that can be computed in a
distributed way; Functions count, sum, minimum, and maximum are distributive,
distinct count is not; Example: S = {3, 3, 4, 5, 8, 4, 7, 3, 8}  partitioned in
subsets {3, 3, 4}, {5, 8, 4}, {7, 3, 8} gives a result of 8, while the answer over the
original set is 5
Algebraic measures  are deﬁned by an aggregation function that can be expressed
as a scalar function of distributive ones; example: average, computed by dividing
the sum by the count-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/26
================================================================================
PAGE 13 of 26
================================================================================

Data Warehouse Concepts
Outline
OLAP versus OLTP
The Multidimensional Model
OLAP Operations
Data Warehouse Architecture
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/26
================================================================================
PAGE 14 of 26
================================================================================

OLAP Operations
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/26
================================================================================
PAGE 15 of 26
================================================================================

OLAP Operations
Starting cube: quarterly sales (in thousands) by product category and
customer cities for 2012
We ﬁrst compute the sales quantities by country: a roll-up operation
to the Country level along the Customer dimension
Sales of category Seafood in France signiﬁcantly higher in the ﬁrst
quarter
To explore alternative visualizations, we sort products by name
To see the cube with the Time dimension on the x axis, we rotate the
axes of the original cube, without changing granularities → pivoting
(see next 2 slides)To ﬁnd out if this occurred during a particular month, we take cube back to
City aggregation level, and drill-down along Time to the Month level-
TOP                    ISIT312 Big Data Management, SIM S4 2025 15/26
================================================================================
PAGE 16 of 26
================================================================================

OLAP Operations
To visualize the data only for Paris → slice operation, results in a 2-
dimensional sub-cube, basically a collection of time series (see next
slide)
To obtain a 3-dimensional sub-cube containing only sales for the ﬁrst
two quarters and for the cities Lyon and Paris, we go back to the original
cube and apply a dice operation
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/26
================================================================================
PAGE 17 of 26
================================================================================

OLAP Operations
TOP                    ISIT312 Big Data Management, SIM S4 2025 17/26
================================================================================
PAGE 18 of 26
================================================================================

OLAP Operations
The operations in the previous slides can be deﬁned using the following
algebraic operators.
Roll-up: aggregates measures along a dimension hierarchy (using an
aggregate function) to obtain measures at a coarser granularity
Extended roll-up: similar to rollup, but drops all dimensions not involved
in the operation
Recursive roll-up: aggregates over a recursive hierarchy (a level rolls-up
to itself)
ROLLUP(CubeName,  (Dimension → Level)*, AggFunction( Measure)*)
ROLLUP(Sales, Customer → Country, SUM(Quantity))
OLAP
ROLLUP*(CubeName,  [(Dimension → Level)*], AggFunction( Measure)*)
ROLLUP*(Sales,  Time → Quarter, SUM(Quantity))
ROLLUP*(Sales,  Time → Quarter, COUNT(Product) AS ProdCount)
OLAP
RECROLLUP( CubeName,  Dimension → Level, AggFunction( Measure)*)
 OLAP
TOP                    ISIT312 Big Data Management, SIM S4 2025 18/26
================================================================================
PAGE 19 of 26
================================================================================

OLAP Operations
Drill-down moves from a more general level to a more detailed level
in a hierarchy
Sort returns a cube where the members of a dimension have been
sorted according to the value of Expression
DRILLDOWN( CubeName,  (Dimension → Level)*)
DRILLDOWN( Sales, Time → Month)
OLAP
NAME is a predeﬁned keyword in the algebra representing the name of a
member
SORT(CubeName,  Dimension,  Expression [ASC | DESC])
 OLAP
SORT(Sales, Product, NAME)
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 19/26
================================================================================
PAGE 20 of 26
================================================================================

OLAP Operations
Pivot
Slice:where the axes are speciﬁed as {X, Y, Z, X , Y, Z, ... }.
PIVOT(CubeName,  (Dimension → Axis)*)
 OLAP
- 1 1 1
PIVOT(Sales, Time → X, Customer → Y, Product → Z)
 OLAP
Dimension will be dropped by ﬁxing a single Value in the Level, other
dimensions unchanged
Slice supposes that the granularity of the cube is at the speciﬁed level of the
dimension
SLICE(CubeName,  Dimension,  Level = Value)
 OLAP
-
SLICE(Sales, Customer,  City = 'Paris')
 OLAP
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 20/26
================================================================================
PAGE 21 of 26
================================================================================

OLAP Operations
Dice:
where ? is a Boolean condition over dimension levels, attributes, and measures.
DICE(CubeName,  ? )
 OLAP
-
DICE(Sales, (Customer. City = 'Paris' OR Customer. City = 'Lyon') AND 
            ( Time.Quarter = 'Q1' OR Time. Quarter = 'Q2') )
OLAP
TOP                    ISIT312 Big Data Management, SIM S4 2025 21/26
================================================================================
PAGE 22 of 26
================================================================================

Data Warehouse Concepts
Outline
OLAP versus OLTP
The Multidimensional Model
OLAP Operations
Data Warehouse Architecture
TOP                    ISIT312 Big Data Management, SIM S4 2025 22/26
================================================================================
PAGE 23 of 26
================================================================================

Typical Data Warehouse Architecture
TOP                    ISIT312 Big Data Management, SIM S4 2025 23/26
================================================================================
PAGE 24 of 26
================================================================================

Data Warehouse Architecture
General data warehouse architecture: several tiers
Back-end tier composed of:
Data warehouse tier  composed of:
OLAP tier composed of:The extraction, transformation , and loading  (ETL) tools: Feed data into the data
warehouse from operational databases and internal and external data sources
The data staging area : An intermediate database where all the data integration
and transformation processes are run prior to the loading of the data into the
data warehouse-
-
An enterprise data warehouse  and/or several data marts
A metadata repository  storing information about the data warehouse and its
contents-
-
An OLAP server which provides a multidimensional view of the data, regardless
the actual way in which data are stored-
TOP                    ISIT312 Big Data Management, SIM S4 2025 24/26
================================================================================
PAGE 25 of 26
================================================================================

Data Warehouse Architecture
Front-end tier  is used for data analysis and visualization
Contains client tools such as OLAP tools , reporting tools , statistical tools, and
data-mining tools-
TOP                    ISIT312 Big Data Management, SIM S4 2025 25/26
================================================================================
PAGE 26 of 26
================================================================================

References
A. VAISMAN, E. ZIMANYI, Data Warehouse Systems: Design and
Implementation, Chapter 3 Data Warehouse Concepts, Springer Verlag,
2014
TOP                    ISIT312 Big Data Management, SIM S4 2025 26/26
================================================================================


################################################################################
FILE: 10conceptdwdesign.pdf
################################################################################

================================================================================
PAGE 1 of 18
================================================================================

         ISIT312 Big Data Management
Conceptual Data Warehouse
Design
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 18
================================================================================

Conceptual Data Warehouse Design
Outline
MultiDim: A Conceptual Model for Data Warehouses
MultiDim Model: Notation
Dimension Hierarchies
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/18
================================================================================
PAGE 3 of 18
================================================================================

MultiDim: A Conceptual Multidimensional Model
Conceptual data models
No well-established conceptual model for multidimensional data
Several proposals based on UML, on the ER model, or using speciﬁc
notations
Problems:Allow better communication between designers and users to understand
application requirements
More stable than implementation-oriented (logical) schema, which changes with
the platform
Provide better support for visual user interfaces-
-
-
Cannot express complex kinds of hierarchies
Lack of a mapping to the implementation platform-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/18
================================================================================
PAGE 4 of 18
================================================================================

MultiDim: A Conceptual Multidimensional Model
Currently, data warehouses are designed using mostly logical models
(star and snowﬂake schemas)
MultiDim data model is based on the entity-relationship model
Includes concepts like:
Supports various kinds of hierarchies existing in real-world applications
Can be mapped to star or snowﬂake relational structuresDi!cult to express requirements (technical knowledge required
Limit users to deﬁning only elements that the underlying implementation
systems can manage-
-
dimensions
hierarchies
facts
measures-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/18
================================================================================
PAGE 5 of 18
================================================================================

Conceptual Datawarehouse Design
Outline
MultiDim: A Conceptual Model for Data Warehouses
MultiDim Model: Notation
Dimension Hierarchies
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/18
================================================================================
PAGE 6 of 18
================================================================================

MultiDim Model: Notation
A graphical notation used for a sample hierarchy
Dimension: level or one or more hierarchies
Hierarchy: several related levels
Level: entity type
Member : every instance of a level
Child and parent levels : the lower and higher levels
Leaf and root levels : ﬁrst and last levels in a hierarchy
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/18
================================================================================
PAGE 7 of 18
================================================================================

MultiDim Model: Notation
A graphical notation used for a sample hierarchy
Cardinality : minimum/maximum numbers of members in a level related
to members in another level
Criterion : expresses di"erent hierarchical structures used for analysis
Key attribute: indicates how child members are grouped
Descriptive attributes: describe characteristics of members
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/18
================================================================================
PAGE 8 of 18
================================================================================

MultiDim Model: Notation
A sample fact with 5 dimension s
Fact: relates measures to leaf levels in dimensions
Dimensions can be related to fact with one-to-one , one-to-many, of
many-to-many
Dimension can be related several times to a fact with di "erent roles
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/18
================================================================================
PAGE 9 of 18
================================================================================

MultiDim Model: Notation
Summary
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/18
================================================================================
PAGE 10 of 18
================================================================================

MultiDim Conceptual Schema of the Northwind
Data Warehouse
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/18
================================================================================
PAGE 11 of 18
================================================================================

Conceptual Data Warehouse Design
Outline
MultiDim: A Conceptual Model for Data Warehouses
MultiDim Model: Notation
Dimension Hierarchies
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/18
================================================================================
PAGE 12 of 18
================================================================================

Balanced Hierarchies
At schema level: only one path where all parent-child relationships are
many-to-one and mandatory
At instance level: members form a balanced tree (all the branches have
the same length)
All parent members have at least one child member, and a child belongs
exactly to one parent
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/18
================================================================================
PAGE 13 of 18
================================================================================

Unbalanced Hierarchies
At schema level: one path where all parent-child relationships are many-
to-one, but some are optional
At instance level: members form a unbalanced tree
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/18
================================================================================
PAGE 14 of 18
================================================================================

Recursive Hierarchies
A special case of unbalanced hierarchies
The same level is linked by the two roles of a parent-child relationship
Used when all hierarchy levels express the same semantics
The characteristics of the parent and child are similar (or the same)
Schema level
Instance level
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/18
================================================================================
PAGE 15 of 18
================================================================================

Generalized Hierarchies
At schema level: multiple exclusive paths sharing at least the leaf level;
may also share other levels
Two aggregation paths, one for each type of customer
At instance level: each member belongs to only one path
TOP                    ISIT312 Big Data Management, SIM S4 2025 15/18
================================================================================
PAGE 16 of 18
================================================================================

Noncovering Hierarchies
Also known as ragged or level-skipping hierarchies
A special case of generalized hierarchies
At the schema level: Alternative paths are obtained by skipping one or
several intermediate levels
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/18
================================================================================
PAGE 17 of 18
================================================================================

Noncovering Hierarchies
At instance level: Path length from the leaves to the same parent can be
di"erent for di "erent members
TOP                    ISIT312 Big Data Management, SIM S4 2025 17/18
================================================================================
PAGE 18 of 18
================================================================================

References
A. VAISMAN, E. ZIMANYI, Data Warehouse Systems: Design and
Implementation, Chapter 4 Conceptual Data Warehouse Design,
Springer Verlag, 2014
TOP                    ISIT312 Big Data Management, SIM S4 2025 18/18
================================================================================


################################################################################
FILE: 11logicaldwdesign.pdf
################################################################################

================================================================================
PAGE 1 of 19
================================================================================

         ISIT312 Big Data Management
Logical Data Warehouse Design
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 19
================================================================================

Logical Data Warehouse Design
Outline
OLAP Technologies
Relational Data Warehouse Design
Relational Implementation of Conceptual Model
The Time Dimension
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/19
================================================================================
PAGE 3 of 19
================================================================================

OLAP Technologies
Relational OLAP (ROLAP): Stores data in relational databases, supports
extensions to SQL and special access methods to e !ciently implement
the model and its operations
Multidimensional OLAP (MOLAP): Stores data in special data structures
(e.g., arrays) and implement OLAP operations in these structures
Hybrid OLAP (HOLAP): Combines both technologiesBetter performance  than ROLAP for query and aggregation, less storage
capacity than ROLAP-
For example, detailed data stored in relational databases, aggregations kept in
a separate MOLAP store, etc-
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/19
================================================================================
PAGE 4 of 19
================================================================================

Logical Data Warehouse Design
Outline
OLAP Technologies
Relational Data Warehouse Design
Relational Implementation of Conceptual Mode
The Time Dimension
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/19
================================================================================
PAGE 5 of 19
================================================================================

Relational Data Warehouse Design
In ROLAP  systems, tables organized in specialized structures
Star schema: One fact table and a set of dimension tables
Snowﬂake schema : Avoids redundancy of star schemas by normalizing
dimension tables
Starﬂake schema: Combination of the star and snowﬂake schemas,
some dimensions normalized, other not
Constellation schema: Multiple fact tables that share dimension tablesReferential integrity constraints between fact table and dimension tables
Dimension tables may contain redundancy in the presence of hierarchies
Dimension tables denormalized, fact tables normalized-
-
-
Normalized tables optimize storage space, but decrease performance-
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/19
================================================================================
PAGE 6 of 19
================================================================================

Example of a Star Schema
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/19
================================================================================
PAGE 7 of 19
================================================================================

Example of a Snowflake Schema
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/19
================================================================================
PAGE 8 of 19
================================================================================

Example of a Constellation Schema
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/19
================================================================================
PAGE 9 of 19
================================================================================

Logical Data Warehouse Design
Outline
OLAP Technologies
Relational Data Warehouse Desig
Relational Implementation of Conceptual Mode
The Time Dimension
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/19
================================================================================
PAGE 10 of 19
================================================================================

Relational Implementation of the Conceptual
Model
A set of rules to translate the conceptual model (the MultiDim model )
into the relational mode
Rule 1: A level L, provided it is not related to a fact with a one-to-one
relationship, is mapped to a table TL that contains all attributes of the
level
Rule 2: A fact F is mapped to a table TF that includes as attributes all
measures of the factA surrogate key may be added to the table, otherwise the identiﬁer of the level
will be the key of the table
Additional attributes will be added to this table when mapping relationships
using Rule 3 below-
-
A surrogate key may be added to the table
Additional attributes will be added to this table when mapping relationships
using Rule 3 below-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/19
================================================================================
PAGE 11 of 19
================================================================================

Relational Implementation of the Conceptual
Model
Rule 3: A relationship between either a fact F and a dimension level L, or
between dimension levels LP and LC (standing for the parent and child
levels, respectively), can be mapped in three di"erent ways, depending
on its cardinalities:
Rule 3a : If the relationship is one-to-one, the table corresponding to the
fact (TF) or to the child level (TC) is extended with all the attributes of the
dimension level or the parent level, respectively
Rule 3b: If the relationship is one-to-many, the table corresponding to
the fact (TF) or to the child level (TC) is extended with the surrogate key
of the table corresponding to the dimension level (TL) or the parent level
(TP), respectively, that is, there is a foreign key in the fact or child table
pointing to the other table
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/19
================================================================================
PAGE 12 of 19
================================================================================

Relational Implementation of the Conceptual
Model
Rule 3c: If the relationship is many-to-many, a new table TB (standing for
bridge table) is created that contains as attributes the surrogate keys of
the tables corresponding to the fact (TF) and the dimension level (TL), or
the parent (TP) and child levels (TC), respectively. If the relationship has a
distributing attribute, an additional attribute is added to the table to
store this information
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/19
================================================================================
PAGE 13 of 19
================================================================================

MultiDim Conceptual Schema of the Northwind
Data Warehouse
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/19
================================================================================
PAGE 14 of 19
================================================================================

Relational Representation of the Northwind Data
Warehouse
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/19
================================================================================
PAGE 15 of 19
================================================================================

Relational Representation of the Northwind Data
Warehouse
The Sales table includes one FK for each level related to the fact with a
one-to-many relationship
For Time , several roles: OrderDate , DueDate , and ShippedDate
Order : related to the fact with a one-to-one relationship, called a
degenerate, or a fact dimension
Fact table contains ﬁve attributes representing the measures:
The many-to-many parent-child relationship between Employee and
Territory is mapped to the table Territories , containing two
foreign keys
Customer has a surrogate key CustomerKey and a database key
CustomerAltKey
SupplierKey in Supplier is a database keyUnitPrice  , Quantity  , Discount  , SalesAmount  , and Freight  . -
TOP                    ISIT312 Big Data Management, SIM S4 2025 15/19
================================================================================
PAGE 16 of 19
================================================================================

Logical Data Warehouse Design
Outline
OLAP Technologies
Relational Data Warehouse Design
Relational Implementation of Conceptual Mode
The Time Dimension
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/19
================================================================================
PAGE 17 of 19
================================================================================

The Time Dimension
Data warehouse: a historical database
Time dimension present in almost all data warehouses.
In a star or snowﬂake schema, time is included both as foreign key(s) in
a fact table and as a time dimension containing the aggregation levels
OLTP databases: temporal information is usually derived from attributes
of a DATE data type
In a data warehouse time information is stored as explicit attributes in
the time dimensionExample: A weekend is computed on-the-ﬂy using appropriate functions-
Easy to compute: Total sales during weekends-
SELECT SUM(SalesAmount)
FROM Time T, Sales S
WHERE T.TimeKey = S.TimeKey AND T.WeekendFlag
SELECT statement filtering time dimension
TOP                    ISIT312 Big Data Management, SIM S4 2025 17/19
================================================================================
PAGE 18 of 19
================================================================================

The Time Dimension
The granularity of the time dimension varies depending on their use
Time dimension with a granularity month spanning 5 years will have 5 *
12 = 60 tuples
TOP                    ISIT312 Big Data Management, SIM S4 2025 18/19
================================================================================
PAGE 19 of 19
================================================================================

References
A. VAISMAN, E. ZIMANYI, Data Warehouse Systems: Design and
Implementation, Chapter 5 Logical Data Warehouse Design, Springer
Verlag, 2014
TOP                    ISIT312 Big Data Management, SIM S4 2025 19/19
================================================================================


################################################################################
FILE: 12sqlfordw.pdf
################################################################################

================================================================================
PAGE 1 of 18
================================================================================

         ISIT312 Big Data Management
SQL for Data Warehousing
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 18
================================================================================

SQL for Data Warehousing
Outline
SQL/OLAP Operations
Window partitioning
Window ordering
Window framing
TOP                    ISIT312 Big Data Management, SIM S2 2025 2/18
================================================================================
PAGE 3 of 18
================================================================================

SQL/OLAP Operations
Consider the SALES fact table
To compute all possible aggregations along the dimensions Product and
Customer we must scan the whole relational table SALES several times
It can be implemented in SQL using NULL and UNION in the following
way:
      
SELECT ProductKey, CustomerKey, SalesAmount
FROM Sales
  UNION
SELECT ProductKey, NULL, SUM(SalesAmount)
FROM Sales
GROUP BY ProductKey
  UNION
SELECT NULL, CustomerKey, SUM(SalesAmount)
FROM Sales
GROUP BY CustomerKey
  UNION
SELECT NULL, NULL, SUM(SalesAmount)
FROM Sales;
Finding aggregations along many dimensions
TOP                    ISIT312 Big Data Management, SIM S2 2025 3/18
================================================================================
PAGE 4 of 18
================================================================================

SQL/OLAP Operations
A data cube created through UNION of individual SELECT statements
each one creating one combination of dimensions looks in the following
way
TOP                    ISIT312 Big Data Management, SIM S2 2025 4/18
================================================================================
PAGE 5 of 18
================================================================================

SQL/OLAP Operations
Computing a cube with n dimensions requires (2*2*2*... *2)(n times)
SELECT statements with GROUP BY clause
SQL/OLAP extends the GROUP BY clause with the ROLLUP and CUBE
operators
ROLLUP computes group subtotals in the order given by a list of
attributes
CUBE computes all totals of such a list
Shorthands for a more powerful operator, GROUPING SETS
Equivalent queries
      
SELECT ProductKey, CustomerKey, SUM(SalesAmount)
FROM Sales
GROUP BY ROLLUP(ProductKey, CustomerKey);
Sample application of ROLLUP operation
      
SELECT ProductKey, CustomerKey, SUM(SalesAmount)
FROM Sales
GROUP BY GROUPING SETS((ProductKey,CustomerKey),(ProductKey),());
Sample application of GROUPING SET operation
TOP                    ISIT312 Big Data Management, SIM S2 2025 5/18
================================================================================
PAGE 6 of 18
================================================================================

SQL/OLAP Operations
Equivalent queries
SELECT ProductKey,  CustomerKey,  SUM(SalesAmount)
FROM Sales
GROUP BY CUBE(ProductKey,  CustomerKey);
Sample application of CUBE operation
SELECT ProductKey,  CustomerKey,  SUM(SalesAmount)
FROM Sales
GROUP BY GROUPING SETS((ProductKey,  CustomerKey),(ProductKey),(CustomerKey),());
Sample application of GROUPING SETS operation
TOP                    ISIT312 Big Data Management, SIM S2 2025 6/18
================================================================================
PAGE 7 of 18
================================================================================

SQL/OLAP Operations
TOP                    ISIT312 Big Data Management, SIM S2 2025 7/18
================================================================================
PAGE 8 of 18
================================================================================

SQL for Data Warehousing
Outline
SQL/OLAP Operations
Window partitioning
Window ordering
Window framing
TOP                    ISIT312 Big Data Management, SIM S2 2025 8/18
================================================================================
PAGE 9 of 18
================================================================================

Window partitioning
Allows to compare detailed data with aggregate values
For example, ﬁnd a relevance of each customer with respect to the sales
of the product
First three columns are obtained from the Sales table
The fourth column is created in the following way
    
SELECT ProductKey,  CustomerKey,  SalesAmount,
       MAX( SalesAmount)  OVER (PARTITION BY ProductKey)  AS MaxAmount
FROM SALES;
Sample window partitioning
Create a window called partition  that contains all tuples of the same
product
SalesAmount  is aggregated over this window using MAX function-
-
TOP                    ISIT312 Big Data Management, SIM S2 2025 9/18
================================================================================
PAGE 10 of 18
================================================================================

Window partitioning
      
SELECT ProductKey,  CustomerKey,  SalesAmount,
       MAX( SalesAmount)  OVER (PARTITION BY ProductKey)  AS MaxAmount
FROM SALES;
Sample window partitioning
TOP                    ISIT312 Big Data Management, SIM S2 2025 10/18
================================================================================
PAGE 11 of 18
================================================================================

SQL for Data Warehousing
Outline
SQL/OLAP Operations
Window partitioning
Window ordering
Window framing
TOP                    ISIT312 Big Data Management, SIM S2 2025 11/18
================================================================================
PAGE 12 of 18
================================================================================

Window ordering
ORDER BY clause allows the rows within a partition to be ordered
It is useful to compute rankings, with a function RANK()
For example, how does each product rank in the sales of each customer
      
SELECT ProductKey,  CustomerKey,  SalesAmount,
       RANK() OVER (PARTITION BY CustomerKey ORDER BY SalesAmount DESC) AS RowNo
FROM Sales;
Sample window ordering
TOP                    ISIT312 Big Data Management, SIM S2 2025 12/18
================================================================================
PAGE 13 of 18
================================================================================

SQL for Data Warehousing
Outline
SQL/OLAP Operations
Window partitioning
Window ordering
Window framing
TOP                    ISIT312 Big Data Management, SIM S2 2025 13/18
================================================================================
PAGE 14 of 18
================================================================================

Window framing
It is possible to deﬁne a size of a partition
It can be used to compute statistical functions over time series, like
moving average
For example, three-month moving average of sales by product
Processing of a query opens a window with the rows pertaining to the
current product
Then, it orders the window by year and month and computes the
average over the current row and the previous two ones if they exist
      
SELECT ProductKey,  Year, Month, SalesAmount,
       AVG( SalesAmount)  OVER (PARTITION BY ProductKey
                              ORDER BY Year, Month
                              ROWS 2 PRECEDING)  AS MovAvg
FROM SALES;
Sample window framing
TOP                    ISIT312 Big Data Management, SIM S2 2025 14/18
================================================================================
PAGE 15 of 18
================================================================================

Window framing
      
SELECT ProductKey,  Year, Month, SalesAmount,
       AVG( SalesAmount)  OVER (PARTITION BY ProductKey
                              ORDER BY Year, Month
                              ROWS 2 PRECEDING)  AS MovAvg
FROM SALES;
Sample window framing
TOP                    ISIT312 Big Data Management, SIM S2 2025 15/18
================================================================================
PAGE 16 of 18
================================================================================

Window framing
Another example, a year-to-date sum of sales by product
Processing of a query, opens a window with the tuples of the current
product and year ordered by month
AVG() is applied to all rows before the current row (ROWS UNBOUNDED
PRECEDING )
      
SELECT ProductKey,  Year, Month, SalesAmount,
       SUM( SalesAmount)  OVER (PARTITION BY ProductKey,  Year
                              ORDER BY Month
                              ROWS UNBOUNDED PRECEDING)  AS YTD
FROM SALES;
Sample window framing
TOP                    ISIT312 Big Data Management, SIM S2 2025 16/18
================================================================================
PAGE 17 of 18
================================================================================

Window framing
      
SELECT ProductKey,  Year, Month, SalesAmount,
       SUM( SalesAmount)  OVER (PARTITION BY ProductKey,  Year
                              ORDER BY Month
                              ROWS UNBOUNDED PRECEDING)  AS YTD
FROM SALES;
Sample window framing
TOP                    ISIT312 Big Data Management, SIM S2 2025 17/18
================================================================================
PAGE 18 of 18
================================================================================

References
A. VAISMAN, E. ZIMANYI, Data Warehouse Systems: Design and
Implementation, Chapter 5 Logical Data Warehouse Design, Springer
Verlag, 2014
TOP                    ISIT312 Big Data Management, SIM S2 2025 18/18
================================================================================


################################################################################
FILE: 13hiveprogramming.pdf
################################################################################

================================================================================
PAGE 1 of 28
================================================================================

         ISIT312 Big Data Management
Hive Programming
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 28
================================================================================

Hive Programming
Outline
Data Selection and Scope
Data Manipulation
Data Aggregation and Sampling
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/28
================================================================================
PAGE 3 of 28
================================================================================

Data Selection and Scope
To query data Hive provides SELECT statement
Typically SELECT statement projects the rows satisfying the query
conditions speciﬁed in the WHERE clause and returns the result set
SELECT statement is usually used with FROM, DISTINCT, WHERE, and
LIMIT keywords
SELECT C_NAME,  C_PHONE
FROM customer
WHERE C_ACCTBAL >  0
LIMIT 2;
SELECT statement with LIMIT clause
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/28
================================================================================
PAGE 4 of 28
================================================================================

Data Selection and Scope
Multiple SELECT statements can be combined into complex queries
using nested queries  or subqueries
Subqueries  can use Common Table Expressions  (CTE ) in the format of
WITH clause
When using subqueries , an alias  should be given for the subquery
      
WITH cord AS ( SELECT *
               FROM customer JOIN orders
                             ON c_custkey =  o_custkey)
SELECT c_name,  c_phone,  o_orderkey,  o_orderstatus
FROM cord;
WITH clause
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/28
================================================================================
PAGE 5 of 28
================================================================================

Data Selection and Scope
Multiple SELECT statements can be combined into complex queries
using nested queries  or subqueries
Nested queries  can use SELECT statement wherever a table is expected
or a scalar value is expected
      
SELECT c_name,  c_phone,  o_orderkey,  o_orderstatus
FROM ( SELECT *
       FROM customer JOIN orders
                     ON c_custkey =  o_custkey)  cord
Nested query
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/28
================================================================================
PAGE 6 of 28
================================================================================

Data Selection and Scope
When inner join  is performed between multiple tables the MapReduce
jobs are created to process data in HDFS
It is recommended to put the big table right at the end for better
because the last table in the sequence is streamed through the reducers
where the others are bu !ered in the reducer by default
      
SELECT /*+ STREAMTABLE(lineitem) */ c_name,  o_orderkey,  l_linenumber
FROM customer JOIN orders
              ON c_custkey =  o_custkey
              JOIN lineitem
              ON l_orderkey =  o_orderkey;
Inner join
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/28
================================================================================
PAGE 7 of 28
================================================================================

Data Selection and Scope
Outer join  (left, right, and full) and cross join preserve their HQL
semantics
Map join  means that join is computed only by map job without reduce
job
In map join  all data are read from a small table to memory and
broadcasted to all maps
During map phase  each row in from a big table is compared with the
rows in small tables against join conditions
Join performance is improved because there is no reduce phase
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/28
================================================================================
PAGE 8 of 28
================================================================================

Data Selection and Scope
Hive automatically converts the JOIN to MAPJOIN at runtime when
hive.auto.convert.join setting is set to true
Bucket map join is a special type of MAPJOIN that uses bucket columns
in join condition.
Then instead of fetching the whole table bucket map join only fetches
the required bucket data.
A variable hive.optimize.bucketmapjoin must be set to true to
enable bucket map join
SELECT /*+ MAPJOIN(orders) */ c_name,  c_phone,  o_orderkey,  o_orderstatus
FROM customer JOIN orders
              ON c_custkey =  o_custkey;
Map join
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/28
================================================================================
PAGE 9 of 28
================================================================================

Data Selection and Scope
Hive supports LEFT SEMI JOIN
In LEFT SEMI JOIN the right-hand side table should only be referenced
in the join condition and not in WHERE or SELECT clauses
      
SELECT c_name,  c_phone
FROM customer LEFT SEMI JOIN orders
              ON c_custkey =  o_custkey;
Left semi join
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/28
================================================================================
PAGE 10 of 28
================================================================================

Data Selection and Scope
Hive supports UNION ALL it does not support INTERSECT and MINUS
operations
INTERSECT operation can be implemented as JOIN operation
MINUS operation can be implemented as LEFT OUTER JOIN operation
with IS NULL condition in WHERE clause
SELECT p_name
FROM PART
UNION ALL
SELECT c_name
FROM CUSTOMER;
UNION ALL operation
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/28
================================================================================
PAGE 11 of 28
================================================================================

Hive Programming
Outline
Data Selection and Scope
Data Manipulation
Data Aggregation and Sampling
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/28
================================================================================
PAGE 12 of 28
================================================================================

Data Manipulation
LOAD statement can be used to load data to Hive tables from local ﬁle
system or from HDFS
Load data to Hive table from a local ﬁle
Load data to Hive partitioned table from a local ﬁle
LOCAL keyword determines a location of the input ﬁles
      
LOAD DATA LOCAL INPATH '/local/home/janusz/HIVE-EXAMPLES/TPCHR/part.txt'
OVERWRITE INTO TABLE part;
Loading data from a local file
LOAD DATA LOCAL INPATH '/local/home/janusz/HIVE-EXAMPLES/TPCHR/part.txt'
OVERWRITE INTO TABLE part PARTITION
(P_BRAND='GoldenBolts');
Loading data into partitioned table from a local file
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/28
================================================================================
PAGE 13 of 28
================================================================================

Data Manipulation
Load HDFS data to the Hive table using the default system path
Load HDFS data to the Hive table using using full URI
If LOCAL keyword is not speciﬁed, the ﬁles are loaded from the full URI
speciﬁed after INPATH or the value from the fs.default
OVERWRITE keyword decides whether to append or replace the existing
data in the target table/partition
      
LOAD DATA INPATH '/user/janusz/part.txt'
OVERWRITE INTO TABLE part;
Loading dta from HDFS
      
LOAD DATA INPATH 'hdfs://10.9.28.14:8020/user/janusz/part.txt'
OVERWRITE INTO TABLE part;
Loading data from HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/28
================================================================================
PAGE 14 of 28
================================================================================

Data Manipulation
EXPORT and IMPORT statements are available to support the import and
export of data in HDFS for data migration or backup/restore purposes
EXPORT statement exports both data and metadata from a table or
partition
Metadata is exported to a ﬁle called _metadata
After EXPORT the exported ﬁles can be copied to other Hive instances or
to other HDFS clusters
EXPORT TABLE part TO '/user/tpchr/part'
Exporting table to HDFS
-rwxr-xr-x 3 janusz supergroup 2739 2017-07-09
14:37 /user/tpchr/part/_metadata
drwxr-xr-x - janusz supergroup 0  2017-07-09
14:37 /user/tpchr/part/p_brand=GoldenBolts
Contents of HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/28
================================================================================
PAGE 15 of 28
================================================================================

Data Manipulation
IMPORT statement imports ﬁles exported from other HIVE instances into
an internal table
An imported table is located in a default HIVE location in HDFS
IMPORT EXTERNAL statement imports a ﬁle exported from other HIVE
instances into an external table
An imported table is located in a default HIVE location in HDFS
IMPORT table new_part FROM '/user/tpchr/part';
 HQL
drwxrwxr- x - janusz supergroup 0  2017-07-09
14:56 /user/hive/warehouse/ new_part
Importing data from HDFS
IMPORT EXTERNAL table new_extpart FROM '/user/tpchr/
part';
Importing external table from HDFS
drwxrwxr- x - janusz supergroup 0  2017-07-09
15:04 /user/hive/warehouse/ new_extpart
Contents of HDFS
TOP                    ISIT312 Big Data Management, SIM S4 2025 15/28
================================================================================
PAGE 16 of 28
================================================================================

Data Manipulation
ORDER BY sorts the results of SELECT statement
An order is maintained across all of the output from every reducer and
global sort is performed using only one reducer
SORT BY does the same job as ORDER BY and indicates which columns
to sort when ordering the reducer input records
SORT BY completes sorting before sending data to the reducer
SORT BY statement does not perform a global sort and only makes sure
data is locally sorted in each reducer
SELECT p_partkey,  p_name
FROM part
ORDER BY p_name ASC;
ORDER BY clause
SET mapred. reduce.tasks = 2;
SELECT p_partkey,  p_name
FROM part
SORT BY p_name ASC;
SORT BY clause
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/28
================================================================================
PAGE 17 of 28
================================================================================

Data Manipulation
When DISTRIBUTE BY clause is applied rows with matching column
values are partitioned by the same reducer
DISTRIBUTE BY clause is similar to GROUP BY in relational systems in
terms of deciding which reducer is used to distribute the mapper
When using with SORT BY, DISTRIBUTE BY must be speciﬁed before
the SORT BY statement
SELECT p_partkey,  p_name FROM part
DISTRIBUTE BY p_partkey
SORT BY p_name;
DISTRIBUTE BY clause
TOP                    ISIT312 Big Data Management, SIM S4 2025 17/28
================================================================================
PAGE 18 of 28
================================================================================

Data Manipulation
CLUSTER BY clause is a shorthand operator to perform
DISTRIBUTE BY and SORT BY operations on the same group of
columns.
ORDER BY performs a global sort, while CLUSTER BY sorts in each
distributed group
To fully utilize all the available reducers we can do CLUSTER BY ﬁrst and
then ORDER BY
SELECT p_partkey,  p_name
FROM part
CLUSTER BY p_name;
CLUSTER BY clause
SELECT p_partkey,  p_name
FROM part
CLUSTER BY p_name
ORDER BY p_name;
CLUSTER BY clause
TOP                    ISIT312 Big Data Management, SIM S4 2025 18/28
================================================================================
PAGE 19 of 28
================================================================================

Hive Programming
Outline
Data Selection and Scope
Data Manipulation
Data Aggregation and Sampling
TOP                    ISIT312 Big Data Management, SIM S4 2025 19/28
================================================================================
PAGE 20 of 28
================================================================================

Data Aggregation and Sampling
Hive supports several aggregation functions, analytic functions working
with GROUP BY and PARTITION BY, and windowing clauses
Hive supports advanced aggregation by using GROUPING SETS,
ROLLUP, CUBE, analytic functions, and windowing
Basic aggregation uses GROUP BY clause and aggregation functions
To aggregate into sets a function collect_set can be used
SELECT p_type,  count(*)
FROM part
GROUP BY p_type;
GROUP BY clause
SELECT p_type,  collect_set( p_name), count(*)
FROM part
GROUP BY p_type;
GROUP BY clause with collect_set function
TOP                    ISIT312 Big Data Management, SIM S4 2025 20/28
================================================================================
PAGE 21 of 28
================================================================================

Data Aggregation and Sampling
GROUPING SETS clause implements advanced multiple GROUP BY
operations against the same set of data
ROLLUP clause allows to calculate multiple levels of aggregations across a
speciﬁed group of dimensions
CUBE clause allows to create aggregations over all possible subsets of
attributes in a given set
SELECT p_type, p_name, count(*)
FROM part
GROUP BY p_type, p_name
GROUPING SETS ( (p_type), (p_name) );
GROUPING SETS clause
SELECT p_type, p_name, count(*)
FROM part
GROUP BY p_type, p_name WITH ROLLUP;
ROLLUP clause
SELECT p_type, p_name, count(*)
FROM part
GROUP BY p_type, p_name WITH CUBE;
CUBE clause
TOP                    ISIT312 Big Data Management, SIM S4 2025 21/28
================================================================================
PAGE 22 of 28
================================================================================

Data Aggregation and Sampling
GROUPING__ID function works as an extension to distinguish entire
rows from each other
HAVING can be used for the conditional ﬁltering of GROUP BY results
SELECT GROUPING__ID,  p_type, p_name, count(*)
FROM part
GROUP BY p_type,  p_name WITH CUBE
ORDER BY grouping__id;
GROUPING__ID function
SELECT GROUPING__ID,  p_type, p_name, count(*)
FROM part
GROUP BY p_type,  p_name WITH CUBE
HAVING count(*) > 1
ORDER BY grouping__id;
GROUPING__ID function
TOP                    ISIT312 Big Data Management, SIM S4 2025 22/28
================================================================================
PAGE 23 of 28
================================================================================

Data Aggregation and Sampling
Analytic functions scan multiple input rows to compute each output
value
Analytic functions are usually used with OVER, PARTITION BY,
ORDER BY, and the windowing speciﬁcation
Analytic functions operate on windows where the input rows are
ordered and grouped using ﬂexible conditions expressed through an
OVER PARTITION clause
Syntax is the following
For standard aggregation function (arg1,..., argn) can be either
COUNT(), SUM(), MIN(), MAX(), or AVG()
      
function (arg1,..., argn)
OVER ([PARTITION BY <...>]
[ORDER BY <....>] [])
Syntax of analytic functions
TOP                    ISIT312 Big Data Management, SIM S4 2025 23/28
================================================================================
PAGE 24 of 28
================================================================================

Data Aggregation and Sampling
Typical aggregations implemented as analytic functions in the following
way
Other analytic functions are used as follows
      
SELECT p_name,
       COUNT(*) OVER (PARTITION BY p_name)
FROM PART;
PARTITION BY clause
      
SELECT l_orderkey,  l_partkey,  l_quantity,
       RANK() OVER (ORDER BY l_quantity),
       DENSE_RANK() OVER (ORDER BY l_quantity)
FROM lineitem;
ORDER BY clause
      
SELECT l_orderkey,  l_partkey,  l_quantity,
       RANK() OVER (PARTITION BY l_orderkey ORDER BY l_quantity),
       DENSE_RANK() OVER (PARTITION BY l_orderkey ORDER BY l_quantity)
FROM lineitem;
PARTITION BY clause
TOP                    ISIT312 Big Data Management, SIM S4 2025 24/28
================================================================================
PAGE 25 of 28
================================================================================

Data Aggregation and Sampling
More analytic functions ...
      
SELECT l_orderkey,  l_partkey,  l_quantity,
       FIRST_VALUE( l_quantity)  OVER (PARTITION BY l_orderkey ORDER BY l_quantity),
       LAST_VALUE( l_quantity)  OVER (PARTITION BY l_orderkey ORDER BY l_quantity)
FROM lineitem;;
PARTITION BY clause
      
SELECT l_orderkey,  l_partkey,  l_quantity,
       MAX( l_quantity)  OVER (PARTITION BY l_orderkey ORDER BY l_partkey
                             ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)
FROM lineitem;
PARTITION BY clause
      
SELECT l_orderkey,  l_partkey,  l_quantity,
       MAX( l_quantity)  OVER (PARTITION BY l_orderkey ORDER BY l_partkey
                             ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)
FROM lineitem;
PARTITION BY clause
TOP                    ISIT312 Big Data Management, SIM S4 2025 25/28
================================================================================
PAGE 26 of 28
================================================================================

Data Aggregation and Sampling
When data volume is extra large we can use a subset of data to speed
up data analysis.
Random sampling uses the RAND() function and LIMIT clause to get the
samples of data
DISTRIBUTE and SORT clauses are used here to make sure the data is
also randomly and e "ciently distributed among mappers and reducers
Bucket table sampling is a special sampling optimized for bucket tables
      
SELECT *
FROM lineitem DISTRIBUTE BY RAND() SORT BY RAND() LIMIT 5;
DISTRIBUTE BY clause
SELECT *
FROM customer TABLESAMPLE( BUCKET 3 OUT OF 8 ON rand());
Bucket sampling
TOP                    ISIT312 Big Data Management, SIM S4 2025 26/28
================================================================================
PAGE 27 of 28
================================================================================

Data Aggregation and Sampling
Block sampling allows to randomly pick up N rows of data, percentage (n
percentage) of data size, or N byte size of data
SELECT *
FROM lineitem TABLESAMPLE( 4 ROWS);
Block sampling
SELECT *
FROM lineitem TABLESAMPLE( 50 PERCENT);
Block sampling
SELECT *
FROM lineitem TABLESAMPLE( 20B);
Block sampling
TOP                    ISIT312 Big Data Management, SIM S4 2025 27/28
================================================================================
PAGE 28 of 28
================================================================================

References
Gross C., GuptaA., Shaw S., Vermeulen A. F., Kjerrumgaar D., Practical
Hive: A guide to Hadoop's Data Warehouse System, Apress 2016,
Chapter 4 (Available through UOW library)
Lee D., Instant Apache Hive essentials how-to: leverage your knowledge
of SQL to easily write distributed data processing applications on
Hadoop using Apache Hive, Packt Publishing Ltd. 2013 (Available
through UOW library)
Apache Hive TM, https://hive.apache.org/
TOP                    ISIT312 Big Data Management, SIM S4 2025 28/28
================================================================================


################################################################################
FILE: 14physicaldwdesign.pdf
################################################################################

================================================================================
PAGE 1 of 23
================================================================================

         ISIT312 Big Data Management
Physical Data Warehouse Design
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 23
================================================================================

Physical Data Warehouse Design
Outline
Techniques for Physical Data Warehouse Design
Materialized View
Indexes for Data Warehouses
Evaluation of Star Queries
Data Warehouse Partitioning
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/23
================================================================================
PAGE 3 of 23
================================================================================

Techniques for Physical Data Warehouse Design
Materialized Views
Indexing
PartitioningA view physically stored in the DB
Typical problems: view update, view selection-
-
Used in Data Warehouse together with materialized views
Speciﬁc for Data Warehouse: bitmap and join indexes-
-
Divides the contents of a relational table into several ﬁles
Horizontal and vertical partitioning-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/23
================================================================================
PAGE 4 of 23
================================================================================

Physical Data Warehouse Design
Outline
Techniques for Physical Data Warehouse Design
Materialized View
Indexes for Data Warehouses
Evaluation of Star Queries
Data Warehouse Partitioning
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/23
================================================================================
PAGE 5 of 23
================================================================================

Materialized Views
Materialized view  is a relational table that contains the rows that would
be returned by the view deﬁnition - usually SELECT statement of SQL
If we consider relational views as stored queries  then materialized views
can be considered as stored results
Materialized views  are created and used to reduce an amount of time
needed to compute SELECT statements, for example join materialized
views eliminate the needs to join the relational table
There are two ways how materialized view can be used:
In brute force method SQL is written to explicitly access the view
Transparent query rewrite  method is applied when a query optimizer
detects that a query can be computed against a materialized view
instead of the source relational tablesbrute force method
transparent query rewrite-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/23
================================================================================
PAGE 6 of 23
================================================================================

Materialized Views
View maintenance  means that when the base relational tables are
updated then a materialized view must be updated too
Incremental view maintenance  means that updated view is computed
from the individual modiﬁcations to the relational tables and not from
the entire relational tables
Creating materialized view
Direct access to materialized view
      
CREATE MATERIALIZED VIEW MV_ORDERS
REFRESH ON COMMIT
ENABLE QUERY REWRITE
AS( SELECT O_ORDERKEY, O_CUSTKEY, O_TOTALPRICE, O_ORDERDATE 
    FROM ORDERS
    WHERE O_ORDERDATE > TO_DATE('31-DEC-1986','DD-MON-YYYY') );
Creating materialized view
      
SELECT * 
FROM MV_ORDERS 
WHERE O_ORDERDATE = TO_DATE('01-JAN-1992','DD-MON-YYYY')
Directaccess to materialized view
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/23
================================================================================
PAGE 7 of 23
================================================================================

Materialized Views
Access to materialized view  through query rewriting
      
SELECT O_ORDERKEY,  O_CUSTKEY,  O_TOTALPRICE,  O_ORDERDATE 
FROM ORDERS
WHERE O_ORDERDATE >  TO_DATE( '31-DEC-1986', 'DD-MON-YYYY');
Indirect access to materialized view through query rewriting
The results from EXPLAIN PLAN  statement -
PLAN_TABLE_OUTPUT 
----------------------------------------------------------------------------------
| 0  | SELECT STATEMENT            |            |  108K| 2539K| 507 (1)| 00:00:01 | 
|* 1 | MAT_VIEW REWRITE ACCESS FULL|  MV_ORDERS |  108K| 2539K| 507 (1)| 00:00:01 | 
----------------------------------------------------------------------------------  
Predicate Information ( identified by operation id):
1 - filter("MV_ORDERS". "O_ORDERDATE"> TO_DATE(' 1986-12-31 00:00:00',  
           'syyyy-mm-dd hh24:mi:ss'))
Query processing plan
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/23
================================================================================
PAGE 8 of 23
================================================================================

Physical Data Warehouse Design
Outline
Techniques for Physical Data Warehouse Design
Materialized View
Indexes for Data Warehouses
Evaluation of Star Queries
Data Warehouse Partitioning
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/23
================================================================================
PAGE 9 of 23
================================================================================

Indexes for Data Warehouses
An index provides a quick way to locate data of interest
Sample query
With the help of an index over a column EmployeeKey (primary key in
EMPLOYEE table), a single disk block access will su!ce to answer the
query
Without this index, we should perform a complete scan of table
EMPLOYEE
Drawback: Almost every update on an indexed attribute also requires an
index update
Too many indexes may degrade performance
Most popular indexing techniques in relational databases include B*-
trees and bitmap indexes
      
SELECT *
FROM EMPLOYEE
WHERE EmployeeKey = 007;
SELECT statement with equality condition in WHERE clause
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/23
================================================================================
PAGE 10 of 23
================================================================================

B*-tree index implementation
B*-tree  can be traversed either:
vertically from root to leaf level  of a tree
horizontally  either from left corner of leaf level  to right corner of leaf level  or the
opposite
vertically and later on horizontally  either towards left lower corner or right lower
corner of leaf level-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/23
================================================================================
PAGE 11 of 23
================================================================================

Bitmap Indexes
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/23
================================================================================
PAGE 12 of 23
================================================================================

Bitmap Indexes: Example
Products having between 45 and 55  pieces per unit, and  with a unit price
between 100 and 200
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/23
================================================================================
PAGE 13 of 23
================================================================================

Indexes for Data Warehouses: Requirements
Symmetric partial match queries
Indexing at multiple levels of aggregation
E!cient batch update
Sparse dataAll dimensions of the cube should be symmetrically indexed, to be searched
simultaneously-
Summary tables must be indexed in the same way as base nonaggregated
tables-
The refreshing time of a data warehouse must be considered when designing
the indexing schema-
Typically, only 20% of the cells in a data cube are nonempty
The indexing schema must deal e !ciently with sparse and nonsparse data-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/23
================================================================================
PAGE 14 of 23
================================================================================

Physical Data Warehouse Design
Outline
Techniques for Physical Data Warehouse Design
Materialized View
Indexes for Data Warehouses
Evaluation of Star Queries
Data Warehouse Partitioning
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/23
================================================================================
PAGE 15 of 23
================================================================================

Star Queries
Queries over star schemas are called star queries
Join the fact table with the dimension tables
A typical star query: total sales of discontinued products, by customer
name and product name
Three basic steps to evaluate the query:
      
SELECT ProductName,  CustomerName,  SUM(SalesAmount)
FROM Sales S,  Customer C,  Product P
WHERE S.CustomerKey = C.CustomerKey AND S.ProductKey = P.ProductKey AND
      P.Discontinued = 'Yes'
GROUP BY C.CustomerName,  P.ProductName;
Star query
(1) Evaluation of the join conditions
(2) Evaluation of the selection conditions over the dimensions
(3) Aggregation of the tuples that passed the ﬁlter-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 15/23
================================================================================
PAGE 16 of 23
================================================================================

Evaluation of Star Queries with Bitmap Indexes:
Example
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/23
================================================================================
PAGE 17 of 23
================================================================================

Evaluation of Star Queries using Bitmap Indexes
Evaluation of star query requires
Example of query evaluationa B+ tree over CustomerKey  and ProductKey
Bitmap indexes on the foreign key columns in Sales  and on
Discontinued in Product-
-
(1) Obtain the record numbers of the records that satisfy the condition
Discontinued = 'Yes'
Answer: Records with ProductKey  values p2 , p4  , and p6
(2) To access the bitmap vectors in Sales  with these labels perform a join
between Product  and Sales
(3) Vectors labeled p2 and p4 match, no fact record for p6
(4) Obtain the values for the CustomerKey  in these records ( c2 andc3 )
(5) Use B+-tree index on ProductKey  and CustomerKey  to ﬁnd the
names of products and customers
(6) Answer: ( cust2,prod2,200 ) and (cust3,prod4,100 )-
-
-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 17/23
================================================================================
PAGE 18 of 23
================================================================================

Physical Data Warehouse Design
Outline
Techniques for Physical Data Warehouse Design
Materialized View
Indexes for Data Warehouses
Evaluation of Star Queries
Data Warehouse Partitioning
TOP                    ISIT312 Big Data Management, SIM S4 2025 18/23
================================================================================
PAGE 19 of 23
================================================================================

Data Warehouse Partitioning
Partitioning (or fragmentation) divides a table into smaller data sets
(each one called a partition)
Applied to tables and indexes
Vendors provide several d "erent partitioning methods
Vertical partitioning splits the attributes of a table into groups that can
be independently stored
Horizontal partitioning  divides a table into smaller tables with same
structure than the full tableE.g., most often used attributes are stored in one partition, less often used
attributes in another one
More records ﬁt into main memory, reducing their processing time-
-
For example, if some queries require the most recent data, partition
horizontally according to time-
TOP                    ISIT312 Big Data Management, SIM S4 2025 19/23
================================================================================
PAGE 20 of 23
================================================================================

Queries over Partitioned Databases
Partition pruning  is the typical way of improving query performance
using partitioning
Example: A Sales  fact table in a warehouse can be partitioned by
month
A query requesting orders for a single month only needs to access the
partition of such a month
Joins  also enhanced by using partitioning:
When the two tables are partitioned on the join attributes
When the reference table is partitioned on its primary key
Large join is broken down into smaller joins-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 20/23
================================================================================
PAGE 21 of 23
================================================================================

Partitioning Strategies
Three partitioning strategies: Range partitioning, hash partitioning, and
list partitioning
Range partitioning maps records to partitions based on ranges of values
of the partitioning key
Time dimension is a natural candidate for range partitioning
Example: A table with a date  column deﬁned as the partitioning key
Hash partitioning  uses a hashing algorithm over the partitioning key to
map records to partitionsJanuary-2012  partition will contain rows with key values from January 1 to
January 31, 2012-
Hashing algorithm distributes rows among partitions in a uniform fashion,
yielding, ideally, partitions of the same size
Typically used when partitions are distributed in several devices, and when data
are not partitioned based on time-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 21/23
================================================================================
PAGE 22 of 23
================================================================================

Partitioning Strategies
List partitioning speciﬁes a list of values for the partitioning key
Some vendors (e.g. Oracle) support the notion of composite partitioning,
combining the basic data distribution methods
Thus, a table can be range partitioned, and each partition can be
subdivided using hash partitioning
TOP                    ISIT312 Big Data Management, SIM S4 2025 22/23
================================================================================
PAGE 23 of 23
================================================================================

References
A. VAISMAN, E. ZIMANYI, Data Warehouse Systems: Design and
Implementation, Chapter 7 Physical Data Warehouse Design, Springer
Verlag, 2014
TOP                    ISIT312 Big Data Management, SIM S4 2025 23/23
================================================================================


################################################################################
FILE: 15hbasedatamodel.pdf
################################################################################

================================================================================
PAGE 1 of 28
================================================================================

TOP               ISIT312 Big Data Management, SIM, Session 4, 2020    ISIT312 Big Data Management
HBase Data Model
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 28
================================================================================

HBase Data Model
Outline
Background
Logical view of data
Design fundamentals
Physical implementation
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/28
================================================================================
PAGE 3 of 28
================================================================================

Background
Hbase  is open source distributed database based on a data model of
Google's BigTable
HBase  provides a BigTable  view of data stored in HDFS
HBase  is also called as Hadoop Data Base
HBase  still provides a tabular view of data however it is also very
di!erent from the traditional relational data model
HBase  data model is a sparse, distributed, persistent multidimensional
sorted map
It is indexed by a row key, column key, and timestamp
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/28
================================================================================
PAGE 4 of 28
================================================================================

HBase Data Model
Outline
Background
Logical view of data
Design fundamentals
Physical implementation
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/28
================================================================================
PAGE 5 of 28
================================================================================

Logical  view  of  data
HBase  organizes data into tables
HBase table  consists of rows
Each row is uniquely identiﬁed by a row key
Data within a row is grouped by a column family
Column families  have an important impact on the physical
implementation  of HBase table
Every row has the same column families although some of them can be
empty
Data within a column family is addressed via its column qualiﬁer, or
simply, column name
Hence, a combination of row key, column family, and column qualiﬁer
uniquely identiﬁes a cell
Values in cells do not have a data type and are always treated as
sequences of bytesTOP                    ISIT312 Big Data Management, SIM S4 2025 5/28
================================================================================
PAGE 6 of 28
================================================================================

Logical  view  of  data
Values  within a cell have  multiple versions
Versions  are identiﬁed by their version number, which by default is a
timestamp  when the cell was writtenTOP                    ISIT312 Big Data Management, SIM S4 2025 6/28
================================================================================
PAGE 7 of 28
================================================================================

Logical  view  of  data
If a timestamp  is not determined at write time, then the current
timestamp  is used
If a timestamp  is not determined during a read, the latest one  is
returned
The maximum allowed number of cell value versions  is determined for
each column family
The default number of cell versions is three
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/28
================================================================================
PAGE 8 of 28
================================================================================

Logical  view  of  data
A view of HBase table  as a nested structure
{"Row-0001":
            { "Home":
                    { "Name":
                            { "timestamp-1": "James"}
                     "Phones":
                              { "timestamp-1": "2 42 214339"
                               "timestamp-2": "2 42 213456"
                               "timestamp-3": "+61 2 4567890"}
                    }
             "Office":
                      { "Phone":
                               { "timestamp-4": "+64 345678"}
                       "Address":
                                 { "timestamp-5": "10 Ellenborough Pl"}
                      }
            }
}
  
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/28
================================================================================
PAGE 9 of 28
================================================================================

Logical  view  of  data
A view of HBase table  as a nested structure
{"Row-0002":
            { "Home":
                    { "Name":
                            { "timestamp-6": "Harry"}
                     "Phones":
                              { "timestamp-7": "2 42 214234"}
                    }
             "Office":
                      { "Phone":
                               { "timepstamp-8": "+64 345678"}
                       "Address":
                                 { "timestamp-9": "10 Bong Bong Rd"
                                  "timestamp-10": "23 Victoria Rd"}
                      }
            }
}
    
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/28
================================================================================
PAGE 10 of 28
================================================================================

Logical  view  of  data
A key can be row key or a combination of a row key, column family,
qualiﬁer , and timestamp  depending on what supposed to be retrieved
If all the cells in a row are of interest then a key is a row key
If only speciﬁc cells are of interest, the appropriate column families and
qualiﬁers  are a part of a key
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/28
================================================================================
PAGE 11 of 28
================================================================================

HBase Data Model
Outline
Background
Logical view of data
Design fundamentals
Physical implementation
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/28
================================================================================
PAGE 12 of 28
================================================================================

Design  Fundamentals
When designing Hbase table  we have to consider the following
questions:
In fact HBase table is a four level hierarchical structure  where a table
consists of rows, rows consists of column families, column families
consist of columns  and columns  consists of versions
If cells contain the keys then HBase table  becomes a network /graph
structureWhat should be a row key and what should it contain?
How many column families should a table have?
What columns  (qualiﬁers ) should be included in each column family?
What information should go into the cells?
How many versions should be stored for each cell?-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/28
================================================================================
PAGE 13 of 28
================================================================================

Design  Fundamentals
Important facts to remember:
Indexing  is performed only for a row key
Hbase tables  are sorted based on a row key
Everything in Hbase tables  is stored as untyped sequence of bytes
Atomicity is guaranteed only at a row level and there are no multi-row
transactions
Column families  must be deﬁned at Hbase table  creation time
Column qualiﬁers  are dynamic and can be deﬁned at write time
Column qualiﬁers  are stored as sequences of bytes such that they can
represent data-
-
-
-
-
-
-
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/28
================================================================================
PAGE 14 of 28
================================================================================

Design  Fundamentals
Implementation of Entity type
{"007":
       {"CUSTOMER":
                   { "first-name":  {"timestamp-1": "James"},
                    "last-name":  {"timestamp-2": "Bond"},
                    "phone":  {"timestamp-1": "007-007"},
                    "email":  {"timestamp-1": "jb@mi6.com"}
                   }
       }
}
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/28
================================================================================
PAGE 15 of 28
================================================================================

Design  Fundamentals
Implementation of one-to-one relationship
{"Sales":
         { "DEPARTMENT":
                       { "dname": {"timestamp-1": "Sales"},
                       { "budget":  {"timestamp-1": "1000"}
                       }
         }
         { "MANAGER":
                    { "enumber":  {"timestamp-2": "007"},
                     "first-name":  {"timestamp-3": "James"},
                     "last-name":  {"timestamp-4": "Bond"}
                    }
         }
}
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 15/28
================================================================================
PAGE 16 of 28
================================================================================

Design  Fundamentals
Implementation of one-to-many relationship
{"007":
       {"EMPLOYEE":
                   { "enumber":  {"timestamp-1": "007"},
                    "first-name":  {"timestamp-2": "James"},
                    "last-name":  {"timestamp-3": "Bond"},
                    "department":{"timestamp-4": "Sales"}
                   }
       }
}
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/28
================================================================================
PAGE 17 of 28
================================================================================

Design  Fundamentals
Another implementation of one-to-many relationship
{"Sales":
       {"DEPARTMENT":
                     { "dname": {"timestamp-1": "Sales"},
                     { "budget":  {"timestamp-1": "1234567"}
                     }
       }
       {"EMPLOYEE":
                   { "007": {"timestamp-2": "James Bond"},
                    "008":  {"timestamp-3": "Harry Potter"},
                    "009":  {"timestamp-4": "Robin Hood" }
                    ...
                   }
       }
}
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 17/28
================================================================================
PAGE 18 of 28
================================================================================

Design  Fundamentals
Yet another implementation of one-to-many relationship
{"Sales":
         { "DEPARTMENT":
                       { "dname": {"timestamp-1": "Sales"},
                       { "budget":  {"timestamp-1": "1000"}
         }
         { "HAS-EMPLOYEES":
                          { "employees":  {"timestamp-2": "007 James Bond"},
                                        { "timestamp-3": "008 Harry Potter"},
                                        { "timestamp-4": "009 Robin Hood"}
                                        ...
                          }
         }
}
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 18/28
================================================================================
PAGE 19 of 28
================================================================================

Design  Fundamentals
Implementation of many-to-many relationship
{"participation-1":
                   { "EMPLOYEE":
                               { "enumber":  {"timestamp-1": "007"},
                                "first-name":  {"timestamp-2": "James"},
                                "last-name":  {"timestamp-3": "Bond"},
                                "pnumber":  {"timestamp-4": "project-1"},
                                           { "timestamp-5": "project-2"}
                                            ...
                               }
                  }   
}
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 19/28
================================================================================
PAGE 20 of 28
================================================================================

Design  Fundamentals
Another implementation of many-to-many relationship
{"participation-1":
                   { "PROJECT":
                              { "pnumber":  {"timestamp-1": "project-1"},
                               "budget:":  {"timestamp-2": "12345.25"},
                               "employee":  {"timestamp-3": "007"},
                                           { "timestamp-4": "008"},
                                           { "timestamp-5": "009"},
                                            ...
                              }
                   }   
}
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 20/28
================================================================================
PAGE 21 of 28
================================================================================

Design  Fundamentals
Another implementation of many-to-many relationship
{"participation-1":
                   {"PARTICIPATION":
                                   {"pnumber": {"timestamp-1":"project-1"},
                                    "employee": {"timestamp-2":"employee-007"}
                                   }
                   }
}
HBase Table
{"participation-2":
                   {"PARTICIPATION":
                                   {"pnumber": {"timestamp-1":"project-1"},
                                    "employee": {"timestamp-2":"employee-008"}
                                   }
                   }
}
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 21/28
================================================================================
PAGE 22 of 28
================================================================================

Design  Fundamentals
Note, that it is possible to group in one Hbase table  rows of di !erent
types
{"employee-007":
                {"EMPLOYEE":
                            {"enumber": {"timestamp-1":"007"},
                             "first-name": {"timestamp-2":"James"},
                             "last-name": {"timestamp-3":"Bond"}
                            }
                }
},
{"project-1":
             {"PROJECT":
                        {"pnumber": {"timestamp-4":"1"},
                         "budget": {"timestamp-5":"12345.25"}
                                   }
                   }
},
{"participation-2":
                   {"PARTICIPATION":
                                   {"pnumber": {"timestamp-1":"project-1"},
                                    "employee": {"timestamp-2":"employee-007"}
                                   }
                   }
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 22/28
================================================================================
PAGE 23 of 28
================================================================================

Design  Fundamentals
Implementation of fact with dimensions
{"1234567":
                {"MEASURE":
                           {"amount": {"timestamp-1":"1000000"}
                           },
                 "BUYER":
                           {"phone": {"timestamp-1":"242214339"},
                            "first-name": {"timestamp-1":"James"},
                            "last-name": {"timestamp-1":"Bond"}
                           },
                 "SELLER":
                           {"phone": {"timestamp-1":"242215612"},
                            "first-name": {"timestamp-1":"Harry"},
                            "last-name": {"timestamp-1":"potter"}
                           }
                }
}
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 23/28
================================================================================
PAGE 24 of 28
================================================================================

Design  Fundamentals
Implementation of graph structure
{"A":
     {"R":
          {"1":{"timestamp-1":"B"},
           "2":{"timestamp-1":"D"}
          },
      "S":
          {"1":{"timestamp-1":"C"}
          }
     }
}
HBase Table
{"B":
     {"R":
          {"1":{"timestamp-1":"D"}
          }
     }
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 24/28
================================================================================
PAGE 25 of 28
================================================================================

Design  Fundamentals
Implementation of graph structure
      
{"C":
     {"S":
          {"1":{"timestamp-1":"D"}
          }
     }
}
HBase Table
{"D":
}
HBase Table
TOP                    ISIT312 Big Data Management, SIM S4 2025 25/28
================================================================================
PAGE 26 of 28
================================================================================

HBase Data Model
Outline
Background
Logical view of data
Design fundamentals
Physical implementation
TOP                    ISIT312 Big Data Management, SIM S4 2025 26/28
================================================================================
PAGE 27 of 28
================================================================================

Physical  implementation
HBase  is a database built on top of HDFS
HBase tables can scale up to billions of rows and millions of columns
Because Hbase tables  can grow up to terabytes or even petabytes,
Hbase tables  are split into smaller chunks of data  that are distributed
across multiple servers
Chunks of data  are called as regions  and servers that host regions  are
called as region servers
Region servers  are usually collocated with data nodes  of HDFS
The splits  of Hbase tables  are usually horizontal , however, it is also
possible to beneﬁt from vertical splits separating column families
Region assignments  happen when Hbase table  grows in size or when a
region server is malfunctioning or when a new region server is added
TOP                    ISIT312 Big Data Management, SIM S4 2025 27/28
================================================================================
PAGE 28 of 28
================================================================================

References
Kerzner M., Maniyam S., HBase Design Patterns, Packt Publishing 2014
(Available from UoW Library)
Jiang, Y. HBase Administration Cookbook, Packt Publishing, 2012
(Available from UoW Library)
Dimiduk N., Khurana A., HBase in Action, Mannig Publishers, 2013
Spaggiari J-M., O'Dell K., Architecting HBase Applications, O'Reilly, 2016
TOP                    ISIT312 Big Data Management, SIM S4 2025 28/28
================================================================================


################################################################################
FILE: 16hbaseoperations.pdf
################################################################################

================================================================================
PAGE 1 of 21
================================================================================

    ISIT312 Big Data Management
HBase Operations
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 21
================================================================================

HBase Operations
Outline
HBase shell
Data deﬁnition commands
Data manipulation commands
HBase Java API
TOP                    ISIT312 Big Data Management, SIM S4 2025 2/21
================================================================================
PAGE 3 of 21
================================================================================

HBase shell
HBase  provides extensible JRuby-based (Java Interactive Ruby - JIRB) shell
as a feature to execute some commands
The shell is a typical Read–Eval–Print-Loop (REPL) shell also known as a
language shell
It is a simple, interactive computer programming environment that takes
single user inputs evaluates them, and returns the result to the user; a
program written in a REPL  environment is executed piecewise
It means that HBase shell allows for computation of Ruby scripts and
brings all features enabled in JIRB shell
It allows to process a script of HBase  commands saved in a ﬁle file-
name.hb  in the following way:
source 'file-name.hb'
 HBase shell
TOP                    ISIT312 Big Data Management, SIM S4 2025 3/21
================================================================================
PAGE 4 of 21
================================================================================

HBase Operations
Outline
HBase shell
Data deﬁnition commands
Data manipulation commands
HBase Java API
TOP                    ISIT312 Big Data Management, SIM S4 2025 4/21
================================================================================
PAGE 5 of 21
================================================================================

Data definition commands
In Hbase , a set of data deﬁnition  commands includes: create , list ,
describe , disable , disable_all , enable , enable_all , drop ,
drop_all , show_filters , alter , alter_status
Create a table 'student'  with a column family 'personal'
Show a structure of a table 'student'
Implement a column family 'personal'  in transient memory
Add a column family 'uni'  to a table 'student'
create 'student', 'personal' 
 HBase shell
describe 'student' 
 HBase shell
alter 'student',{NAME=>'personal', IN_MEMORY=>true}  
 HBase shell
alter 'student',{NAME=>'uni', VERSIONS=>'4'}  
 HBase shell
TOP                    ISIT312 Big Data Management, SIM S4 2025 5/21
================================================================================
PAGE 6 of 21
================================================================================

Data definition commands
Delete a column family 'uni' from a table 'student'
Add a column family 'university' to a table student and allow for 5
versions in each cell in the column family
Increase a number of allowed versions in a column family
'personal' to 3
alter 'student', 'delete'=>'uni' 
 HBase shell
alter 'student',{NAME=>'university', VERSIONS=>5 } 
 HBase shell
alter 'student',{NAME=>'personal', VERSIONS=>3 } 
 HBase shell
TOP                    ISIT312 Big Data Management, SIM S4 2025 6/21
================================================================================
PAGE 7 of 21
================================================================================

HBase Operations
Outline
HBase shell
Data deﬁnition commands
Data manipulation commands
HBase Java API
TOP                    ISIT312 Big Data Management, SIM S4 2025 7/21
================================================================================
PAGE 8 of 21
================================================================================

Data Manipulation commands
In Hbase , a set of data manipulation  commands includes: count , put,
get, delete , delete_all , truncate , scan
Put a value 'James'  into a cell in a column family 'personal' ,
qualiﬁcation 'first-name' , row key '007' ,
Put a value 'Bond' into a cell in a column family 'personal' ,
qualiﬁcation 'last-name' , row key '007'
Put a value '01-OCT-1960' into a cell in a column family
'personal' , qualiﬁcation dob' , row key '007' ,
put 'student', '007','personal:first-name', 'James' 
 HBase shell
put 'student', '007','personal:last-name', 'Bond' 
 HBase shell
put 'student', '007','personal:dob', '01-OCT-1960' 
 HBase shell
TOP                    ISIT312 Big Data Management, SIM S4 2025 8/21
================================================================================
PAGE 9 of 21
================================================================================

Data Manipulation commands
List the contents of a table 'student'
Put a value '02-OCT-1960'  as the second version into a cell in a
column family 'personal' , qualiﬁcation dob' , row key '007' ,
Get no more than 5 versions of a cell 'dob' in a column family
'personal' from a row '007' in a table 'student'
Get no more than 5 versions of a cell 'dob'  in a column family
'personal' , from a table 'student'
scan 'student' 
 HBase shell
put 'student', '007','personal:dob', '02-OCT-1960' 
 HBase shell
get 'student', '007',{COLUMN=>'personal:dob', VERSIONS=>5 } 
 HBase shell
scan 'student',{COLUMN=>'personal:dob', VERSIONS=>5 } 
 HBase shell
TOP                    ISIT312 Big Data Management, SIM S4 2025 9/21
================================================================================
PAGE 10 of 21
================================================================================

Data Manipulation commands
Get all column families in a row '666' in a table 'student'
Get no more than 5 versions of values from all cells in a column family
'grade'  in a row '666'  in a table 'student'
Get no more than 5 versions of values from a cell 'CSCI235' in a
column family 'grade' in a row '666'  in a table 'student'
Get no more than 5 versions of values from a cell 'dob' in a column
family 'grade' in a row '666' in a table 'student'
get 'student', '666' 
 HBase shell
get 'student', '666',{COLUMN=>'grade', VERSIONS=>5 } 
 HBase shell
get 'student', '666',{COLUMN=>'grade:CSCI235', VERSIONS=>5 } 
 HBase shell
get 'student', '007',{COLUMN=>'personal:dob', VERSIONS=>5 } 
 HBase shell
TOP                    ISIT312 Big Data Management, SIM S4 2025 10/21
================================================================================
PAGE 11 of 21
================================================================================

Data Manipulation commands
Count total number of rows in a table 'student'
Get entire table 'student' , one version per cell
Get entire table 'student' , at most 5 versions per cell
Get all cells 'dob'  from in a column family 'personal' from entire
table'student' , at most 5 versions per cell
count 'student'
 HBase shell
scan 'student'
 HBase shell
scan 'student',{VERSIONS=>5 }
 HBase shell
scan 'student',{COLUMNS=>'personal:dob', VERSIONS=>5 }
 HBase shell
TOP                    ISIT312 Big Data Management, SIM S4 2025 11/21
================================================================================
PAGE 12 of 21
================================================================================

Data Manipulation commands
Get all cells from the column families 'personal'  and
'university'  from entire table 'student'
Get at most 5 versions of cells 'dob' with timestamps in a range
[1,1502609828830] , from a column family 'personal' from
entire table 'student'
Get at most 5 versions of cells 'dob' with timestamps in a range
[1,1502609828830] , from a column family 'personal'  from entire
table 'student'
scan 'student',{COLUMNS=>['personal', 'university']}
 HBase shell
scan 'student',{COLUMNS=>'personal:dob', TIMERANGE=>[1 ,1502609828830],
         VERSIONS=>5 }
HBase shell
scan 'student',{COLUMNS=>'personal:dob',
         FILTER=>"TimestampsFilter(1,1502609828830)", VERSIONS=>5 }
HBase shell
TOP                    ISIT312 Big Data Management, SIM S4 2025 12/21
================================================================================
PAGE 13 of 21
================================================================================

Data Manipulation commands
Get all cells whose name is >= than 'f' in a table 'student'
Get all rows from a table 'student' that have value of a cell >= than 'J'
Get all rows from a table 'student' that have value of a cell in a range
['J','K']
Get all values of cells 'dob' in a column family 'personal' from rows
in a table 'student' where a cell 'dob' has a value '02-OCT-1960'
scan 'student',{FILTER=>"QualifierFilter(>=,'binary:f')"}
 HBase shell
scan 'student',{FILTER=>"ValueFilter(>=,'binary:J')"}  
 HBase shell
scan 'student',{FILTER=>"ValueFilter(>=,'binary:J') AND 
                ValueFilter(<=,'binary:K')"}  
HBase shell
scan 'student',{COLUMNS=>'personal:dob', FILTER=>
                "QualifierFilter(=,'binary:dob') AND 
        ValueFilter(=,'binary:02-OCT-1960')"}
HBase shell
TOP                    ISIT312 Big Data Management, SIM S4 2025 13/21
================================================================================
PAGE 14 of 21
================================================================================

Data Manipulation commands
Delete a cell 'CSCI235' from a column family 'student'  in a row
'666'  in a table 'student'
Delete entire row '007'  from a table 'student'
delete 'student', '666','grade:CSCI235' 
 HBase shell
deleteall 'student',  '007
 HBase shell
TOP                    ISIT312 Big Data Management, SIM S4 2025 14/21
================================================================================
PAGE 15 of 21
================================================================================

HBase Operations
Outline
HBase shell
Data deﬁnition commands
Data manipulation commands
HBase Java API
TOP                    ISIT312 Big Data Management, SIM S4 2025 15/21
================================================================================
PAGE 16 of 21
================================================================================

HBase Java API
HBase Java Application Program Interface  allows to access HBase tables
from programs written in Java
The client APIs provide both data deﬁnition and data manipulation
features
Creating a table 'my-table'  and column families 'Address'  and
'Name'
import org.apache.hadoop.conf.Configuration; 
import org.apache.hadoop.hbase.HBaseConfiguration; 
import org.apache.hadoop.hbase.HColumnDescriptor; 
import org.apache.hadoop.hbase.HTableDescriptor; 
import org.apache.hadoop.hbase.TableName; 
import org.apache.hadoop.hbase.client.HBaseAdmin; 
Java
public class CreateTable { 
  public static void main(String[] args) throws Exception { 
    Configuration conf = HBaseConfiguration.create(); 
    HBaseAdmin admin = new HBaseAdmin(conf); 
    HTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf("my-table")); 
    tableDescriptor.addFamily(new HColumnDescriptor("Address")); 
    tableDescriptor.addFamily(new HColumnDescriptor("Name")); 
    admin.createTable(tableDescriptor); 
    boolean tableAvailable = admin.isTableAvailable("my-table"); 
    System.out.println("tableAvailable = " + tableAvailable); } }
Java
TOP                    ISIT312 Big Data Management, SIM S4 2025 16/21
================================================================================
PAGE 17 of 21
================================================================================

HBase Java API
Inserting data into HBase table  'my-table'
public class PutRow { 
  public static void main( String[] args)  throws Exception { 
    Configuration conf =  HBaseConfiguration. create(); 
    HTable table =  new HTable(conf, "my-table"); 
    Put put = new Put(Bytes.toBytes("007")); 
    put.add(Bytes.toBytes("Address"),Bytes. toBytes("City"),Bytes. toBytes("Dapto")); 
    put.add(Bytes.toBytes("Address"),Bytes. toBytes("Street"),Bytes. toBytes("Ellenborough"  
    put.add(Bytes.toBytes("Name"),Bytes. toBytes("First"),Bytes. toBytes("James")); 
    put.add(Bytes.toBytes("Name"),Bytes. toBytes("Last"),Bytes. toBytes("Bond")); 
    table. put(put); 
    table. flushCommits(); 
    table. close(); 
  } 
} 
Java
TOP                    ISIT312 Big Data Management, SIM S4 2025 17/21
================================================================================
PAGE 18 of 21
================================================================================

HBase Java API
Getting data from HBase table  'my-table'
import java. util.Map;
import java. util.NavigableMap
Java
public class GetRow {
    public static void main( String[] args)  throws Exception {
        Configuration conf =  HBaseConfiguration. create();
        HTable table =  new HTable(conf, "my-table");
        Get get = new Get(Bytes.toBytes("007"));
        get. setMaxVersions( 3);
        get. addFamily( Bytes.toBytes("Address"));
        get. addColumn( Bytes.toBytes("Name"), Bytes.toBytes("First"));
        get. addColumn( Bytes.toBytes("Name"), Bytes.toBytes("Last"));
Java
// Get a specific value
        Result result =  table.get(get); 
        String row =  Bytes.toString( result.getRow());
Java
TOP                    ISIT312 Big Data Management, SIM S4 2025 18/21
================================================================================
PAGE 19 of 21
================================================================================

HBase Java API
Getting data from HBase table  'my-table'
     
   String specificValue =  Bytes.toString( result.getValue( Bytes.toBytes("Address"),
                                         Bytes. toBytes("City")));
   System. out.println("Latest Address:City is: " + specificValue);
          specificValue =  Bytes.toString( result.getValue( Bytes.toBytes("Address"),
                                 Bytes. toBytes("Street")));
   System. out.println("Latest Address:Street is: " + specificValue);
          specificValue =  Bytes.toString( result.getValue( Bytes.toBytes("Name"),
                                 Bytes. toBytes("First")));
   System. out.println("Latest Name:First is: " + specificValue);
          specificValue =  Bytes.toString( result.getValue( Bytes.toBytes("Name"),
                                 Bytes. toBytes("Last")));
   System. out.println("Latest Name:Last is: " + specificValue)  
Java
TOP                    ISIT312 Big Data Management, SIM S4 2025 19/21
================================================================================
PAGE 20 of 21
================================================================================

HBase Java API
// Traverse entire returned row
        System. out.println("Row key: " + row);
        NavigableMap>> map =  result.getMap();
        for (Map.Entry>> navigableMapEntry :  map.entrySet()) {
            String family =  Bytes.toString( navigableMapEntry. getKey());
            System. out.println("\t" + family);
            NavigableMap>  familyContents =  navigableMapEntry. getValue();
            for (Map.Entry> mapEntry :  familyContents. entrySet()) {
                String qualifier =  Bytes.toString( mapEntry. getKey());
                System. out.println("\t\t" + qualifier);
                NavigableMap qualifierContents =  mapEntry. getValue();
                for (Map.Entry entry :  qualifierContents. entrySet()) {
                    Long timestamp =  entry.getKey();
                    String value =  Bytes.toString( entry.getValue());
                    System. out.printf("\t\t\t%s, %d\n",  value, timestamp);
                }
            }
        }
        table. close();
    }
} 
Java
TOP                    ISIT312 Big Data Management, SIM S4 2025 20/21
================================================================================
PAGE 21 of 21
================================================================================

References
HBase shell commands,
https://learnhbase.wordpress.com/2013/03/02/hbase-
shell-commands/
HBase shell and General commands,
https://www.guru99.com/hbase-shell-general-
commands.html#4
HBase Java API, https://dzone.com/articles/handling-big-
data-hbase-part-4
Kerzner M., Maniyam S., HBase Design Patterns, Packt Publishing 2014
(Available from UoW Library)
Jiang, Y. HBase Adminstration Cookbook, Pack Publishing, 2012
(Available from UoW Library)
Dimiduk N., Khurana A., HBase in Action, Mannig Publishers, 2013
Spaggiari J-M., O'Dell K., Architecting HBase Applications, O'Reilly, 2016
TOP                    ISIT312 Big Data Management, SIM S4 2025 21/21
================================================================================


################################################################################
FILE: 17sparkintroduction.pdf
################################################################################

================================================================================
PAGE 1 of 25
================================================================================

        ISIT312 Big Data Management
Introduction to Spark
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 25
================================================================================

Introduction to Spark
Outline
MapReduce Challenges
Meet Spark !
Features of Spark
Spark Architecture
Overview of Spark Components
Spark Glossary
TOP                    ISIT312 Big Data Management,  SIM S4 2025 2/25
================================================================================
PAGE 3 of 25
================================================================================

MapReduce Challenges
Google introduced MapReduce , which was a breakthrough in the history
of big data technologies
However, Hadoop MapReduce framework releases the developer from
the distributing computing trickiness, its Java API is still too low-level
More importantly, the persistent storage I/O makes the interactive and
iterative computation (two important forms of data processing) very
ine!cient and leads to to potential I/O latencyIt makes the processing of big data feasible and practical-
Think how would you implement an inner join in MapReduce  ?
Hive, Pig and other frameworks based on MapReduce  can help-
-
TOP                    ISIT312 Big Data Management,  SIM S4 2025 3/25
================================================================================
PAGE 4 of 25
================================================================================

MapReduce Challenges
Data Sharing in Hadoop
In the Hadoop MapReduce  framework, the reuse data between computations
(e.g., between two MapReduce  jobs) is to write it to an external stable storage
system, for example HDFS)
Although Hadoop  provides numerous abstractions for data access, data sharing
is slow due to replication, serialisation and persistent storage IO
More than 90% of the time for running a MapReduce job is doing HDFS read-
write operations
Substantial overhead is caused by data replication, persistent storage I/O, and
serialization-
-
-
-
TOP                    ISIT312 Big Data Management,  SIM S4 2025 4/25
================================================================================
PAGE 5 of 25
================================================================================

Introduction to Spark
Outline
MapReduce Challenges
Meet Spark !
Features of Spark
Spark Architecture
Overview of Spark Components
Spark Glossary
TOP                    ISIT312 Big Data Management,  SIM S4 2025 5/25
================================================================================
PAGE 6 of 25
================================================================================

Meet Spark !
Apache Spark, as a cluster computing platform, is designed to be fast
and general-purpose
On the speed, Spark  extends the MapReduce  model to e !cient support
more types of computations, for example, interactive and iterative
computations
On the generality, Spark  covers a wide ranges of workloads, including
batch applications, iterative queries, streaming and advanced analyticsThis is mainly due to Spark 's in-memory computing, although Spark  is also
more e!cient than MapReduce  for persistent storage complex applications-
It provides a uniﬁed environment  makes the combination of those di"erent
engines easy and inexpensive
It provides multiple language APIs , including Scala, Python, Java, SQL, and R, as
well as a very rich (yet fast-growing) built-in libraries-
-
TOP                    ISIT312 Big Data Management,  SIM S4 2025 6/25
================================================================================
PAGE 7 of 25
================================================================================

Meet Spark !
Data Scientists use Spark to:
Data Engineers use Spark to:Analyse and model data
Make prediction based on data
Build data pipelines to fulﬁl certain tasks-
-
-
Develop data processing applications
Deploy the output of data scientists in production-
-
TOP                    ISIT312 Big Data Management,  SIM S4 2025 7/25
================================================================================
PAGE 8 of 25
================================================================================

Introduction to Spark
Outline
MapReduce Challenges
Meet Spark !
Features of Spark
Spark Architecture
Overview of Spark Components
Spark Glossary
TOP                    ISIT312 Big Data Management,  SIM S4 2025 8/25
================================================================================
PAGE 9 of 25
================================================================================

Features of Spark
As mentioned, Spark  supports not only batch computing (as MapReduce
does) but also interactive, iterative, and real-time  computing
The main feature of Spark  is its in-memory cluster computing that
increases the processing speed of an application
The programming model of Spark  is based on Directed Acyclic Graphs
(DAG) and it is more ﬂexible than the MapReduce  model
Unlike MapReduce , Spark  has built-in high-level APIs to process
structured data, e.g., tables in Hive
Spark  reduces the management burden of maintaining separate tools
TOP                    ISIT312 Big Data Management,  SIM S4 2025 9/25
================================================================================
PAGE 10 of 25
================================================================================

Features of Spark
Speed
Spark  helps to run an application in Hadoop  cluster, up to 100 times
faster in memory, and 10 times faster when running on disk
Support for multiple languages
Spark  provides built-in APIs in Java, Scala, or Python
Advanced Analytics
Spark  not only supports MapReduce  It also supports SQL queries,
Streaming data, Machine Learning (ML), and Graph algorithmsThis is achieved by reducing number of read/write operations to persistent
storage
It stores the intermediate processing data in memory-
-
TOP                    ISIT312 Big Data Management,  SIM S4 2025 10/25
================================================================================
PAGE 11 of 25
================================================================================

Introduction to Spark
Outline
MapReduce Challenges
Meet Spark !
Features of Spark
Spark Architecture
Overview of Spark Components
Spark Glossary
TOP                    ISIT312 Big Data Management,  SIM S4 2025 11/25
================================================================================
PAGE 12 of 25
================================================================================

Spark Architecture
Driver program is the main Spark  application that consists of the data
processing logic
Executor is a JVM process that runs on each worker node and processes
the jobs that the driver program submits
Task is a subcomponent of a data processing job
TOP                    ISIT312 Big Data Management,  SIM S4 2025 12/25
================================================================================
PAGE 13 of 25
================================================================================

Spark Architecture
Two modes of Building Spark on Hadoop
These modes are di "erent from full-distributed mode and pseudo-
distributed mode
TOP                    ISIT312 Big Data Management,  SIM S4 2025 13/25
================================================================================
PAGE 14 of 25
================================================================================

Spark Architecture
Spark on YARN: Client-Mode
TOP                    ISIT312 Big Data Management,  SIM S4 2025 14/25
================================================================================
PAGE 15 of 25
================================================================================

Spark Architecture
Spark on YARN: Cluster-Mode
TOP                    ISIT312 Big Data Management,  SIM S4 2025 15/25
================================================================================
PAGE 16 of 25
================================================================================

Spark Architecture
Writing to and reading from memory in Spark reduces I/O overhead
Data persists in memory and enables fast access
TOP                    ISIT312 Big Data Management,  SIM S4 2025 16/25
================================================================================
PAGE 17 of 25
================================================================================

Spark Architecture
High-level API of Spark  eases the development of a data processing
pipeline
No need to conﬁrm to a speciﬁc pattern, for example MapReduce
-
TOP                    ISIT312 Big Data Management,  SIM S4 2025 17/25
================================================================================
PAGE 18 of 25
================================================================================

Spark Architecture
Components of Spark
TOP                    ISIT312 Big Data Management,  SIM S4 2025 18/25
================================================================================
PAGE 19 of 25
================================================================================

Introduction to Spark
Outline
MapReduce Challenges
Meet Spark !
Features of Spark
Spark Architecture
Overview of Spark Components
Spark Glossary
TOP                    ISIT312 Big Data Management,  SIM S4 2025 19/25
================================================================================
PAGE 20 of 25
================================================================================

Overview of Spark Components
Spark Core
Spark SQL
Spark StreamingSpark Core  is the underlying general execution engine for Spark  platform that
all other functionality is built upon
It provides In-Memory  computing and referencing datasets in external storage
systems-
-
Spark SQL  is a component on top of Spark Core  that introduces a new data
abstraction called SchemaRDD, which provides support for structured and
semi-structured data-
Spark Streaming  leverages Spark Core  fast scheduling capability to perform
streaming analytics-
TOP                    ISIT312 Big Data Management,  SIM S4 2025 20/25
================================================================================
PAGE 21 of 25
================================================================================

Overview of Spark Components
SparkML
GraphX
SparkRSparkML  is a distributed machine learning framework above Spark  because of
the distributed memory-based Spark  architecture-
GraphX  is a distributed graph-processing framework on top of Spark
It provides an API for expressing graph computation and provides an optimized
runtime for this abstraction-
-
SparkR  is an R package that provides a light-weight frontend to use Spark  from
R-
TOP                    ISIT312 Big Data Management,  SIM S4 2025 21/25
================================================================================
PAGE 22 of 25
================================================================================

Introduction to Spark
Outline
MapReduce Challenges
Meet Spark !
Features of Spark
Spark Architecture
Overview of Spark Components
Spark Glossary
TOP                    ISIT312 Big Data Management,  SIM S4 2025 22/25
================================================================================
PAGE 23 of 25
================================================================================

Spark Glossary
TOP                    ISIT312 Big Data Management,  SIM S4 2025 23/25
================================================================================
PAGE 24 of 25
================================================================================

Spark Glossary
TOP                    ISIT312 Big Data Management,  SIM S4 2025 24/25
================================================================================
PAGE 25 of 25
================================================================================

References
A Gentle Introduction to Spark, Databricks, (Available in READINGS
folder)
Spark Overview
Karau H., Fast data processing with Spark Packt Publishing, 2013
(Available from UOW Library)
Srinivasa, K.G., Guide to High Performance Distributed Computing: Case
Studies with Hadoop, Scalding and SparkSpringer, 2015 (Available from
UOW Library)
Chambers B., Zaharia M.,Spark: The Deﬁnitive Guide, O'Reilly 2017
TOP               ISIT312 Big Data Management,  SIM S4 2025 25/25
================================================================================


################################################################################
FILE: 18sparkdatamodel.pdf
################################################################################

================================================================================
PAGE 1 of 20
================================================================================

        ISIT312 Big Data Management
Spark Data Model
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 20
================================================================================

Introduction to Spark
Outline
Resilient Distributed Data Sets (RDDs)
DataFrames
SQL Tables/View
Datasets
TOP                    ISIT312 Big Data Management,  SIM S4 2025 2/20
================================================================================
PAGE 3 of 20
================================================================================

Resilient Distributed Data Sets (RDDs)
Resilient Distributed Datasets  (RDDs) is the lowest level (and the oldest)
data abstraction available to the users
An RDD represents an immutable, partitioned collection of elements
that can be operated on in parallel
Every row in an RDD is a Java object
RDD does not need to have any schema deﬁned in advance
It makes RDD very ﬂexible for various applications but in the same
moment makes the manipulations on data more complicated
Users must implement their own functions to perform simple tasks like
for example aggregation functions: average, count, maximum, etc
RDDs provide more control on how data is distributed over a cluster and
how it is operated
TOP                    ISIT312 Big Data Management,  SIM S4 2025 3/20
================================================================================
PAGE 4 of 20
================================================================================

Resilient Distributed Data Sets (RDDs)
RDD is characterized by the following properties
Speciﬁcation of custom partitions may provide signiﬁcant performance
improvements when using key-value RDDs
The properties of RDDs determine how Spark schedules and processes
the applications
Di!erent RDDs may use di!erent properties depending on the required
outcomesA list of partitions
A function for computing each split
A list of dependencies on other RDDs
Optionally, a Partitioner for key-value  RDDs
Optionally, a list of preferred locations to compute each split-
-
-
-
-
TOP                    ISIT312 Big Data Management,  SIM S4 2025 4/20
================================================================================
PAGE 5 of 20
================================================================================

Resilient Distributed Data Sets (RDDs)
RDD can be created from a list
Listing RDD
   
val strings = "hello hello !". split(" ") 
strings: Array[String] = Array(hello, hello, !)
Creating a list of strings
  
val rdset =  spark.sparkContext. parallelize( strings);
rdset: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[ 1] at parallelize
                                                                 at :26
Creating RDD
  
rdset.collect()
res1: Array[String] = Array(hello, hello, !)
Listing RDD
TOP                    ISIT312 Big Data Management,  SIM S4 2025 5/20
================================================================================
PAGE 6 of 20
================================================================================

Resilient Distributed Data Sets (RDDs)
Creating RDD from a ﬁle by reading line-by-line
   
val lines =  spark.sparkContext.textFile("sales.txt")
lines: org.apache.spark.rdd.RDD[String] = sales.txt MapPartitionsRDD[1] at textFile at :24
Creating RDD
   
lines.collect()
res0: Array[String] = Array(bolt 45, bolt 5, drill 1, drill 1, screw 1, screw 2, screw 3)
Listing RDD
   
val pairs = lines.map(s => (s, 1))
pairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[2] at map at :26)
Creating key-value pairs
   
val counts = pairs.reduceByKey((a, b) => a + b)
counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[3] at reduceByKey at :28)
Counting
   
counts.collect()
res1: Array[(String, Int)] = Array((bolt 5,1), (drill 1,2), (bolt 45,1), (screw 2,1), (screw 3,1),
(screw 1,1))
Listing RDD
TOP                    ISIT312 Big Data Management,  SIM S4 2025 6/20
================================================================================
PAGE 7 of 20
================================================================================

Introduction to Spark
Outline
Resilient Distributed Data Sets (RDDs)
DataFrames
SQL Tables/Views
Datasets
TOP                    ISIT312 Big Data Management,  SIM S4 2025 7/20
================================================================================
PAGE 8 of 20
================================================================================

DataFrames
A DataFrame  is a table of data with rows and columns
A list of columns in DataFrame  together with the types of columns is
called as a schema
A DataFrame  can span over many systems in a cluster
The concepts similar to DataFrame  has been used in Python and R
A DataFrame  is the simplest data model available in Spark
A DataFrame  consist of zero or more partitions
To parallelize data processing all data are divided into chunks called as
partitions
A partition  is a collection of rows located on a single system in a cluster
When operating DataFrame  Spark simultaneously processes all relevant
partitions
Shu" e operation  is performed to share data among the systems in a cluster-
-
-
-
TOP                    ISIT312 Big Data Management,  SIM S4 2025 8/20
================================================================================
PAGE 9 of 20
================================================================================

DataFrames
A DataFrame  can be created in the following way
The contents of a DataFrame can be listed in the following way
val dataFrame =  spark.read.json("/bigdata/people.json")
Creating a DataFrame
dataFrame. show()
      
+----+-------+
| age| name  |
+----+-------+
|null|Michael|
| 30 | Andy  |
| 19 | Justin|
+----+-------+
Listing results
TOP                    ISIT312 Big Data Management,  SIM S4 2025 9/20
================================================================================
PAGE 10 of 20
================================================================================

DataFrames
A schema of a DataFrame  can be listed in the following way
dataFrame. printSchema()
    
root
|-- age: long (nullable = true)
|-- name:  string (nullable = true)
Listing a schema
TOP                    ISIT312 Big Data Management,  SIM S4 2025 10/20
================================================================================
PAGE 11 of 20
================================================================================

Introduction to Spark
Outline
Resilient Distributed Data Sets (RDDs)
DataFrames
SQL Tables/Views
Datasets
TOP                    ISIT312 Big Data Management,  SIM S4 2025 11/20
================================================================================
PAGE 12 of 20
================================================================================

SQL Tables/Views
SQL can be used to operate on DataFrames
It is possible to register any DataFrame  as a table or view (a temporary
table) and apply SQL to it
There is no performance overhead when registering a DataFrame  as SQL
Table  and when processing it
A DataFrame  dataFrame can be registered as SQL temporary view
people in the following way
dataFrame. createOrReplaceTempView( "people")
val sqlDF =  spark.sql("SELECT * FROM people")
sqlDF.show()
DataFrame as SQL Table
+----+-------+
| age| name  |
+----+-------+
|null|Michael|
|30  |Andy   |
|19  |Justin |
+----+-------+
Results
TOP                    ISIT312 Big Data Management,  SIM S4 2025 12/20
================================================================================
PAGE 13 of 20
================================================================================

SQL Tables/Views
SQL Tables are logically equivalent to DataFrames
A di!erence between SQL Tables and DataFrames  is such that
DataFrames  are deﬁned within the scope of a programming language
while SQL Tables are deﬁned within a database
When created, SQL table belongs to a default database
SQL table can be created in the following way
SQL view can be created in the following way
CREATE TABLE flights(
 DEST_COUNTRY_NAME STRING,
 ORIGIN_COUNTRY_NAME STRING,  count LONG)
   USING JSON OPTIONS (  path '/mnt/defg/chapter-1-data/json/2015-summary.json')
CREATE TABLE
CREATE VIEW just_usa_view AS
 SELECT *
 FROM flights
 WHERE dest_country_name =  'United States'
CREATE VIEW
TOP                    ISIT312 Big Data Management,  SIM S4 2025 13/20
================================================================================
PAGE 14 of 20
================================================================================

SQL Tables/Views
Global SQL view can be created in the following way
Temporary SQL view can be created in the following way
Global and Temporary SQL view can be created in the following way
      
CREATE GLOBAL VIEW just_usa_global AS
 SELECT *
 FROM flights
 WHERE dest_country_name =  'United States'
CREATE VIEW
      
CREATE TEMP VIEW just_usa_global AS
 SELECT *       
 FROM flights
 WHERE dest_country_name =  'United States'
CREATE VIEW
      
CREATE GLOBAL TEMP VIEW just_usa_global AS
 SELECT *
 FROM flights
 WHERE dest_country_name =  'United States'
CREATE VIEW
TOP                    ISIT312 Big Data Management,  SIM S4 2025 14/20
================================================================================
PAGE 15 of 20
================================================================================

SQL Tables/Views
Spark SQL view includes three core complex types: sets , lists, and structs
Structs allow to create nested data
The functions collect_set and collect_list functions create
sets and lists
      
CREATE VIEW nested_data AS
 SELECT (DEST_COUNTRY_NAME,  ORIGIN_COUNTRY_NAME)  as country,  count
 FROM flights
CREATE VIEW
      
SELECT DEST_COUNTRY_NAME as new_name,
       collect_list( count) as flight_counts,
       collect_set( ORIGIN_COUNTRY_NAME)  as origin_set
FROM flights
GROUP BY DEST_COUNTRY_NAME
CREATE VIEW
TOP                    ISIT312 Big Data Management,  SIM S4 2025 15/20
================================================================================
PAGE 16 of 20
================================================================================

SQL Tables/Views
Managed versus unmanaged tables
Creating unmanaged (external) table in SparkManaged table  is a table that stored data and metadata, it is equivalent to an
internal  table in Hive
Unmanaged table  is a table that stores only data, it is equivalent to an external
table in Hive-
-
CREATE EXTERNAL TABLE hive_flights(
 DEST_COUNTRY_NAME STRING,
 ORIGIN_COUNTRY_NAME STRING,
 count LONG)
  ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
  LOCATION '/mnt/defg/flight-data-hive/'
CREATE EXTERNAL TABLE
TOP                    ISIT312 Big Data Management,  SIM S4 2025 16/20
================================================================================
PAGE 17 of 20
================================================================================

Introduction to Spark
Outline
Resilient Distributed Data Sets (RDDs)
DataFrames
SQL Tables/Views
Datasets
TOP                    ISIT312 Big Data Management,  SIM S4 2025 17/20
================================================================================
PAGE 18 of 20
================================================================================

Datasets
A Dataset is a distributed collection of data
A DataFrame , is a Dataset of type Row
Datasets  consists of the objects included in the rows; one object per row
Datasets  are a strictly JVM language feature that only work with Scala
and Java
A Dataset can be constructed from JVM and then manipulated using
functional transformations
In Scala a case class object deﬁnes a schema of an object
Datasets  use a specialized Encoder to serialize the objects for processing
or transmitting over the network
Dataset supports all operations of DataFrame
TOP                    ISIT312 Big Data Management,  SIM S4 2025 18/20
================================================================================
PAGE 19 of 20
================================================================================

Datasets
A Dataset can be deﬁned, created, and used in the following way
      
case class Person(name: String, age: Long)
Creating Dataset Schema
      
val caseClassDS = Seq(Person("Andy", 32)).toDS()
Creating Dataset
      
caseClassDS.show()
Listing Dataset
      
+----+---+
|name|age|
+----+---+
|Andy| 32|
+----+---+
Results
caseClassDS.select($"name").show()
Using a Dataset
      
+-----+
| name|
+-----+
|James|
Results
TOP                    ISIT312 Big Data Management,  SIM S4 2025 19/20
================================================================================
PAGE 20 of 20
================================================================================

References
A Gentle Introduction to Spark, Databricks, (Available in READINGS
folder)
RDD Programming Guide
Spark SQL, DataFrames and Datasets Guide
Karau H., Fast data processing with Spark Packt Publishing, 2013
(Available from UOW Library)
Srinivasa, K.G., Guide to High Performance Distributed Computing: Case
Studies with Hadoop, Scalding and Spark Springer, 2015 (Available from
UOW Library)
Chambers B., Zaharia M.,Spark: The Deﬁnitive Guide, O'Reilly 2017
TOP               ISIT312 Big Data Management,  SIM S4 2025 20/20
================================================================================


################################################################################
FILE: 19sparkoperations.pdf
################################################################################

================================================================================
PAGE 1 of 41
================================================================================

        ISIT312 Big Data Management
Spark Operations
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 41
================================================================================

Spark Operations
Outline
The Programming Language Scala
Quick Start
Self Contained Application
Web User Interface
Operations on Resilient Distributed Datasets (RDDs)
Operations on Datasets
Operations on DataFrames
SQL Module
TOP                    ISIT312 Big Data Management,  SIM S4 2025 2/41
================================================================================
PAGE 3 of 41
================================================================================

The Programming Language Scala
Spark  has built-in APIs for Java, Scala, and Python, and is also integrated
with R
Among all languages, Scala is the most supported language
Also, Spark  project is implemented using Scala
Therefore, we choose Scala as our working language in Spark
Scala is a Java-like programming language which uniﬁes object-oriented
and functional programming
Scala is a pure object-oriented language in the sense that every value is
an object
Types and behaviour of objects are described by classes
Scala is a functional programming language in the sense that every
function is a value
Nesting of function deﬁnitions and higher-order functions are naturally
supportedTOP                    ISIT312 Big Data Management,  SIM S4 2025 3/41
================================================================================
PAGE 4 of 41
================================================================================

The Programming Language Scala
Hello World ! in Scala
Instead of including main method, it can be extended with App trait
Using command line arguments
      
object Hello {
    def main(args: Array[String]) = {
        println("Hello, world")
    }
}
Hello World ! in Scala
object Hello2 extends App {
println("Hello, world")
}
Extending App trait
object HelloYou extends App {
    if (args.size == 0)
        println("Hello, you")
    else
        println("Hello, " + args(0))
}
Command line arguments
TOP                    ISIT312 Big Data Management,  SIM S4 2025 4/41
================================================================================
PAGE 5 of 41
================================================================================

The Programming Language Scala
Di!erence between var, val, and def
When lazy keyword is used then a value is only computed when it is
needed
      
var x = 7
x = x * 2
Variable
val x = 7
x = x * 2
'error: reassignment to val'
Value
def hello(name: String) = "Hello : " + name
hello("James") // "Hello : James"
hello("")      // "Hello : "
Function declaration
lazy val x = {
  println("calculating value of x")
  13 }
val y = {
  println("calculating value of y")
  20 }
Lazy evaluation
TOP                    ISIT312 Big Data Management,  SIM S4 2025 5/41
================================================================================
PAGE 6 of 41
================================================================================

The Programming Language Scala
Deﬁning a class
      
class Point(var x: Int, var y: Int) {
Class Point
  def move(dx: Int, dy: Int): Unit = {
    x = x + dx
    y = y + dy
  }
Method move
  override def toString: String =
    s"($x, $y)"
}
Method toString
val point1 = new Point(2, 3)
println(point1.x)            // 2
println(point1)              // prints (2, 3)
Applications
TOP                    ISIT312 Big Data Management,  SIM S4 2025 6/41
================================================================================
PAGE 7 of 41
================================================================================

Spark Operations
Outline
The Programming Language Scala
Quick Start
Self Contained Application
Web User Interface
Operations on Resilient Distributed Datasets (RDDs)
Operations on Datasets
Operations on DataFrames
SQL Module
TOP                    ISIT312 Big Data Management,  SIM S4 2025 7/41
================================================================================
PAGE 8 of 41
================================================================================

Quick Start
To open Scala version of Spark shell  in standalone mode process the
following command
To open Spark shell  shell with YARN, process the following command
    
bin/spark-shell --master local[*]
Starting Spark shell in standalone mode
      
bin/spark-shell --master yarn
Starting Spark shell with Yarn
TOP                    ISIT312 Big Data Management,  SIM S4 2025 8/41
================================================================================
PAGE 9 of 41
================================================================================

Quick Start
A SparkSession  instance is an entry to a Spark  application
You can use SparkSession  instance spark to interact with Spark  and
to develop your data processing pipeline
For example,If you type spark  in the spark-shell interface then you get the following
messages-
   
res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@...
Message
  
val myRange = spark.range(1000).toDF("number")
myRange: org.apache.spark.sql.DataFrame = [number: bigint]
Creating Data Frame
  
myRange.show(2)
+------+
|number|
+------+
|     0|
|     1|
+------+
Listing Data Frame
TOP                    ISIT312 Big Data Management,  SIM S4 2025 9/41
================================================================================
PAGE 10 of 41
================================================================================

Quick Start
Sample processing of a ﬁle README.md
       
val YOUR_SPARK_HOME = "path-to-your-Spark-home"
Setting Spark Home folder
   
val textFile =  spark.read.textFile( "$YOUR_SPARK_HOME/README.md")
textFile:  org.apache.spark.sql.Dataset[String] = [value: string]
Reading a text file
   
textFile. count()
res0: Long = 104
Counting rows
   
textFile. first()
res1: String = # Apache Spark
Reading the first row
textFile. filter(line =>line.contains( "Spark")).count()
res2: Long = 20
Filtering and counting rows
TOP                    ISIT312 Big Data Management,  SIM S4 2025 10/41
================================================================================
PAGE 11 of 41
================================================================================

Quick Start
More operations on a ﬁle
       
textFile.map(line => line.split(" ").size).reduce((a, b) => if (a > b) a else b)
res3: Int = 22
Counting number of words in the longest line
   
val wordCounts = textFile.flatMap(line => line.split("")).groupByKey(identity).count()
wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]
Filtering and counting rows
   
wordCounts.show(2)
+----------+--------+
|     value|count(1)|
+----------+--------+
|    online|       1|
|    graphs|       1|
+----------+--------+
only showing top 2 rows
Listing results
   
wordCounts.collect()
res7: Array[(String, Long)] = Array((online,1), (graphs,1), (["Parallel,1), (["Building,1), (thread,1),
(documentation,3), (command,,2), (abbreviated,1), (overview,1), (rich,1), (set,2), ...
Listing results
TOP                    ISIT312 Big Data Management,  SIM S4 2025 11/41
================================================================================
PAGE 12 of 41
================================================================================

Spark Operations
Outline
The Programming Language Scala
Quick Start
Self Contained Application
Web User Interface
Operations on Resilient Distributed Datasets (RDDs)
Operations on Datasets
Operations on DataFrames
Spark SQL Module
TOP                    ISIT312 Big Data Management,  SIM S4 2025 12/41
================================================================================
PAGE 13 of 41
================================================================================

Self-Contained Application
A sample self-contained application
    
import org. apache.spark.sql.SparkSession
object SimpleApp {
  def main( args: Array[String]) {
    val logFile =  "YOUR_SPARK_HOME/README.md"
   // Should be some file on your system
   val spark =  SparkSession. builder
    .appName("Simple Application")
    .config("spark.master",  "local[*]")
    .getOrCreate()
  val logData =  spark.read.textFile( logFile).cache()
  val numAs =  logData. filter(line => line.contains( "a")).count()
  val numBs =  logData. filter(line => line.contains( "b")).count()
  println( s"Lines with a: $numAs, Lines with b: $numBs")
  spark.stop()
  }
}
SimpleApp.scala
TOP                    ISIT312 Big Data Management,  SIM S4 2025 13/41
================================================================================
PAGE 14 of 41
================================================================================

Self-Contained Application
Compiling Scala source code using scalac
Creating a jar ﬁle in the following way
Process it with Spark-shell in the following way
       
scalac -classpath "$SPARK_HOME/jars/*" SimpleApp. scala
Compiling Scala source code
       
jar cvf app. jar SimpleApp*.class
Creating jar
$SPARK_HOME/ bin/spark-submit --master local[*] --class SimpleApp app. jar
Processing
TOP                    ISIT312 Big Data Management,  SIM S4 2025 14/41
================================================================================
PAGE 15 of 41
================================================================================

Spark Operations
Outline
The Programming Language Scala
Quick Start
Self Contained Application
Web User Interface
Operations on Resilient Distributed Datasets (RDDs)
Operations on Datasets
Operations on DataFrames
Spark SQL Module
TOP                    ISIT312 Big Data Management,  SIM S4 2025 15/41
================================================================================
PAGE 16 of 41
================================================================================

Web UI
Each driver program has a Web UI, typically on port 4040
Spark Web UI displays information about running tasks, executors, and
storage usage.
TOP                    ISIT312 Big Data Management,  SIM S4 2025 16/41
================================================================================
PAGE 17 of 41
================================================================================

Spark Operations
Outline
The Programming Language Scala
Quick Start
Self Contained Application
Web User Interface
Operations on Resilient Distributed Datasets (RDDs)
Operations on Datasets
Operations on DataFrames
Spark SQL Module
TOP                    ISIT312 Big Data Management,  SIM S4 2025 17/41
================================================================================
PAGE 18 of 41
================================================================================

Operations on Resilient Distributed Datasets
(RDDs)
Operations on RDDs are performed on raw Java or Scala objects
Creating a simple RDD with words and distributing over 2 partitions
Eliminating duplicates and counting words
Filtering
       
val myCollection = "Spark The Definitive Guide : Big Data Processing Made Simple".split(" ")
val words = spark.sparkContext.parallelize(myCollection, 2)
Creating RDD
words.distinct().count()
Distinct and counting
      
def startsWithS(individual:String) = { individual.startsWith("S") }
Filtering function
val onlyS = words.filter(word => startsWithS(word))
Filtering
onlyS.collect()
Results of filtering
TOP                    ISIT312 Big Data Management,  SIM S4 2025 18/41
================================================================================
PAGE 19 of 41
================================================================================

Operations on Resilient Distributed Datasets
(RDDs)
Sorting of RDD uses sortBy method and a function that extracts a value
from the objects
Random split into Array
Reduce RDD to one value
       
words.sortBy(word => word.length() * -1).take(2))
Sorting
    
val fiftyFiftySplit = words.randomSplit(Array[Double](0.5, 0.5))
Split into Array
def wordLengthReducer(leftWord:String, rightWord:String): String = {
  if (leftWord.length >= rightWord.length)
    return leftWord
  else
   return rightWord }
Reducing
words.reduce(wordLengthReducer)
Reducing
TOP                    ISIT312 Big Data Management,  SIM S4 2025 19/41
================================================================================
PAGE 20 of 41
================================================================================

Operations on Resilient Distributed Datasets
(RDDs)
Some operations on RDDs are available on key-value pairs
The most common ones are distributed "shu"e" operations, such as
grouping or aggregating the elements by a key
For example, reduceByKey operation on key-value pairs can be used to
count how many times each line of text occurs in a ﬁle
Some of the transformations of RDDs
val lines = sc.textFile("data.txt")
val pairs = lines.map(s => (s, 1))
val counts = pairs.reduceByKey((a, b) => a + b)
Sorting
map(func):                      passes each element of RDD through a function
filter(func):                   selects all element for which a function returms true
sample(withReplacement, fraction, seed): extracts  sample from RDD
union(otherDataset):            unions two RRDs
intersection(otherDataset):     finds intersection of two RDDs
distinct([numPartitions])):     eliminates duplicates
groupByKey([numPartitions]):    when called on RDD with (K, V) pairs, returns RDD with (K, Iterable) pairs
sortByKey([ascending], [numPartitions]: when called on (K, V) pairs where K implements Ordered,
Transformations
TOP                    ISIT312 Big Data Management,  SIM S4 2025 20/41
================================================================================
PAGE 21 of 41
================================================================================

Spark Operations
Outline
The Programming Language Scala
Quick Start
Self Contained Application
Web User Interface
Operations on Resilient Distributed Datasets (RDDs)
Operations on Datasets
Operations on DataFrames
Spark SQL Module
TOP                    ISIT312 Big Data Management,  SIM S4 2025 21/41
================================================================================
PAGE 22 of 41
================================================================================

Operations on Datasets
Operations on a Dataset start from creation of case class
Dataset supports all operations of DataFrame
  
case class Person(name: String, age: Long)
defined class Person
Creating case class
   
val caseClassDS = Seq(Person("Andy", 32)).toDS()
caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]
Creating Dataset
   
caseClassDS.show()
+----+---+
|name|age|
+----+---+
|Andy| 32|
+----+---+
Listing Dataset
caseClassDS.select($"name").show()
+----+
|name|
+----+
|Andy|
Using a Dataset
TOP                    ISIT312 Big Data Management,  SIM S4 2025 22/41
================================================================================
PAGE 23 of 41
================================================================================

Operations on Datasets
Operations on Datasets  start from creation of case class
Next we create a DataFrame
Finally, DataFrame  is casted to Dataset
Filtering a Dataset
Mapping a Dataset
      
case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)
Creating case class
val flightsDF = spark.read.parquet("/mnt/defg/chapter-1-data/parquet/2010-summary.parquet/")
Creating DataFrame
val flights = flightsDF.as[Flight]
Creating Dataset
def originIsDestination(flight_row: Flight): Boolean = {
return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME}
Defining a function
flights.filter(flight_row => originIsDestination(flight_row)).first()
 Filtering
val destinations = flights.map(f => f.DEST_COUNTRY_NAME)
 MappingTOP                    ISIT312 Big Data Management,  SIM S4 2025 23/41
================================================================================
PAGE 24 of 41
================================================================================

Spark Operations
Outline
The Programming Language Scala
Quick Start
Self Contained Application
Web User Interface
Operations on Resilient Distributed Datasets (RDDs)
Operations on Datasets
Operations on DataFrames
Spark SQL Module
TOP                    ISIT312 Big Data Management,  SIM S4 2025 24/41
================================================================================
PAGE 25 of 41
================================================================================

Operations on DataFrames
Dataset and DataFrame  are the data abstractions for Spark SQL
Dataset is a distributed collection of data
DataFrame  is a Dataset organized into named columns.It supports the use of self-deﬁned functions to process data
For example, map  and reduce  functions in the previous slides
Dataset is typed; typing is checked at compiling time-
-
-
It is conceptually equivalent to a table in a relational database or a data frame
in R/Python
To use self-deﬁned functions, you need to register them with Spark
DataFrame  is untyped, i.e., typing is checked at runtime
DataFrame  is more performance-optimal than Dataset-
-
-
-
TOP                    ISIT312 Big Data Management,  SIM S4 2025 25/41
================================================================================
PAGE 26 of 41
================================================================================

Operations on DataFrames
DataFrame  can be created in the following way
   
val df = spark.read.json("people.json")
df.show()
Creating a DataFrame
    
+----+-------+
| age| name  |
+----+-------+
|null|Michael|
| 30 | Andy  |
| 19 | Justin|
+----+-------+
Results
 
df.printSchema()
Results
    
root
|-- age: long (nullable = true)
|-- name:  string (nullable = true)
Results
TOP                    ISIT312 Big Data Management,  SIM S4 2025 26/41
================================================================================
PAGE 27 of 41
================================================================================

Operations on DataFrames
Select on a DataFrame
      
df.select($"name", $"age" + 1).show()
Selecting from a DataFrame
            
+-------+---------+
| name  |(age + 1)|
+-------+---------+
|Michael|     null|
|   Andy|       31|
| Justin|       20|
+-------+---------+
Results
df.filter($"age" > 21).show()
Filtering a DataFrame
            
+---+----+
|age|name|
+---+----+
| 30|Andy|
+---+----+
Results
TOP                    ISIT312 Big Data Management,  SIM S4 2025 27/41
================================================================================
PAGE 28 of 41
================================================================================

Operations on DataFrames
Count people by age
df.groupBy("age".count().show()
Counting in a DataFrame
            
+----+-----+
| age|count|
+----+-----+
|  19|    1|
|null|    1|
|  30|    1|
+----+-----+
Results
TOP                    ISIT312 Big Data Management,  SIM S4 2025 28/41
================================================================================
PAGE 29 of 41
================================================================================

Operations on DataFrames
Register a DataFrame  as SQL temporary view
df.createOrReplaceTempView( "people")
val sqlDF =  spark.sql("SELECT * FROM people")
sqlDF.show()
Registering and selecting from  DataFrame
                
+----+-------+
| age| name  |
+----+-------+
|null|Michael|
|30  | Andy  |
|19  | Justin|
+----+-------+
Results
TOP                    ISIT312 Big Data Management,  SIM S4 2025 29/41
================================================================================
PAGE 30 of 41
================================================================================

Operations on DataFrames
When to use DataFrames  ?
Except for the following few cases, you can use them interchangeable (if
performance is not a concern). You also can convert one to the other
easily.
In the Bigdata pipeline, you read an unstructured data source, for example, a
text ﬁle as a Dataset and continue processing the data
You can directly read an structured source like Hive table, JSON document as a
DataFrame
If you expect to use self-deﬁned function easily, especially in the data cleaning
or preprocessing stage of the pipeline, you should use a Dataset-
-
-
TOP                    ISIT312 Big Data Management,  SIM S4 2025 30/41
================================================================================
PAGE 31 of 41
================================================================================

Operations on DataFrames
Create a Dataset of Person  objects from a text ﬁle and convert it to a
DataFrame
val peopleDF =  spark.sparkContext
.textFile( "examples/src/main/resources/people.txt")
.map(_.split(","))
.map(attributes => Person(attributes( 0), attributes( 1).trim.toInt))
.toDF()
Converting a Dataset to DataFrame
      
peopleDF:  org.apache.spark.sql.DataFrame = [name: string, age: bigint]
Results
TOP                    ISIT312 Big Data Management,  SIM S4 2025 31/41
================================================================================
PAGE 32 of 41
================================================================================

Operations on DataFrames
Convert DataFrame  to Dataset
      
case class  Employee( name: String, salary: Long)
val ds =
     spark. read.json(".../examples/src/main/resources/employees.json").as[ Employee]
Converting a Dataset to DataFrame
      
ds: org.apache.spark.sql.Dataset[Employee]  = [name: string, salary: bigint]
Results
TOP                    ISIT312 Big Data Management,  SIM S4 2025 32/41
================================================================================
PAGE 33 of 41
================================================================================

Operations on DataFrames
Spark  DataFrame /Dataset support two types of operations:
transformations  and actions
Transformations are operations on DataFrames /Datasets  that return a
new DataFrame /Dataset
Actions are operations that return a result to the driver program or write
it to storage, and kick o! a computation
Return type di!erence: transformations  return DataFrames /Datasets ,
whereas actions return some other data type
Spark  treats the two operations very di!erentlyFor example select() , groupBy() , map() , and filter() -
For example show() , count() , and first() -
TOP                    ISIT312 Big Data Management,  SIM S4 2025 33/41
================================================================================
PAGE 34 of 41
================================================================================

Operations on DataFrames
Transformations  are lazily evaluated, meaning that Spark  will not begin
to execute until it sees an action
Instead, Spark  internally records metadata to indicate that some
transformation operation has been requested
For example transformation  creates another DataFrame
Action triggers the computation
val sqlDF =  spark.sql("SELECT * FROM people")
Creating a DataFrame
  
sqlDF.show()
Action on a DataFrame
TOP                    ISIT312 Big Data Management,  SIM S4 2025 34/41
================================================================================
PAGE 35 of 41
================================================================================

Operations on DataFrames
The lazy evaluation to reduce the number of passes it has to take over
the dataset
In Hadoop MapReduce , developers often have to consider how to group
together operations to minimize the number of MapReduce passes
In Spark, there is no substantial beneﬁt to writing a single complex map
instead of chaining together many simple operations
Thus, users are free to organize their program into smaller, more
manageable operations
TOP                    ISIT312 Big Data Management,  SIM S4 2025 35/41
================================================================================
PAGE 36 of 41
================================================================================

Spark Operations
Outline
The Programming Language Scala
Quick Start
Self Contained Application
Web User Interface
Operations on Resilient Distributed Datasets (RDDs)
Operations on Datasets
Operations on DataFrames
Spark SQL
TOP                    ISIT312 Big Data Management,  SIM S4 2025 36/41
================================================================================
PAGE 37 of 41
================================================================================

Spark SQL
Spark SQL  is a Spark  module for general data processing and analytics
It can be used for all sorts of data, from unstructured log ﬁles to semi-
structured CSV  ﬁles and highly structured Parquet ﬁles
To interact with Spark SQL , you can either use SQL or Spark Structured
API, or both
The same execution engine is used, independent of which API/language
you use to express the computation
The APIs of Spark SQL  provide a rich set of pre-built, high-level
operations for accomplishing sophisticate data processing and ETL jobs,
and mechanism to implement your own operations, for example self-
deﬁned functions and aggregations
TOP                    ISIT312 Big Data Management,  SIM S4 2025 37/41
================================================================================
PAGE 38 of 41
================================================================================

Spark SQL
Spark SQL  has two data abstractions
Both are distributed table-like collections with well-deﬁned rows and
columns.DataFrame
Dataset (available in Scala/Java APIs, but not Python/R APIs)
DataFrame  can be represented as SQL tables and views-
-
-
DataFrame  vs. spreadsheet
 -
TOP                    ISIT312 Big Data Management,  SIM S4 2025 38/41
================================================================================
PAGE 39 of 41
================================================================================

Spark SQL
Spark SQL  allows to code SQL statements in Scala, Java and Python
language APIs.
To use SQL to manipulate a DataFrame , we ﬁrst need to create a
temporal view for it
All standard SQL statements + functions are applicable in Spark SQL
Spark implements a subset of ANSI SQL:2003
df.createOrReplaceTempView( "dfTable")
Creating a temporal view
TOP               ISIT312 Big Data Management,  Session 2, 2022 39/41
================================================================================
PAGE 40 of 41
================================================================================

Spark SQL
Using SQL
spark.sql(
"SELECT DEST_COUNTRY_NAME, sum(count) 
 FROM dfTable
 GROUP BY DEST_COUNTRY_NAME"
)
.where("DEST_COUNTRY_NAME like 'S%'")
.where("'sum(count)' > 10")
.show(2)
Applying sql method
      
+-----------------+----------+
|DEST_COUNTRY_NAME| sum(count)|
+-----------------+----------+
| Senegal         |  40       |
| Sweden          |  118      |
+-----------------+----------+
Results
TOP               ISIT312 Big Data Management,  Session 2, 2022 40/41
================================================================================
PAGE 41 of 41
================================================================================

References
The Scala Programming Language
A Gentle Introduction to Spark, Databricks, (Available in READINGS
folder)
RDD Programming Guide
Spark SQL, DataFrames and Datasets Guide
Karau H., Fast data processing with Spark Packt Publishing, 2013
(Available from UOW Library)
Srinivasa, K.G., Guide to High Performance Distributed Computing: Case
Studies with Hadoop, Scalding and SparkSpringer, 2015 (Available from
UOW Library)
Chambers B., Zaharia M.,Spark: The Deﬁnitive Guide, O'Reilly 2017
Perrin J-G., Spark in Action, 2nd ed., Manning Publications Co. 2020
TOP               ISIT312 Big Data Management,  SIM S4 2025 41/41
================================================================================


################################################################################
FILE: 20sparkstream.pdf
################################################################################

================================================================================
PAGE 1 of 17
================================================================================

        ISIT312 Big Data Management
Spark Stream Processing
Dr Fenghui Ren
School of Computing and Information Technology -
University of Wollongong
================================================================================
PAGE 2 of 17
================================================================================

Spark Structured Data and Stream Processing
Outline
Spark Stream Processing Modules
Stream Processing
Structured Stream Processing: Quick Start
Programming Model
TOP              ISIT312 Big Data Management, SIM S4 2025 2/17
================================================================================
PAGE 3 of 17
================================================================================

Spark Stream Processing Modules
Stream processing is a key requirement in many big data applications
As soon as an application computes something of value, for example, a
report or a machine learning model, an organization may want to
compute this result continuously in a production environment
This capability is lacked in Hadoop MapReduce  framework due to
slowness of hard-disk IO
In-memory computation implemented in Spark  make stream processing
possible
Spark Streaming  based on its low-level API Resilient Distributed Dataset
is available since Spark 1.2
Spark Structured Streaming based on the Spark SQL  engine is available
since Spark 2.1
Luckily, our VM has installation of Spark 2.1.1
TOP              ISIT312 Big Data Management, SIM S4 2025 3/17
================================================================================
PAGE 4 of 17
================================================================================

Spark Structured Data and Stream Processing
Outline
Spark Stream Processing Modules
Stream Processing
Structured Stream Processing: Quick Start
Programming Model
TOP              ISIT312 Big Data Management, SIM S4 2025 4/17
================================================================================
PAGE 5 of 17
================================================================================

Stream Processing
Stream processing is the act of continuously incorporating the new data
in the stream to compute a result
Sample sources of streams
Stream processing vs. batch processingBank transactions
Clicks on a website
Sensor readings from IoT devices
Scientiﬁc observations and experiments
Manufacturing processes, and the others-
-
-
-
-
Batch processing runs to a ﬁxed set of data, but stream processing handles an
unbounded set of data
Batch processing has low timeliness requirement, but stream processing
requires to work at near realtime-
-
TOP              ISIT312 Big Data Management, SIM S4 2025 5/17
================================================================================
PAGE 6 of 17
================================================================================

Stream Processing
Use cases of stream processing
Notiﬁcations and alerting
Real-time reporting
Incremental ETL
Update data to serve in real time
Real-time decision making
Online machine learning-
-
-
-
-
-
TOP              ISIT312 Big Data Management, SIM S4 2025 6/17
================================================================================
PAGE 7 of 17
================================================================================

Stream Processing
To see the challenges of stream processing, we consider the following
example
Suppose we received the following data from a sensor
What actions should be performed when receiving single values, say, 5 ?
How to react to a pattern, say, 2 -> 10 -> 5
What if data arrives out-of-order, for example, 10 before 5
Other issues: What if a machine in the system fails, losing some state?
What if the load is imbalanced? How can an application signal
downstream consumers when analysis for some event is done, and so
on
{value: 1,  time: "2017-04-07T00:00:00"}
{value: 2,  time: "2017-04-07T01:00:00"}
{value: 5,  time: "2017-04-07T02:00:00"}
{value: 10, time: "2017-04-07T01:30:00"}
{value: 7,  time: "2017-04-07T03:00:00"}
Sample data
TOP              ISIT312 Big Data Management, SIM S4 2025 7/17
================================================================================
PAGE 8 of 17
================================================================================

Stream Processing
Main challenges of stream processing are the following
Processing out-of-order data based on application timestamps (also called
event time)
Maintaining large amounts of states
Supporting high-data throughput
Processing each event exactly once despite machine failures
Handling load imbalance and strugglers
Responding to events at low latency
Joining with external data in other storage systems
Determining how to update output sinks as new events arrive
Writing data transactionally to output systems
Updating application business logic at runtime-
-
-
-
-
-
-
-
-
-
TOP              ISIT312 Big Data Management, SIM S4 2025 8/17
================================================================================
PAGE 9 of 17
================================================================================

Spark Structured Data and Stream Processing
Outline
Spark Stream Processing Modules
Stream Processing
Structured Stream Processing: Quick Start
Programming Model
TOP              ISIT312 Big Data Management, SIM S4 2025 9/17
================================================================================
PAGE 10 of 17
================================================================================

Structured Stream Processing: Quick Start
Structured Streaming Processing suppose to provide fast, scalable, fault-
tolerant, end-to-end exactly-once stream processing without the user
having to reason about streaming
A streaming version of the word-count example
val lines =  spark.readStream
                 . format("socket")             // socket source
                 . option("host", "localhost")  // listen to the localhost
                 . option("port", 9999)        // and port 9999
                 . load()
Reading a stream
import spark. implicits. _
 Importing methods
val words =  lines.as[String].flatMap( _.split(" "))
 sql
val wordCounts =  words.groupBy("value").count()
 Grouping
val query =  wordCounts. writeStream
                      . outputMode( "complete")  // accumulate the counting result
                      . format("console")       // use the console as the sink
                      . start()
Writing stream
TOP              ISIT312 Big Data Management, SIM S4 2025 10/17
================================================================================
PAGE 11 of 17
================================================================================

Structured Stream Processing: Quick Start
The input is simulated by Netcat (a small utility found in most Unix-like
systems) as a data server
In a di!erent Terminal, we start Spark-shell and input the Scala code
from the previous slides
If we input in the ﬁrst Terminal session
nc -lk 9999
Starting Netcat
nc -lk 9999
apache spark
apache hadoop
...
Starting Netcat
TOP              ISIT312 Big Data Management, SIM S4 2025 11/17
================================================================================
PAGE 12 of 17
================================================================================

Structured Stream Processing: Quick Start
Then we should see the right hand-side output in Spark-shell
TOP              ISIT312 Big Data Management, SIM S4 2025 12/17
================================================================================
PAGE 13 of 17
================================================================================

Spark Structured Data and Stream Processing
Outline
Spark Stream Processing Modules
Stream Processing
Structured Stream Processing: Quick Start
Programming Model
TOP              ISIT312 Big Data Management, SIM S4 2025 13/17
================================================================================
PAGE 14 of 17
================================================================================

Programming Model
The key idea in Structured Streaming is to treat a live data stream as a
table that is being continuously appended
This leads to a new stream processing model that is very similar to a
batch processing model
Users can express the streaming computation as standard batch-like
query as on a static table, and Spark  runs it as an incremental query on
the unbounded Input Table
A new data item arriving on the stream is like a new row being
appended to Input Table
TOP              ISIT312 Big Data Management, SIM S4 2025 14/17
================================================================================
PAGE 15 of 17
================================================================================

Programming Model
A query on the input will generate Result Table
Every trigger interval, let us say, every X seconds, the new rows get
appended to Input Table
It will eventually updates Result Table
Whenever Result Table gets updated, we would want to write the
changed result rows to an external sink
TOP              ISIT312 Big Data Management, SIM S4 2025 15/17
================================================================================
PAGE 16 of 17
================================================================================

Programming Model
A complete process
TOP              ISIT312 Big Data Management, SIM S4 2025 16/17
================================================================================
PAGE 17 of 17
================================================================================

References
A Gentle Introduction to Spark, Databricks, (Available in READINGS
folder)
RDD Programming Guide
Spark SQL, DataFrames and Datasets Guide
Karau H., Fast data processing with Spark Packt Publishing, 2013
(Available from UOW Library)
Srinivasa, K.G., Guide to High Performance Distributed Computing: Case
Studies with Hadoop, Scalding and SparkSpringer, 2015 (Available from
UOW Library)
Chambers B., Zaharia M.,Spark: The Deﬁnitive Guide, O'Reilly 2017
Perrin J-G., Spark in Action, 2nd ed., Manning Publications Co. 2020
TOP              ISIT312 Big Data Management, SIM S4 2025 17/17
================================================================================


################################################################################
FILE: 21 Subject Review.pdf
################################################################################

================================================================================
PAGE 1 of 4
================================================================================

1
ISIT312 
Big Data Management
SIM S
 4
 
2025
================================================================================
PAGE 2 of 4
================================================================================

2Final Exam
Exam D&T:   TBC   
   3 hours
   face-to-face
   closed–book 
   
   The venue and seat will be announced by 
  SIM
Question:   50 marks in total
   6 short answer questions
   no multiple choice questions
   no Java programming questions
   a cheat sheet with HDFS, HQL, Hbase , and 
  Spark examples will be provided
================================================================================
PAGE 3 of 4
================================================================================

3Final Exam
Contents:   
1.Hadoop architecture (HDFS, YARN, MapReduce layers)
2.MapReduce Model (read ->map ->shuffle ->sort ->reduce)
3.Basic Hadoop HDFS commands
4.Hive concept, structure
5.Basic HQL commands
6.Data warehouse concept and design
7.Multidimensional data model
8.Relational data model
9.Hbase  model (basic commands)
10.Spark concept, data model, and operations (basic 
commands)
   
================================================================================
PAGE 4 of 4
================================================================================

4Questions ?
================================================================================


################################################################################
FILE: exams-s2-2017-isit312(4).pdf
################################################################################

================================================================================
PAGE 1 of 10
================================================================================

You may print or download ONE copy of this document for the purpose of your own research or study.UNIVERSITY OF WOLLONGONG
COPYRIGHT WARNING
You may print or download ONE copy of this document for the purpose of your own research or
study. The University does not authorise you to copy, communicate or otherwise make availableelectronically to any other person any copyright material contained on this site. You are reminded ofthe following:
Copyright owners are entitled to take legal action against persons who infringe their copyright. A
reproduction of material that is protected by copyright may be a copyright infringement. A court mayimpose penalties and award damages in relation to offences and infringements relating to copyrightmaterial. Higher penalties may apply, and higher damages may be awarded, for offences andinfringements involving the conversion of material into digital or electronic form.
================================================================================
PAGE 2 of 10
================================================================================


================================================================================
PAGE 3 of 10
================================================================================


================================================================================
PAGE 4 of 10
================================================================================


================================================================================
PAGE 5 of 10
================================================================================


================================================================================
PAGE 6 of 10
================================================================================


================================================================================
PAGE 7 of 10
================================================================================


================================================================================
PAGE 8 of 10
================================================================================


================================================================================
PAGE 9 of 10
================================================================================


================================================================================
PAGE 10 of 10
================================================================================


================================================================================
